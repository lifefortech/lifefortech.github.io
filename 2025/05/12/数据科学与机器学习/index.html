<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>数据科学与机器学习 | Welcome To LifeTech's Blog</title><meta name="keywords" content="python相关学习"><meta name="author" content="Jackey Zhou"><meta name="copyright" content="Jackey Zhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="数据科学与机器学习"><meta name="application-name" content="数据科学与机器学习"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="数据科学与机器学习"><meta property="og:url" content="http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="Welcome To LifeTech's Blog"><meta property="og:description" content="目录 1. Python 在数据科学中的作用 为什么选择 Python 进行数据科学? 核心数据科学库概览  2. 核心库实践 - 数据处理与可视化2.1. NumPy (Numerical Python) NumPy 数组 (ndarray) 数组创建 数组操作与数学运算 索引与切片 广播 (Br"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="Jackey Zhou"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="目录 1. Python 在数据科学中的作用 为什么选择 Python 进行数据科学? 核心数据科学库概览  2. 核心库实践 - 数据处理与可视化2.1. NumPy (Numerical Python) NumPy 数组 (ndarray) 数组创建 数组操作与数学运算 索引与切片 广播 (Br"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2024/07/27/125766904/ba62475f396df9de3316a08ed9e65d86_5680958632268053399..png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Jackey Zhou","link":"链接: ","source":"来源: Welcome To LifeTech's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Welcome To LifeTech's Blog',
  title: '数据科学与机器学习',
  postAI: '',
  pageFillDescription: ', , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , 1. Python 在数据科学中的作用, 为什么选择 Python 进行数据科学?, 核心数据科学库概览, 2. 核心库实践 - 数据处理与可视化, 2.1. NumPy (Numerical Python), NumPy 数组 (ndarray), 数组创建, 数组操作与数学运算, 索引与切片, 广播 (Broadcasting), 2.2. Pandas (Python Data Analysis Library), Series 与 DataFrame, 数据导入与导出 (CSV Excel 等), 数据查看与检测, 数据选择与索引 (loc iloc), 数据清洗 (处理缺失值、重复值), 数据转换与操作 (apply map), 分组与聚合 (groupby), 合并与连接 (merge concat join), 2.3. Matplotlib 与 Seaborn (数据可视化), Matplotlib 基础, Seaborn 统计可视化, 3. Scikit-learn 简介与核心概念, Scikit-learn 是什么?, 设计哲学与 API 一致性, 核心对象类型, Estimator (估计器), Transformer (转换器), Predictor (预测器), 基本 API 方法 fit() transform() predict() score(), 4. 数据预处理 (Preprocessing), 4.1. 标准化与归一化 (Scaling), StandardScaler, MinMaxScaler, RobustScaler, 4.2. 编码分类特征 (Encoding Categorical Features), LabelEncoder, OneHotEncoder, Pandas get_dummies(), 4.3. 处理缺失值 (Handling Missing Values), SimpleImputer, KNNImputer (更高级), 4.4. 特征离散化x2F分箱 (Discretizationx2FBinning), KBinsDiscretizer, 4.5. 生成多项式特征 (Polynomial Features), PolynomialFeatures, 4.6. 自定义转换器 (Custom Transformers), 5. 模型选择与评估 (Model Selection and Evaluation), 5.1. 数据集划分 (train_test_split), 5.2. 交叉验证 (Cross-Validation), cross_val_score, KFold StratifiedKFold 等策略, 5.3. 模型评估指标 (Metrics), 分类指标, 回归指标, 聚类指标 (简述), 5.4. 超参数调优 (Hyperparameter Tuning), GridSearchCV, RandomizedSearchCV, 其他调优策略 (如 Bayesian Optimization - 外部库), 5.5. 学习曲线与验证曲线 (Learning and Validation Curves), 6. 监督学习 (Supervised Learning), 6.1. 回归 (Regression), 线性回归 (LinearRegression), 岭回归 (Ridge) 与 Lasso 回归 (Lasso), 支持向量回归 (SVR), 决策树回归 (DecisionTreeRegressor), 集成方法回归 (RandomForestRegressor GradientBoostingRegressor AdaBoostRegressor XGBoost LightGBM), 6.2. 分类 (Classification), 逻辑回归 (LogisticRegression), K 近邻 (KNeighborsClassifier), 支持向量机 (SVC), 决策树分类 (DecisionTreeClassifier), 朴素贝叶斯 (GaussianNB MultinomialNB BernoulliNB), 集成方法分类 (RandomForestClassifier GradientBoostingClassifier AdaBoostClassifier XGBoost LightGBM), 6.3. 特征重要性 (Feature Importance), 6.4. 处理不平衡数据集 (Handling Imbalanced Datasets), 7. 无监督学习 (Unsupervised Learning), 7.1. 聚类 (Clustering), K 均值聚类 (KMeans), DBSCAN (DBSCAN), 层次聚类 (AgglomerativeClustering), 评估聚类性能 (轮廓系数、Calinski-Harabasz 指数等), 7.2. 降维 (Dimensionality Reduction), 主成分分析 (PCA), t-SNE (TSNE - 主要用于可视化), 其他降维方法 (LDA NMF), 7.3. 异常检测 (Anomaly Detection - 简述), 8. 构建 Pipeline (Pipelines), 使用 Pipeline 串联转换器和估计器, 使用 ColumnTransformer 对不同列应用不同转换, Pipeline 与 GridSearchCVx2FRandomizedSearchCV 结合, 9. 模型持久化 (Model Persistence), 使用 joblib, 使用 pickle (注意事项), 10. 实践案例：一个完整的数据科学项目流程示例, 10.1. 问题定义与数据加载, 10.2. EDA 与数据预处理, 10.3. 特征工程, 10.4. 模型训练与评估, 10.5. 超参数调优, 10.6. 最终模型与预测, 11. 总结与进阶学习目录在数据科学中的作用为什么选择进行数据科学核心数据科学库概览核心库实践数据处理与可视化数组数组创建数组操作与数学运算索引与切片广播与数据导入与导出等数据查看与检测数据选择与索引数据清洗处理缺失值重复值数据转换与操作分组与聚合合并与连接与数据可视化基础统计可视化简介与核心概念是什么设计哲学与一致性核心对象类型估计器转换器预测器基本方法数据预处理标准化与归一化编码分类特征处理缺失值更高级特征离散化分箱生成多项式特征自定义转换器模型选择与评估数据集划分交叉验证等策略模型评估指标分类指标回归指标聚类指标简述超参数调优其他调优策略如外部库学习曲线与验证曲线监督学习回归线性回归岭回归与回归支持向量回归决策树回归集成方法回归分类逻辑回归近邻支持向量机决策树分类朴素贝叶斯集成方法分类特征重要性处理不平衡数据集无监督学习聚类均值聚类层次聚类评估聚类性能轮廓系数指数等降维主成分分析主要用于可视化其他降维方法异常检测简述构建使用串联转换器和估计器使用对不同列应用不同转换与结合模型持久化使用使用注意事项实践案例一个完整的数据科学项目流程示例问题定义与数据加载与数据预处理特征工程模型训练与评估超参数调优最终模型与预测总结与进阶学习在数据科学中的作用为什么选择进行数据科学已成为数据科学机器学习和人工智能领域最受欢迎的编程语言之一主要原因包括简洁易学语法清晰接近自然语言上手快使得研究人员和分析师可以专注于问题本身庞大的生态系统拥有极其丰富的第三方库专为数据处理分析可视化和机器学习设计强大的社区支持遇到问题时很容易找到解决方案和帮助通用性不仅仅局限于数据科学还可以用于开发自动化脚本应用程序开发等方便将模型集成到更大的系统中可扩展性可以与等语言集成用于性能优化解释性便于交互式数据探索和快速原型开发核心数据科学库概览提供高效的多维数组对象及相关操作是科学计算的基础提供高性能易用的数据结构如和数据分析工具功能强大的基础绘图库可创建各种静态动态交互式图表基于的高级统计数据可视化库提供更美观更简洁的接口数据科学领域最重要的机器学习库提供了大量监督学习和无监督学习算法模型选择预处理工具等侧重于统计建模统计测试和时间序列分析包含用于科学和技术计算的模块如优化信号处理统计线性代数等深度学习领域用于构建和训练深度神经网络本指南将重点介绍的基础并深入探讨核心库实践数据处理与可视化在进行机器学习之前数据通常需要大量的处理和探索是中科学计算的基础包它提供了一个强大的维数组对象以及用于处理这些数组的各种例程数组是一个包含相同类型数据的多维容器从列表创建一维数组或取决于系统从嵌套列表创建二维数组维度数量数组创建创建全零数组创建全一数组类似但返回数组在指定间隔内返回均匀间隔的数字创建的随机数数组到之间创建单位矩阵数组操作与数学运算允许对整个数组执行高效的元素级运算向量化操作无需显式循环对每个元素计算正弦值布尔数组矩阵运算矩阵乘法或者使用运算符转置聚合函数按列计算均值按行计算标准差索引与切片与列表类似但可以用于多维赋值第行第列索引切片子矩阵布尔索引一维数组所有偶数注意数组的切片是原始数组的视图不是副本修改切片会影响原始数组如果需要副本使用方法广播广播描述了在算术运算期间如何处理具有不同形状的数组它允许在没有显式复制数据的情况下进行向量化操作规则如果两个数组的维度数不同则将维度较少的数组的形状在其前面补直到维度数相同如果两个数组在某个维度上的大小不匹配并且其中一个数组在该维度上的大小为则该数组会在该维度上进行扩展以匹配另一个数组的大小如果在任何维度上的大小都不匹配且不等于则会引发错误广播变为概念上广播和保持变为提供了两种主要的数据结构一维和二维以及用于数据清洗转换分析和可视化的丰富功能与一维标记数组能够存储任何数据类型整数字符串浮点数对象等它有一个关联的标签数组称为索引二维标记数据结构具有可能不同类型的列可以将其视为电子表格表或对象的字典它有行索引和列索引数据导入与导出等写入避免写入行索引读取写入读取其他格式数据查看与检测查看前几行默认行查看后行查看行索引查看列名查看每列的数据类型生成描述性统计仅对数值列的简要摘要包括内存使用非空值等行数列数转置按列降序排序数据选择与索引选择列返回或返回基于标签的索引选择第一行选择所有行的列选择特定行和列选择特定单元格的值基于条件的行选择基于整数位置的索引选择第一行类似选择前两行和前两列选择特定位置的行和列数据清洗处理缺失值重复值创建一个带缺失值的统计每列的缺失值数量删除包含缺失值的行或列删除任何包含的行删除所有值都为的列填充缺失值用填充所有用列均值填充列的处理重复值检查列是否有重复删除列的重复行保留第一个数据转换与操作将函数应用于的行或列沿列累加计算每行的极差用于将函数应用于的每个元素使用字典映射格式化字符串忽略用于将函数应用于的每个元素分组与聚合类似于中的操作通常分为分割应用合并步骤计算每队的均值计算每队的分数总和对不同列应用不同聚合合并与连接沿指定轴连接多个对象默认行连接类似于的操作外连接基于索引或指定列连接与数据可视化数据可视化是理解数据模式趋势和异常的关键基础是的核心绘图库简单线图设置图像大小散点图显示颜色条直方图子图行列的子图调整子图间距统计可视化基于提供了更高级的接口来绘制引人入胜的统计图形加载示例数据集内置数据集散点图带回归线分布图直方图箱线图热力图配对图简介与核心概念是什么通常简写为是中一个功能强大且广泛使用的开源机器学习库它提供了简单高效的工具用于数据挖掘和数据分析构建在和之上设计哲学与一致性的设计注重易用性高性能良好文档和一致的一致性所有对象共享一个通用简洁的接口检验所有算法参数和输入数据都经过严格检验组合性许多机器学习任务可以表示为基本算法的序列或组合通过机制很好地支持了这一点合理的默认值大多数参数都有合理的默认值使得初学者可以快速上手核心对象类型估计器任何可以从数据中学习一些参数的对象都被称为估计器例如是一个聚类估计器是一个回归估计器估计本身是通过方法执行的它接受一个数据集通常是数组作为参数对于监督学习还会接受标签或目标值所有估计器的参数都可以在实例化时设置或者在实例化后通过其属性直接修改转换器一种特殊类型的估计器可以转换输入数据例如用于特征标准化用于降维除了方法用于学习转换所需的参数如均值和标准差转换器还有一个方法用于将学习到的转换应用于新数据通常还有一个便捷的方法它等同于先调用然后调用预测器一种特殊类型的估计器可以基于学习到的模型对新数据点进行预测除了预测器还有一个方法它接受新数据并返回预测值还有一个方法用于评估模型在给定测试数据和标签上的性能对于分类返回准确率对于回归返回分数基本方法从训练数据特征和目标监督学习中中学习模型参数返回估计器本身允许链式调用应用学习到的转换到数据学习转换参数并应用转换对新数据进行预测评估预测器在测试数据上的性能数据预处理数据预处理是机器学习流程中至关重要的一步它将原始数据转换为适合机器学习模型的格式的模块提供了多种工具标准化与归一化许多机器学习算法对特征的尺度敏感通过移除均值并缩放到单位方差来进行标准化将特征缩放到给定的最小值和最大值之间通常是使用对异常值更鲁棒的统计量如中位数和四分位数范围进行缩放适用于包含许多异常值的数据集编码分类特征机器学习算法通常需要数值输入将分类标签通常是目标变量编码为到之间的数值注意通常不推荐用于特征除非是序数特征因为它引入了人为的顺序关系将具有个可能值的分类特征转换为个二元特征或假设特征是一个二维数组每个内层数组是一个样本的特征注意期望输入是二维的先用或手动将类别转为数字如果需要或直接处理字符串可以直接处理字符串类别如果设置或提供类别列表示例对中的列进行返回密集数组可以通过来处理测试集中出现的新类别或者来避免多重共线性也提供了方便的独热编码方法处理缺失值用提供的策略均值中位数众数或常量填充缺失值更高级使用最近邻方法来填充缺失值每个样本的缺失值都是使用在训练集中找到的个最近邻居的值进行估算的特征离散化分箱将连续特征划分为离散的区间箱子将连续数据特征分箱支持多种分箱策略生成多项式特征通过组合现有特征生成新的多项式特征和交互特征可以帮助线性模型捕捉非线性关系表示最高次默认是会添加一列作为偏置项假设有特征会生成自定义转换器通过继承和或使用可以创建自己的转换器以便与集成无实际参数只是示例无需学习假设是数组添加新特征列使用示例模型选择与评估模块是模型选择和评估的关键数据集划分将数据集划分为训练集和测试集以评估模型在未见过数据上的泛化能力假设是特征是目标变量用于分类确保类别比例一致测试集所占比例或绝对数量随机数种子确保每次划分结果一致对于分类问题确保训练集和测试集中各类别的比例与原数据集相似交叉验证一种更鲁棒的模型评估技术将训练数据多次划分为训练子集和验证子集对估计器进行交叉验证并返回每次运行的评分折交叉验证参数可以指定不同的评估指标等策略参数可以接受不同的交叉验证策略对象如折交叉验证分层折确保每折中类别比例与整体一致适用于分类留一法交叉验证模型评估指标模块提供了大量用于评估模型性能的函数分类指标准确率正确分类的样本比例对不平衡数据集可能产生误导精确率在所有被预测为正类的样本中实际为正类的比例召回率真正类率在所有实际为正类的样本中被正确预测为正类的比例分数精确率和召回率的调和平均数混淆矩阵是真实类别为但被预测为类别的样本数量分类报告显示主要分类指标的文本报告曲线与曲线以不同阈值下的假正类率为横轴真正类率为纵轴是曲线下的面积值越接近模型性能越好假设是真实标签是模型预测标签是模型预测概率用于需要正类的概率回归指标平均绝对误差的平均值均方误差的平均值均方根误差的平方根没有直接的函数但可以分数决定系数表示模型解释的数据方差比例值越接近越好也可能为负聚类指标简述如轮廓系数指数调整兰德指数等用于评估聚类效果的好坏超参数调优超参数是模型在学习开始前设置的参数如正则化强度核函数类型通过构建参数网格穷举搜索所有参数组合找到最佳参数定义参数网格假设已定义从参数的给定分布中随机采样一定数量的参数组合对于高维参数空间通常比网格搜索更高效其他调优策略如外部库如等库提供了更高级的贝叶斯优化等调优方法学习曲线与验证曲线学习曲线显示模型在不同训练集大小下的训练得分和验证得分有助于判断模型是欠拟合过拟合还是数据量不足验证曲线显示模型在单个超参数不同取值下的训练得分和验证得分有助于选择合适的超参数值监督学习监督学习是从标记数据即每个数据点都有一个已知的输出或目标中学习一个函数该函数可以将新的输入映射到输出回归预测连续值输出线性回归拟合一个线性模型通过找到最佳的系数来最小化真实值和预测值之间的残差平方和岭回归与回归都是线性回归的正则化版本用于防止过拟合正则化在损失函数中添加系数的平方和作为惩罚项使得系数趋向于较小的值控制正则化强度正则化在损失函数中添加系数的绝对值之和作为惩罚项倾向于产生稀疏解一些系数变为零可以用于特征选择控制正则化强度和正则化的结合支持向量回归支持向量机的回归版本尝试在拟合数据的同时保持尽可能多的点在指定的间隔由参数定义内决策树回归通过学习简单的决策规则基于特征值进行分裂来预测目标值树的叶节点包含预测值通常是该叶节点训练样本的均值容易过拟合通常通过剪枝或集成来改进集成方法回归集成方法通过组合多个基学习器通常是决策树来获得更好的预测性能随机森林构建多个决策树每棵树在数据的随机子集和特征的随机子集上训练预测结果是所有树预测的平均值梯度提升顺序构建树每棵新树都试图纠正前一棵树的残差顺序构建学习器后续学习器更关注先前学习器错误分类的样本之外的更高级性能通常更好的梯度提升库但可以与兼容分类预测离散类别标签逻辑回归尽管名为回归但它是一种线性分类模型它使用函数将线性组合的输出映射到之间的概率选择很重要获取类别概率参数是正则化强度的倒数较小的意味着更强的正则化近邻基于实例的学习对新样本进行分类时查找训练集中与其最近的个邻居并根据这些邻居的多数类别进行投票决定需要选择合适的值和距离度量特征缩放非常重要支持向量机寻找一个能将不同类别最大间隔分开的超平面可以使用不同的核函数如来处理非线性可分数据关键超参数正则化对于核特征缩放非常重要决策树分类与回归树类似但叶节点包含类别标签或类别概率分布通过信息增益基尼不纯度等标准来选择最佳分裂朴素贝叶斯基于贝叶斯定理的一系列分类器假设特征之间相互独立朴素假设适用于特征是连续高斯分布的情况适用于离散特征如文本分类中的词频适用于二元特征集成方法分类与回归类似但用于分类任务预测结果通常是多数投票或平均概率特征重要性许多基于树的模型如决策树随机森林梯度提升可以提供特征重要性分数指示每个特征对模型预测的贡献程度假设是一个训练好的或如果是处理不平衡数据集当一个类别的样本数量远多于其他类别时模型可能会偏向多数类重采样技术过采样少数类如通常使用库欠采样多数类随机移除多数类样本调整类别权重许多分类器有参数例如使用合适的评估指标准确率不适用应关注精确率召回率分数集成方法有些集成方法对不平衡数据更鲁棒无监督学习无监督学习是从无标签数据中发现隐藏结构或模式聚类将数据点分组成相似的簇均值聚类迭代算法旨在将个观测值划分为个簇使得每个观测值都属于具有最近均值簇中心的簇需要预先指定簇的数量对初始质心敏感对特征尺度敏感通常在缩放后的数据上进行选择的方法包括肘部法则和轮廓分析基于密度的聚类算法可以发现任意形状的簇并且能够识别噪声点主要参数邻域半径和形成核心对象的最小样本数不需要预先指定簇的数量层次聚类构建一个簇的层次结构树状图可以是凝聚的自底向上或分裂的自顶向下实现的是凝聚的可以指定簇的数量或切割树状图的距离阈值评估聚类性能轮廓系数指数等轮廓系数衡量一个样本与其自身簇的紧密程度相对于其他簇的分离程度值在到之间越接近越好指数基于簇间离散度和簇内离散度的比率分数越高越好如果真实标签已知通常用于评估可以使用调整兰德指数互信息等降维减少数据中的特征数量同时保留重要信息目的缓解维度灾难减少计算复杂度数据可视化去除冗余特征主成分分析一种线性降维技术通过正交变换将数据投影到由最大方差方向定义的一系列新的低维坐标系主成分上需要选择保留的主成分数量可以通过解释方差比来决定特征缩放很重要保留的方差或指定整数通常在缩放后的数据上进行主要用于可视化是一种非线性降维技术特别擅长于高维数据的可视化它将高维数据点之间的相似性映射到低维空间通常是或使得相似的点在低维空间中也彼此靠近计算成本较高其他降维方法线性判别分析一种监督降维技术也可用作分类器旨在找到最大化类间可分性的特征子空间非负矩阵分解将数据分解为两个非负矩阵的乘积常用于特征提取如文本主题建模异常检测简述识别与大多数数据显著不同的数据点基于统计的方法如基于距离的方法如基于密度的方法如中的工具学习一个边界来包围正常数据通过随机分割来隔离异常点假设数据服从高斯分布拟合一个椭圆构建允许将多个处理步骤转换器和最终的估计器串联起来形成一个单一的估计器使用串联转换器和估计器创建一个包含缩放和的像普通估计器一样使用优点方便只需调用一次和避免数据泄露确保交叉验证的每一折都正确地在训练数据上转换器然后训练和验证数据超参数调优可以对中的所有步骤的参数进行网格搜索参数名称格式为使用对不同列应用不同转换允许对的不同列应用不同的转换序列假设有数值列和分类列或其他未指定列的处理方式或与结合假设中有名为的步骤假设中有名为的步骤模型持久化训练好的模型可以保存到磁盘以便将来加载和重用而无需重新训练使用是推荐的持久化方法尤其对于包含大型数组的对象更高效假设是一个训练好的模型保存模型加载模型使用注意事项内置的模块也可以用于序列化和反序列化对象保存模型加载模型安全警告模块不安全不要反序列化来自不受信任或未经身份验证来源的数据在内部使用但主要针对数组进行了优化对于模型两者通常都可以工作但更受推荐实践案例一个完整的数据科学项目流程示例这里概述一个简化的流程使用内置的数据集问题定义与数据加载问题根据鸢尾花的花萼和花瓣的测量值预测其种类这是一个多分类问题目标是数字与数据预处理对于数据集通常不需要太多预处理但我们会进行标准化简单的查看特征分布仅示例一个划分数据集标准化仅对训练集然后对训练集和测试集使用训练集的均值和标准差特征工程数据集特征已经很好此步骤可以跳过或简化在实际项目中这可能包括创建交互特征多项式特征文本特征转换等模型训练与评估选择一个分类模型例如逻辑回归预测评估超参数调优使用尝试改进逻辑回归最终模型与预测使用找到的最佳模型进行最终评估或预测可以保存总结与进阶学习本指南详细介绍了在数据科学中的核心库的基础用法并深入探讨了库的各个方面包括数据预处理模型选择监督学习算法无监督学习算法构建和模型持久化进阶学习方向深度学习框架更高级的统计建模库特定领域的库如自然语言处理计算机视觉大数据工具模型可解释性模型部署监控版本控制如贝叶斯方法与概率编程时间序列分析不断实践参与等竞赛阅读论文和博客是提升数据科学技能的关键',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-12 02:29:26',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">Welcome To LifeTech's Blog</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/CSharp%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">CSharp学习<sup>3</sup></a><a href="/tags/IDE%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 1.05rem;">IDE常用快捷键<sup>2</sup></a><a href="/tags/Rust%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 1.05rem;">Rust基础知识<sup>21</sup></a><a href="/tags/csharp%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">csharp相关学习<sup>5</sup></a><a href="/tags/docker%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">docker学习<sup>1</sup></a><a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/" style="font-size: 1.05rem;">hexo博客常见命令<sup>1</sup></a><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">python相关学习<sup>4</sup></a><a href="/tags/tauri%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 1.05rem;">tauri基础知识<sup>1</sup></a><a href="/tags/xmake%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">xmake学习<sup>3</sup></a><a href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">个人学习<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">前端学习<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.05rem;">数据分析<sup>1</sup></a><a href="/tags/%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95/" style="font-size: 1.05rem;">无线调试<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">深度学习<sup>1</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" style="font-size: 1.05rem;">系统常用命令<sup>2</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 1.05rem;">系统常用快捷键<sup>2</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">32</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"><a class="article-meta__tags" href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>python相关学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">数据科学与机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-05-11T18:17:28.000Z" title="发表于 2025-05-12 02:17:28">2025-05-12</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-05-11T18:29:26.420Z" title="更新于 2025-05-12 02:29:26">2025-05-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为无锡"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>无锡</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><header><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">python相关学习</a><h1 id="CrawlerTitle" itemprop="name headline">数据科学与机器学习</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Jackey Zhou</span><time itemprop="dateCreated datePublished" datetime="2025-05-11T18:17:28.000Z" title="发表于 2025-05-12 02:17:28">2025-05-12</time><time itemprop="dateCreated datePublished" datetime="2025-05-11T18:29:26.420Z" title="更新于 2025-05-12 02:29:26">2025-05-12</time></header><p><strong>目录</strong></p>
<h2 id="1-Python-在数据科学中的作用"><a href="#1-Python-在数据科学中的作用" class="headerlink" title="1. Python 在数据科学中的作用"></a><a href="#1-python-%E5%9C%A8%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8">1. Python 在数据科学中的作用</a></h2><ul>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9-python-%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6">为什么选择 Python 进行数据科学?</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%BA%93%E6%A6%82%E8%A7%88">核心数据科学库概览</a></li>
</ul>
<h2 id="2-核心库实践-数据处理与可视化"><a href="#2-核心库实践-数据处理与可视化" class="headerlink" title="2. 核心库实践 - 数据处理与可视化"></a><a href="#2-%E6%A0%B8%E5%BF%83%E5%BA%93%E5%AE%9E%E8%B7%B5---%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96">2. 核心库实践 - 数据处理与可视化</a></h2><h3 id="2-1-NumPy-Numerical-Python"><a href="#2-1-NumPy-Numerical-Python" class="headerlink" title="2.1. NumPy (Numerical Python)"></a><a href="#21-numpy-numerical-python">2.1. NumPy (Numerical Python)</a></h3><ul>
<li><a href="#numpy-%E6%95%B0%E7%BB%84-ndarray">NumPy 数组 (ndarray)</a></li>
<li><a href="#%E6%95%B0%E7%BB%84%E5%88%9B%E5%BB%BA">数组创建</a></li>
<li><a href="#%E6%95%B0%E7%BB%84%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97">数组操作与数学运算</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87">索引与切片</a></li>
<li><a href="#%E5%B9%BF%E6%92%AD-broadcasting">广播 (Broadcasting)</a></li>
</ul>
<h3 id="2-2-Pandas-Python-Data-Analysis-Library"><a href="#2-2-Pandas-Python-Data-Analysis-Library" class="headerlink" title="2.2. Pandas (Python Data Analysis Library)"></a><a href="#22-pandas-python-data-analysis-library">2.2. Pandas (Python Data Analysis Library)</a></h3><ul>
<li><a href="#series-%E4%B8%8E-dataframe">Series 与 DataFrame</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA-csv-excel-%E7%AD%89">数据导入与导出 (CSV, Excel 等)</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%9F%A5%E7%9C%8B%E4%B8%8E%E6%A3%80%E6%B5%8B">数据查看与检测</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9%E4%B8%8E%E7%B4%A2%E5%BC%95-loc-iloc">数据选择与索引 (loc, iloc)</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC%E9%87%8D%E5%A4%8D%E5%80%BC">数据清洗 (处理缺失值、重复值)</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E4%B8%8E%E6%93%8D%E4%BD%9C-apply-map">数据转换与操作 (apply, map)</a></li>
<li><a href="#%E5%88%86%E7%BB%84%E4%B8%8E%E8%81%9A%E5%90%88-groupby">分组与聚合 (groupby)</a></li>
<li><a href="#%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BF%9E%E6%8E%A5-merge-concat-join">合并与连接 (merge, concat, join)</a></li>
</ul>
<h3 id="2-3-Matplotlib-与-Seaborn-数据可视化"><a href="#2-3-Matplotlib-与-Seaborn-数据可视化" class="headerlink" title="2.3. Matplotlib 与 Seaborn (数据可视化)"></a><a href="#23-matplotlib-%E4%B8%8E-seaborn-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96">2.3. Matplotlib 与 Seaborn (数据可视化)</a></h3><ul>
<li><a href="#matplotlib-%E5%9F%BA%E7%A1%80">Matplotlib 基础</a></li>
<li><a href="#seaborn-%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%A7%86%E5%8C%96">Seaborn 统计可视化</a></li>
</ul>
<h2 id="3-Scikit-learn-简介与核心概念"><a href="#3-Scikit-learn-简介与核心概念" class="headerlink" title="3. Scikit-learn 简介与核心概念"></a><a href="#3-scikit-learn-%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5">3. Scikit-learn 简介与核心概念</a></h2><ul>
<li><a href="#scikit-learn-%E6%98%AF%E4%BB%80%E4%B9%88">Scikit-learn 是什么?</a></li>
<li><a href="#%E8%AE%BE%E8%AE%A1%E5%93%B2%E5%AD%A6%E4%B8%8Eapi%E4%B8%80%E8%87%B4%E6%80%A7">设计哲学与 API 一致性</a></li>
<li><a href="#%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E7%B1%BB%E5%9E%8B">核心对象类型</a><ul>
<li><a href="#estimator-%E4%BC%B0%E8%AE%A1%E5%99%A8">Estimator (估计器)</a></li>
<li><a href="#transformer-%E8%BD%AC%E6%8D%A2%E5%99%A8">Transformer (转换器)</a></li>
<li><a href="#predictor-%E9%A2%84%E6%B5%8B%E5%99%A8">Predictor (预测器)</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E6%9C%ACapi%E6%96%B9%E6%B3%95-fittransformpredictscore">基本 API 方法: <code>fit()</code>, <code>transform()</code>, <code>predict()</code>, <code>score()</code></a></li>
</ul>
<h2 id="4-数据预处理-Preprocessing"><a href="#4-数据预处理-Preprocessing" class="headerlink" title="4. 数据预处理 (Preprocessing)"></a><a href="#4-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-preprocessing">4. 数据预处理 (Preprocessing)</a></h2><h3 id="4-1-标准化与归一化-Scaling"><a href="#4-1-标准化与归一化-Scaling" class="headerlink" title="4.1. 标准化与归一化 (Scaling)"></a><a href="#41-%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96-scaling">4.1. 标准化与归一化 (Scaling)</a></h3><ul>
<li><a href="#standardscaler"><code>StandardScaler</code></a></li>
<li><a href="#minmaxscaler"><code>MinMaxScaler</code></a></li>
<li><a href="#robustscaler"><code>RobustScaler</code></a></li>
</ul>
<h3 id="4-2-编码分类特征-Encoding-Categorical-Features"><a href="#4-2-编码分类特征-Encoding-Categorical-Features" class="headerlink" title="4.2. 编码分类特征 (Encoding Categorical Features)"></a><a href="#42-%E7%BC%96%E7%A0%81%E5%88%86%E7%B1%BB%E7%89%B9%E5%BE%81-encoding-categorical-features">4.2. 编码分类特征 (Encoding Categorical Features)</a></h3><ul>
<li><a href="#labelencoder"><code>LabelEncoder</code></a></li>
<li><a href="#onehotencoder"><code>OneHotEncoder</code></a></li>
<li><a href="#pandas-get_dummies">Pandas <code>get_dummies()</code></a></li>
</ul>
<h3 id="4-3-处理缺失值-Handling-Missing-Values"><a href="#4-3-处理缺失值-Handling-Missing-Values" class="headerlink" title="4.3. 处理缺失值 (Handling Missing Values)"></a><a href="#43-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC-handling-missing-values">4.3. 处理缺失值 (Handling Missing Values)</a></h3><ul>
<li><a href="#simpleimputer"><code>SimpleImputer</code></a></li>
<li><a href="#knnimputer-%E6%9B%B4%E9%AB%98%E7%BA%A7">KNNImputer (更高级)</a></li>
</ul>
<h3 id="4-4-特征离散化-分箱-Discretization-Binning"><a href="#4-4-特征离散化-分箱-Discretization-Binning" class="headerlink" title="4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)"></a><a href="#44-%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96%E5%88%86%E7%AE%B1-discretizationbinning">4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)</a></h3><ul>
<li><a href="#kbinsdiscretizer"><code>KBinsDiscretizer</code></a></li>
</ul>
<h3 id="4-5-生成多项式特征-Polynomial-Features"><a href="#4-5-生成多项式特征-Polynomial-Features" class="headerlink" title="4.5. 生成多项式特征 (Polynomial Features)"></a><a href="#45-%E7%94%9F%E6%88%90%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81-polynomial-features">4.5. 生成多项式特征 (Polynomial Features)</a></h3><ul>
<li><a href="#polynomialfeatures"><code>PolynomialFeatures</code></a></li>
</ul>
<h3 id="4-6-自定义转换器-Custom-Transformers"><a href="#4-6-自定义转换器-Custom-Transformers" class="headerlink" title="4.6. 自定义转换器 (Custom Transformers)"></a><a href="#46-%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BD%AC%E6%8D%A2%E5%99%A8-custom-transformers">4.6. 自定义转换器 (Custom Transformers)</a></h3><h2 id="5-模型选择与评估-Model-Selection-and-Evaluation"><a href="#5-模型选择与评估-Model-Selection-and-Evaluation" class="headerlink" title="5. 模型选择与评估 (Model Selection and Evaluation)"></a><a href="#5-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%AF%84%E4%BC%B0-model-selection-and-evaluation">5. 模型选择与评估 (Model Selection and Evaluation)</a></h2><h3 id="5-1-数据集划分-train-test-split"><a href="#5-1-数据集划分-train-test-split" class="headerlink" title="5.1. 数据集划分 (train_test_split)"></a><a href="#51-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86-train_test_split">5.1. 数据集划分 (<code>train_test_split</code>)</a></h3><h3 id="5-2-交叉验证-Cross-Validation"><a href="#5-2-交叉验证-Cross-Validation" class="headerlink" title="5.2. 交叉验证 (Cross-Validation)"></a><a href="#52-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-cross-validation">5.2. 交叉验证 (Cross-Validation)</a></h3><ul>
<li><a href="#cross_val_score"><code>cross_val_score</code></a></li>
<li><a href="#kfold-stratifiedkfold-%E7%AD%89%E7%AD%96%E7%95%A5">KFold, StratifiedKFold 等策略</a></li>
</ul>
<h3 id="5-3-模型评估指标-Metrics"><a href="#5-3-模型评估指标-Metrics" class="headerlink" title="5.3. 模型评估指标 (Metrics)"></a><a href="#53-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-metrics">5.3. 模型评估指标 (Metrics)</a></h3><ul>
<li><a href="#%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87">分类指标</a></li>
<li><a href="#%E5%9B%9E%E5%BD%92%E6%8C%87%E6%A0%87">回归指标</a></li>
<li><a href="#%E8%81%9A%E7%B1%BB%E6%8C%87%E6%A0%87-%E7%AE%80%E8%BF%B0">聚类指标 (简述)</a></li>
</ul>
<h3 id="5-4-超参数调优-Hyperparameter-Tuning"><a href="#5-4-超参数调优-Hyperparameter-Tuning" class="headerlink" title="5.4. 超参数调优 (Hyperparameter Tuning)"></a><a href="#54-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98-hyperparameter-tuning">5.4. 超参数调优 (Hyperparameter Tuning)</a></h3><ul>
<li><a href="#gridsearchcv"><code>GridSearchCV</code></a></li>
<li><a href="#randomizedsearchcv"><code>RandomizedSearchCV</code></a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5-%E5%A6%82-bayesian-optimization---%E5%A4%96%E9%83%A8%E5%BA%93">其他调优策略 (如 Bayesian Optimization - 外部库)</a></li>
</ul>
<h3 id="5-5-学习曲线与验证曲线-Learning-and-Validation-Curves"><a href="#5-5-学习曲线与验证曲线-Learning-and-Validation-Curves" class="headerlink" title="5.5. 学习曲线与验证曲线 (Learning and Validation Curves)"></a><a href="#55-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E4%B8%8E%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF-learning-and-validation-curves">5.5. 学习曲线与验证曲线 (Learning and Validation Curves)</a></h3><h2 id="6-监督学习-Supervised-Learning"><a href="#6-监督学习-Supervised-Learning" class="headerlink" title="6. 监督学习 (Supervised Learning)"></a><a href="#6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-supervised-learning">6. 监督学习 (Supervised Learning)</a></h2><h3 id="6-1-回归-Regression"><a href="#6-1-回归-Regression" class="headerlink" title="6.1. 回归 (Regression)"></a><a href="#61-%E5%9B%9E%E5%BD%92-regression">6.1. 回归 (Regression)</a></h3><ul>
<li><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-linearregression">线性回归 (<code>LinearRegression</code>)</a></li>
<li><a href="#%E5%B2%AD%E5%9B%9E%E5%BD%92-ridge-%E4%B8%8E-lasso-%E5%9B%9E%E5%BD%92-lasso">岭回归 (<code>Ridge</code>) 与 Lasso 回归 (<code>Lasso</code>)</a></li>
<li><a href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92-svr">支持向量回归 (<code>SVR</code>)</a></li>
<li><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%9B%9E%E5%BD%92-decisiontreeregressor">决策树回归 (<code>DecisionTreeRegressor</code>)</a></li>
<li><a href="#%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E5%9B%9E%E5%BD%92-randomforestregressor-gradientboostingregressor-adaboostregressor-xgboost-lightgbm">集成方法回归 (RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, XGBoost, LightGBM)</a></li>
</ul>
<h3 id="6-2-分类-Classification"><a href="#6-2-分类-Classification" class="headerlink" title="6.2. 分类 (Classification)"></a><a href="#62-%E5%88%86%E7%B1%BB-classification">6.2. 分类 (Classification)</a></h3><ul>
<li><a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-logisticregression">逻辑回归 (<code>LogisticRegression</code>)</a></li>
<li><a href="#k%E8%BF%91%E9%82%BB-kneighborsclassifier">K 近邻 (<code>KNeighborsClassifier</code>)</a></li>
<li><a href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-svc">支持向量机 (<code>SVC</code>)</a></li>
<li><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB-decisiontreeclassifier">决策树分类 (<code>DecisionTreeClassifier</code>)</a></li>
<li><a href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-gaussiannb-multinomialnb-bernoullinb">朴素贝叶斯 (<code>GaussianNB</code>, <code>MultinomialNB</code>, <code>BernoulliNB</code>)</a></li>
<li><a href="#%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB-randomforestclassifier-gradientboostingclassifier-adaboostclassifier-xgboost-lightgbm">集成方法分类 (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, XGBoost, LightGBM)</a></li>
</ul>
<h3 id="6-3-特征重要性-Feature-Importance"><a href="#6-3-特征重要性-Feature-Importance" class="headerlink" title="6.3. 特征重要性 (Feature Importance)"></a><a href="#63-%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7-feature-importance">6.3. 特征重要性 (Feature Importance)</a></h3><h3 id="6-4-处理不平衡数据集-Handling-Imbalanced-Datasets"><a href="#6-4-处理不平衡数据集-Handling-Imbalanced-Datasets" class="headerlink" title="6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)"></a><a href="#64-%E5%A4%84%E7%90%86%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86-handling-imbalanced-datasets">6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)</a></h3><h2 id="7-无监督学习-Unsupervised-Learning"><a href="#7-无监督学习-Unsupervised-Learning" class="headerlink" title="7. 无监督学习 (Unsupervised Learning)"></a><a href="#7-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-unsupervised-learning">7. 无监督学习 (Unsupervised Learning)</a></h2><h3 id="7-1-聚类-Clustering"><a href="#7-1-聚类-Clustering" class="headerlink" title="7.1. 聚类 (Clustering)"></a><a href="#71-%E8%81%9A%E7%B1%BB-clustering">7.1. 聚类 (Clustering)</a></h3><ul>
<li><a href="#k%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB-kmeans">K 均值聚类 (<code>KMeans</code>)</a></li>
<li><a href="#dbscan-dbscan">DBSCAN (<code>DBSCAN</code>)</a></li>
<li><a href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB-agglomerativeclustering">层次聚类 (<code>AgglomerativeClustering</code>)</a></li>
<li><a href="#%E8%AF%84%E4%BC%B0%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD-%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0calinski-harabasz%E6%8C%87%E6%95%B0%E7%AD%89">评估聚类性能 (轮廓系数、Calinski-Harabasz 指数等)</a></li>
</ul>
<h3 id="7-2-降维-Dimensionality-Reduction"><a href="#7-2-降维-Dimensionality-Reduction" class="headerlink" title="7.2. 降维 (Dimensionality Reduction)"></a><a href="#72-%E9%99%8D%E7%BB%B4-dimensionality-reduction">7.2. 降维 (Dimensionality Reduction)</a></h3><ul>
<li><a href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca">主成分分析 (<code>PCA</code>)</a></li>
<li><a href="#t-sne-tsne---%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%8F%AF%E8%A7%86%E5%8C%96">t-SNE (<code>TSNE</code> - 主要用于可视化)</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95-lda-nmf">其他降维方法 (LDA, NMF)</a></li>
</ul>
<h3 id="7-3-异常检测-Anomaly-Detection-简述"><a href="#7-3-异常检测-Anomaly-Detection-简述" class="headerlink" title="7.3. 异常检测 (Anomaly Detection - 简述)"></a><a href="#73-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-anomaly-detection---%E7%AE%80%E8%BF%B0">7.3. 异常检测 (Anomaly Detection - 简述)</a></h3><h2 id="8-构建-Pipeline-Pipelines"><a href="#8-构建-Pipeline-Pipelines" class="headerlink" title="8. 构建 Pipeline (Pipelines)"></a><a href="#8-%E6%9E%84%E5%BB%BA-pipeline-pipelines">8. 构建 Pipeline (Pipelines)</a></h2><ul>
<li><a href="#%E4%BD%BF%E7%94%A8-pipeline-%E4%B8%B2%E8%81%94%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8">使用 <code>Pipeline</code> 串联转换器和估计器</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8-columntransformer-%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%88%97%E5%BA%94%E7%94%A8%E4%B8%8D%E5%90%8C%E8%BD%AC%E6%8D%A2">使用 <code>ColumnTransformer</code> 对不同列应用不同转换</a></li>
<li><a href="#pipeline-%E4%B8%8E-gridsearchcvrandomizedsearchcv-%E7%BB%93%E5%90%88">Pipeline 与 GridSearchCV&#x2F;RandomizedSearchCV 结合</a></li>
</ul>
<h2 id="9-模型持久化-Model-Persistence"><a href="#9-模型持久化-Model-Persistence" class="headerlink" title="9. 模型持久化 (Model Persistence)"></a><a href="#9-%E6%A8%A1%E5%9E%8B%E6%8C%81%E4%B9%85%E5%8C%96-model-persistence">9. 模型持久化 (Model Persistence)</a></h2><ul>
<li><a href="#%E4%BD%BF%E7%94%A8-joblib">使用 <code>joblib</code></a></li>
<li><a href="#%E4%BD%BF%E7%94%A8-pickle-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">使用 <code>pickle</code> (注意事项)</a></li>
</ul>
<h2 id="10-实践案例：一个完整的数据科学项目流程示例"><a href="#10-实践案例：一个完整的数据科学项目流程示例" class="headerlink" title="10. 实践案例：一个完整的数据科学项目流程示例"></a><a href="#10-%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B">10. 实践案例：一个完整的数据科学项目流程示例</a></h2><h3 id="10-1-问题定义与数据加载"><a href="#10-1-问题定义与数据加载" class="headerlink" title="10.1. 问题定义与数据加载"></a><a href="#101-%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD">10.1. 问题定义与数据加载</a></h3><h3 id="10-2-EDA-与数据预处理"><a href="#10-2-EDA-与数据预处理" class="headerlink" title="10.2. EDA 与数据预处理"></a><a href="#102-eda-%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">10.2. EDA 与数据预处理</a></h3><h3 id="10-3-特征工程"><a href="#10-3-特征工程" class="headerlink" title="10.3. 特征工程"></a><a href="#103-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B">10.3. 特征工程</a></h3><h3 id="10-4-模型训练与评估"><a href="#10-4-模型训练与评估" class="headerlink" title="10.4. 模型训练与评估"></a><a href="#104-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0">10.4. 模型训练与评估</a></h3><h3 id="10-5-超参数调优"><a href="#10-5-超参数调优" class="headerlink" title="10.5. 超参数调优"></a><a href="#105-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98">10.5. 超参数调优</a></h3><h3 id="10-6-最终模型与预测"><a href="#10-6-最终模型与预测" class="headerlink" title="10.6. 最终模型与预测"></a><a href="#106-%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E6%B5%8B">10.6. 最终模型与预测</a></h3><h2 id="11-总结与进阶学习"><a href="#11-总结与进阶学习" class="headerlink" title="11. 总结与进阶学习"></a><a href="#11-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0">11. 总结与进阶学习</a></h2><hr>
<h2 id="1-Python-在数据科学中的作用-1"><a href="#1-Python-在数据科学中的作用-1" class="headerlink" title="1. Python 在数据科学中的作用"></a>1. Python 在数据科学中的作用</h2><h3 id="为什么选择-Python-进行数据科学"><a href="#为什么选择-Python-进行数据科学" class="headerlink" title="为什么选择 Python 进行数据科学?"></a>为什么选择 Python 进行数据科学?</h3><p>Python 已成为数据科学、机器学习和人工智能领域最受欢迎的编程语言之一，主要原因包括：</p>
<ul>
<li><strong>简洁易学</strong>: 语法清晰，接近自然语言，上手快，使得研究人员和分析师可以专注于问题本身。</li>
<li><strong>庞大的生态系统</strong>: 拥有极其丰富的第三方库，专为数据处理、分析、可视化和机器学习设计。</li>
<li><strong>强大的社区支持</strong>: 遇到问题时，很容易找到解决方案和帮助。</li>
<li><strong>通用性</strong>: Python 不仅仅局限于数据科学，还可以用于 Web 开发、自动化脚本、应用程序开发等，方便将模型集成到更大的系统中。</li>
<li><strong>可扩展性</strong>: 可以与 C&#x2F;C++ 等语言集成，用于性能优化。</li>
<li><strong>解释性</strong>: 便于交互式数据探索和快速原型开发。</li>
</ul>
<h3 id="核心数据科学库概览"><a href="#核心数据科学库概览" class="headerlink" title="核心数据科学库概览"></a>核心数据科学库概览</h3><ul>
<li><strong>NumPy</strong>: 提供高效的多维数组对象 (<code>ndarray</code>) 及相关操作，是科学计算的基础。</li>
<li><strong>Pandas</strong>: 提供高性能、易用的数据结构 (如 <code>DataFrame</code>) 和数据分析工具。</li>
<li><strong>Matplotlib</strong>: 功能强大的基础绘图库，可创建各种静态、动态、交互式图表。</li>
<li><strong>Seaborn</strong>: 基于 Matplotlib 的高级统计数据可视化库，提供更美观、更简洁的接口。</li>
<li><strong>Scikit-learn</strong>: 数据科学领域最重要的机器学习库，提供了大量监督学习和无监督学习算法、模型选择、预处理工具等。</li>
<li><strong>Statsmodels</strong>: 侧重于统计建模、统计测试和时间序列分析。</li>
<li><strong>SciPy</strong>: 包含用于科学和技术计算的模块，如优化、信号处理、统计、线性代数等。</li>
<li>(深度学习领域) <strong>TensorFlow, Keras, PyTorch</strong>: 用于构建和训练深度神经网络。</li>
</ul>
<p>本指南将重点介绍 NumPy, Pandas, Matplotlib&#x2F;Seaborn 的基础，并深入探讨 Scikit-learn。</p>
<h2 id="2-核心库实践-数据处理与可视化-1"><a href="#2-核心库实践-数据处理与可视化-1" class="headerlink" title="2. 核心库实践 - 数据处理与可视化"></a>2. 核心库实践 - 数据处理与可视化</h2><p>在进行机器学习之前，数据通常需要大量的处理和探索。</p>
<h3 id="2-1-NumPy-Numerical-Python-1"><a href="#2-1-NumPy-Numerical-Python-1" class="headerlink" title="2.1. NumPy (Numerical Python)"></a>2.1. NumPy (Numerical Python)</h3><p>NumPy 是 Python 中科学计算的基础包。它提供了一个强大的 N 维数组对象 (<code>ndarray</code>)，以及用于处理这些数组的各种例程。</p>
<h4 id="NumPy-数组-ndarray"><a href="#NumPy-数组-ndarray" class="headerlink" title="NumPy 数组 (ndarray)"></a>NumPy 数组 (ndarray)</h4><p><code>ndarray</code> 是一个包含相同类型数据的多维容器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从列表创建一维数组</span></span><br><span class="line">arr1d = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(arr1d)</span><br><span class="line"><span class="built_in">print</span>(arr1d.shape)  <span class="comment"># (5,)</span></span><br><span class="line"><span class="built_in">print</span>(arr1d.dtype)  <span class="comment"># int64 (或 int32, 取决于系统)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从嵌套列表创建二维数组</span></span><br><span class="line">arr2d = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(arr2d)</span><br><span class="line"><span class="built_in">print</span>(arr2d.shape)  <span class="comment"># (2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(arr2d.ndim)   <span class="comment"># 2 (维度数量)</span></span><br></pre></td></tr></table></figure>

<h4 id="数组创建"><a href="#数组创建" class="headerlink" title="数组创建"></a>数组创建</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zeros_arr = np.zeros((<span class="number">2</span>, <span class="number">3</span>))      <span class="comment"># 创建全零数组</span></span><br><span class="line">ones_arr = np.ones((<span class="number">3</span>, <span class="number">2</span>))        <span class="comment"># 创建全一数组</span></span><br><span class="line">arange_arr = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># 类似 Python range，但返回数组 [0 2 4 6 8]</span></span><br><span class="line">linspace_arr = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>) <span class="comment"># 在指定间隔内返回均匀间隔的数字 [0. 0.25 0.5 0.75 1. ]</span></span><br><span class="line">random_arr = np.random.rand(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 创建 2x2 的随机数数组 (0到1之间)</span></span><br><span class="line">identity_matrix = np.eye(<span class="number">3</span>)       <span class="comment"># 创建 3x3 单位矩阵</span></span><br></pre></td></tr></table></figure>

<h4 id="数组操作与数学运算"><a href="#数组操作与数学运算" class="headerlink" title="数组操作与数学运算"></a>数组操作与数学运算</h4><p>NumPy 允许对整个数组执行高效的元素级运算 (向量化操作)，无需显式循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a + b)       <span class="comment"># [5 7 9]</span></span><br><span class="line"><span class="built_in">print</span>(a * <span class="number">2</span>)       <span class="comment"># [2 4 6]</span></span><br><span class="line"><span class="built_in">print</span>(np.sin(a))   <span class="comment"># 对每个元素计算正弦值</span></span><br><span class="line"><span class="built_in">print</span>(a &gt; <span class="number">1</span>)       <span class="comment"># [False True True] (布尔数组)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵运算</span></span><br><span class="line">mat_a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">mat_b = np.array([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="built_in">print</span>(mat_a.dot(mat_b)) <span class="comment"># 矩阵乘法</span></span><br><span class="line"><span class="comment"># 或者使用 @ 运算符 (Python 3.5+)</span></span><br><span class="line"><span class="built_in">print</span>(mat_a @ mat_b)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mat_a.T)      <span class="comment"># 转置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚合函数</span></span><br><span class="line"><span class="built_in">print</span>(arr2d.<span class="built_in">sum</span>())</span><br><span class="line"><span class="built_in">print</span>(arr2d.mean(axis=<span class="number">0</span>)) <span class="comment"># 按列计算均值</span></span><br><span class="line"><span class="built_in">print</span>(arr2d.std(axis=<span class="number">1</span>))  <span class="comment"># 按行计算标准差</span></span><br></pre></td></tr></table></figure>

<h4 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h4><p>与 Python 列表类似，但可以用于多维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">arr = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="built_in">print</span>(arr[<span class="number">2</span>])      <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(arr[<span class="number">2</span>:<span class="number">5</span>])    <span class="comment"># [2 3 4]</span></span><br><span class="line">arr[<span class="number">2</span>:<span class="number">5</span>] = <span class="number">10</span>      <span class="comment"># 赋值 [0 1 10 10 10 5 6 7 8 9]</span></span><br><span class="line"></span><br><span class="line">arr2d = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(arr2d[<span class="number">1</span>, <span class="number">2</span>])   <span class="comment"># 6 (第1行，第2列 - 0索引)</span></span><br><span class="line"><span class="built_in">print</span>(arr2d[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># 切片子矩阵 [[2 3] [5 6]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 布尔索引</span></span><br><span class="line">bool_idx = arr2d &gt; <span class="number">5</span></span><br><span class="line"><span class="built_in">print</span>(arr2d[bool_idx]) <span class="comment"># [6 7 8 9] (一维数组)</span></span><br><span class="line"><span class="built_in">print</span>(arr2d[arr2d % <span class="number">2</span> == <span class="number">0</span>]) <span class="comment"># 所有偶数</span></span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>: NumPy 数组的切片是原始数组的视图 (view)，不是副本 (copy)。修改切片会影响原始数组。如果需要副本，使用 <code>.copy()</code> 方法。</p>
<h4 id="广播-Broadcasting"><a href="#广播-Broadcasting" class="headerlink" title="广播 (Broadcasting)"></a>广播 (Broadcasting)</h4><p>广播描述了 NumPy 在算术运算期间如何处理具有不同形状的数组。它允许在没有显式复制数据的情况下进行向量化操作。<br>规则：</p>
<ol>
<li>如果两个数组的维度数不同，则将维度较少的数组的形状在其前面补 1，直到维度数相同。</li>
<li>如果两个数组在某个维度上的大小不匹配，并且其中一个数组在该维度上的大小为 1，则该数组会在该维度上进行扩展以匹配另一个数组的大小。</li>
<li>如果在任何维度上的大小都不匹配且不等于 1，则会引发错误。</li>
</ol>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]) <span class="comment"># (2, 3)</span></span><br><span class="line">b = np.array([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])            <span class="comment"># (3,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 广播 b: b 变为 [[10, 20, 30], [10, 20, 30]] (概念上)</span></span><br><span class="line">result = a + b</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># [[11 22 33]</span></span><br><span class="line"><span class="comment">#  [14 25 36]]</span></span><br><span class="line"></span><br><span class="line">c = np.array([[<span class="number">10</span>], [<span class="number">20</span>]]) <span class="comment"># (2, 1)</span></span><br><span class="line"><span class="comment"># 广播 a 和 c:</span></span><br><span class="line"><span class="comment"># a 保持 (2, 3)</span></span><br><span class="line"><span class="comment"># c 变为 [[10, 10, 10], [20, 20, 20]]</span></span><br><span class="line">result2 = a + c</span><br><span class="line"><span class="built_in">print</span>(result2)</span><br><span class="line"><span class="comment"># [[11 12 13]</span></span><br><span class="line"><span class="comment">#  [24 25 26]]</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-Pandas-Python-Data-Analysis-Library-1"><a href="#2-2-Pandas-Python-Data-Analysis-Library-1" class="headerlink" title="2.2. Pandas (Python Data Analysis Library)"></a>2.2. Pandas (Python Data Analysis Library)</h3><p>Pandas 提供了两种主要的数据结构：<code>Series</code> (一维) 和 <code>DataFrame</code> (二维)，以及用于数据清洗、转换、分析和可视化的丰富功能。</p>
<h4 id="Series-与-DataFrame"><a href="#Series-与-DataFrame" class="headerlink" title="Series 与 DataFrame"></a>Series 与 DataFrame</h4><ul>
<li><p><strong><code>Series</code></strong>: 一维标记数组，能够存储任何数据类型 (整数、字符串、浮点数、Python 对象等)。它有一个关联的标签数组，称为索引 (index)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">s = pd.Series([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, np.nan, <span class="number">6</span>, <span class="number">8</span>], name=<span class="string">&#x27;MyNumbers&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(s)</span><br><span class="line"><span class="built_in">print</span>(s.index)</span><br></pre></td></tr></table></figure></li>
<li><p><strong><code>DataFrame</code></strong>: 二维标记数据结构，具有可能不同类型的列。可以将其视为电子表格、SQL 表或 Series 对象的字典。它有行索引和列索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = &#123;<span class="string">&#x27;col1&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;col2&#x27;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>]&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line">dates = pd.date_range(<span class="string">&#x27;20250101&#x27;</span>, periods=<span class="number">6</span>)</span><br><span class="line">df_random = pd.DataFrame(np.random.randn(<span class="number">6</span>, <span class="number">4</span>), index=dates, columns=<span class="built_in">list</span>(<span class="string">&#x27;ABCD&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(df_random)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="数据导入与导出-CSV-Excel-等"><a href="#数据导入与导出-CSV-Excel-等" class="headerlink" title="数据导入与导出 (CSV, Excel 等)"></a>数据导入与导出 (CSV, Excel 等)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写入 CSV</span></span><br><span class="line"><span class="comment"># df.to_csv(&#x27;my_data.csv&#x27;, index=False) # index=False 避免写入行索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 CSV</span></span><br><span class="line"><span class="comment"># df_from_csv = pd.read_csv(&#x27;my_data.csv&#x27;)</span></span><br><span class="line"><span class="comment"># print(df_from_csv.head())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入 Excel</span></span><br><span class="line"><span class="comment"># df.to_excel(&#x27;my_data.xlsx&#x27;, sheet_name=&#x27;Sheet1&#x27;, index=False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取 Excel</span></span><br><span class="line"><span class="comment"># df_from_excel = pd.read_excel(&#x27;my_data.xlsx&#x27;, sheet_name=&#x27;Sheet1&#x27;)</span></span><br><span class="line"><span class="comment"># print(df_from_excel.info())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他格式: read_json, read_sql, read_html, etc.</span></span><br></pre></td></tr></table></figure>

<h4 id="数据查看与检测"><a href="#数据查看与检测" class="headerlink" title="数据查看与检测"></a>数据查看与检测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df_random.head())      <span class="comment"># 查看前几行 (默认5行)</span></span><br><span class="line"><span class="built_in">print</span>(df_random.tail(<span class="number">3</span>))     <span class="comment"># 查看后3行</span></span><br><span class="line"><span class="built_in">print</span>(df_random.index)       <span class="comment"># 查看行索引</span></span><br><span class="line"><span class="built_in">print</span>(df_random.columns)     <span class="comment"># 查看列名</span></span><br><span class="line"><span class="built_in">print</span>(df_random.dtypes)      <span class="comment"># 查看每列的数据类型</span></span><br><span class="line"><span class="built_in">print</span>(df_random.describe())  <span class="comment"># 生成描述性统计 (仅对数值列)</span></span><br><span class="line"><span class="built_in">print</span>(df_random.info())      <span class="comment"># DataFrame 的简要摘要 (包括内存使用、非空值等)</span></span><br><span class="line"><span class="built_in">print</span>(df_random.shape)       <span class="comment"># (行数, 列数)</span></span><br><span class="line"><span class="built_in">print</span>(df_random.T)           <span class="comment"># 转置 DataFrame</span></span><br><span class="line"><span class="built_in">print</span>(df_random.sort_values(by=<span class="string">&#x27;B&#x27;</span>, ascending=<span class="literal">False</span>)) <span class="comment"># 按 &#x27;B&#x27; 列降序排序</span></span><br></pre></td></tr></table></figure>

<h4 id="数据选择与索引-loc-iloc"><a href="#数据选择与索引-loc-iloc" class="headerlink" title="数据选择与索引 (loc, iloc)"></a>数据选择与索引 (loc, iloc)</h4><ul>
<li><strong>选择列</strong>: <code>df[&#39;column_name&#39;]</code> (返回 Series) 或 <code>df[[&#39;col1&#39;, &#39;col2&#39;]]</code> (返回 DataFrame)。</li>
<li><strong><code>loc</code> (基于标签的索引)</strong>:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df_random.loc[dates[<span class="number">0</span>]])             <span class="comment"># 选择第一行</span></span><br><span class="line"><span class="built_in">print</span>(df_random.loc[:, [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>]])      <span class="comment"># 选择所有行的 A, B 列</span></span><br><span class="line"><span class="built_in">print</span>(df_random.loc[dates[<span class="number">0</span>:<span class="number">3</span>], [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;C&#x27;</span>]]) <span class="comment"># 选择特定行和列</span></span><br><span class="line"><span class="built_in">print</span>(df_random.loc[dates[<span class="number">0</span>], <span class="string">&#x27;A&#x27;</span>])        <span class="comment"># 选择特定单元格的值</span></span><br><span class="line"><span class="built_in">print</span>(df_random.loc[df_random[<span class="string">&#x27;A&#x27;</span>] &gt; <span class="number">0</span>]) <span class="comment"># 基于条件的行选择</span></span><br></pre></td></tr></table></figure></li>
<li><strong><code>iloc</code> (基于整数位置的索引)</strong>:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df_random.iloc[<span class="number">0</span>])       <span class="comment"># 选择第一行 (类似 df.loc[dates[0]])</span></span><br><span class="line"><span class="built_in">print</span>(df_random.iloc[<span class="number">0</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])  <span class="comment"># 选择前两行和前两列</span></span><br><span class="line"><span class="built_in">print</span>(df_random.iloc[[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">0</span>, <span class="number">3</span>]]) <span class="comment"># 选择特定位置的行和列</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="数据清洗-处理缺失值、重复值"><a href="#数据清洗-处理缺失值、重复值" class="headerlink" title="数据清洗 (处理缺失值、重复值)"></a>数据清洗 (处理缺失值、重复值)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个带缺失值的 DataFrame</span></span><br><span class="line">data_missing = &#123;<span class="string">&#x27;A&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, np.nan, <span class="number">4</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="number">5</span>, np.nan, np.nan, <span class="number">8</span>], <span class="string">&#x27;C&#x27;</span>: [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;x&#x27;</span>]&#125;</span><br><span class="line">df_miss = pd.DataFrame(data_missing)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df_miss.isnull().<span class="built_in">sum</span>()) <span class="comment"># 统计每列的缺失值数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除包含缺失值的行或列</span></span><br><span class="line"><span class="comment"># print(df_miss.dropna(how=&#x27;any&#x27;)) # 删除任何包含 NaN 的行</span></span><br><span class="line"><span class="comment"># print(df_miss.dropna(axis=1, how=&#x27;all&#x27;)) # 删除所有值都为 NaN 的列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充缺失值</span></span><br><span class="line"><span class="comment"># print(df_miss.fillna(value=0)) # 用0填充所有 NaN</span></span><br><span class="line"><span class="comment"># print(df_miss[&#x27;B&#x27;].fillna(df_miss[&#x27;B&#x27;].mean())) # 用列均值填充 B 列的 NaN</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理重复值</span></span><br><span class="line"><span class="built_in">print</span>(df_miss.duplicated(<span class="string">&#x27;C&#x27;</span>)) <span class="comment"># 检查 &#x27;C&#x27; 列是否有重复</span></span><br><span class="line"><span class="comment"># print(df_miss.drop_duplicates([&#x27;C&#x27;], keep=&#x27;first&#x27;)) # 删除 &#x27;C&#x27; 列的重复行，保留第一个</span></span><br></pre></td></tr></table></figure>

<h4 id="数据转换与操作-apply-map"><a href="#数据转换与操作-apply-map" class="headerlink" title="数据转换与操作 (apply, map)"></a>数据转换与操作 (apply, map)</h4><ul>
<li><strong><code>apply(func, axis=0)</code></strong>: 将函数 <code>func</code> 应用于 DataFrame 的行 (<code>axis=1</code>) 或列 (<code>axis=0</code>)。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df_random.apply(np.cumsum)) <span class="comment"># 沿列累加</span></span><br><span class="line"><span class="built_in">print</span>(df_random.apply(<span class="keyword">lambda</span> x: x.<span class="built_in">max</span>() - x.<span class="built_in">min</span>(), axis=<span class="number">1</span>)) <span class="comment"># 计算每行的极差</span></span><br></pre></td></tr></table></figure></li>
<li><strong><code>map(func)</code> (用于 Series)</strong>: 将函数应用于 Series 的每个元素。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series([<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, np.nan, <span class="string">&#x27;rabbit&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(s.<span class="built_in">map</span>(&#123;<span class="string">&#x27;cat&#x27;</span>: <span class="string">&#x27;kitten&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>: <span class="string">&#x27;puppy&#x27;</span>&#125;)) <span class="comment"># 使用字典映射</span></span><br><span class="line"><span class="built_in">print</span>(s.<span class="built_in">map</span>(<span class="string">&#x27;I am a &#123;&#125;&#x27;</span>.<span class="built_in">format</span>, na_action=<span class="string">&#x27;ignore&#x27;</span>)) <span class="comment"># 格式化字符串，忽略 NaN</span></span><br></pre></td></tr></table></figure></li>
<li><strong><code>applymap(func)</code> (用于 DataFrame)</strong>: 将函数应用于 DataFrame 的每个元素。</li>
</ul>
<h4 id="分组与聚合-groupby"><a href="#分组与聚合-groupby" class="headerlink" title="分组与聚合 (groupby)"></a>分组与聚合 (groupby)</h4><p>类似于 SQL 中的 <code>GROUP BY</code> 操作。通常分为“分割-应用-合并” (Split-Apply-Combine) 步骤。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data_group = &#123;<span class="string">&#x27;Team&#x27;</span>: [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>],</span><br><span class="line">              <span class="string">&#x27;Player&#x27;</span>: [<span class="string">&#x27;P1&#x27;</span>, <span class="string">&#x27;P2&#x27;</span>, <span class="string">&#x27;P3&#x27;</span>, <span class="string">&#x27;P4&#x27;</span>, <span class="string">&#x27;P5&#x27;</span>, <span class="string">&#x27;P6&#x27;</span>],</span><br><span class="line">              <span class="string">&#x27;Score&#x27;</span>: [<span class="number">10</span>, <span class="number">15</span>, <span class="number">12</span>, <span class="number">20</span>, <span class="number">13</span>, <span class="number">18</span>],</span><br><span class="line">              <span class="string">&#x27;Assists&#x27;</span>: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">5</span>]&#125;</span><br><span class="line">df_group = pd.DataFrame(data_group)</span><br><span class="line"></span><br><span class="line">grouped_by_team = df_group.groupby(<span class="string">&#x27;Team&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(grouped_by_team.mean())                 <span class="comment"># 计算每队的均值</span></span><br><span class="line"><span class="built_in">print</span>(grouped_by_team[<span class="string">&#x27;Score&#x27;</span>].<span class="built_in">sum</span>())         <span class="comment"># 计算每队的分数总和</span></span><br><span class="line"><span class="built_in">print</span>(grouped_by_team.agg(&#123;<span class="string">&#x27;Score&#x27;</span>: [<span class="string">&#x27;sum&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>], <span class="string">&#x27;Assists&#x27;</span>: <span class="string">&#x27;max&#x27;</span>&#125;)) <span class="comment"># 对不同列应用不同聚合</span></span><br></pre></td></tr></table></figure>

<h4 id="合并与连接-merge-concat-join"><a href="#合并与连接-merge-concat-join" class="headerlink" title="合并与连接 (merge, concat, join)"></a>合并与连接 (merge, concat, join)</h4><ul>
<li><strong><code>pd.concat(objs, axis=0)</code></strong>: 沿指定轴连接多个 Pandas 对象 (Series, DataFrame)。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df1 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A0&#x27;</span>, <span class="string">&#x27;A1&#x27;</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B0&#x27;</span>, <span class="string">&#x27;B1&#x27;</span>]&#125;)</span><br><span class="line">df2 = pd.DataFrame(&#123;<span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A2&#x27;</span>, <span class="string">&#x27;A3&#x27;</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B2&#x27;</span>, <span class="string">&#x27;B3&#x27;</span>]&#125;)</span><br><span class="line">concatenated_df = pd.concat([df1, df2]) <span class="comment"># 默认 axis=0 (行连接)</span></span><br><span class="line"><span class="built_in">print</span>(concatenated_df)</span><br></pre></td></tr></table></figure></li>
<li><strong><code>pd.merge(left, right, how=&#39;inner&#39;, on=None, left_on=None, right_on=None, ...)</code></strong>: 类似于 SQL 的 JOIN 操作。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">left = pd.DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K2&#x27;</span>], <span class="string">&#x27;A&#x27;</span>: [<span class="string">&#x27;A0&#x27;</span>, <span class="string">&#x27;A1&#x27;</span>, <span class="string">&#x27;A2&#x27;</span>]&#125;)</span><br><span class="line">right = pd.DataFrame(&#123;<span class="string">&#x27;key&#x27;</span>: [<span class="string">&#x27;K0&#x27;</span>, <span class="string">&#x27;K1&#x27;</span>, <span class="string">&#x27;K3&#x27;</span>], <span class="string">&#x27;B&#x27;</span>: [<span class="string">&#x27;B0&#x27;</span>, <span class="string">&#x27;B1&#x27;</span>, <span class="string">&#x27;B3&#x27;</span>]&#125;)</span><br><span class="line">merged_df = pd.merge(left, right, on=<span class="string">&#x27;key&#x27;</span>, how=<span class="string">&#x27;outer&#x27;</span>) <span class="comment"># 外连接</span></span><br><span class="line"><span class="built_in">print</span>(merged_df)</span><br></pre></td></tr></table></figure></li>
<li><strong><code>DataFrame.join(other, on=None, how=&#39;left&#39;, ...)</code></strong>: 基于索引或指定列连接。</li>
</ul>
<h3 id="2-3-Matplotlib-与-Seaborn-数据可视化-1"><a href="#2-3-Matplotlib-与-Seaborn-数据可视化-1" class="headerlink" title="2.3. Matplotlib 与 Seaborn (数据可视化)"></a>2.3. Matplotlib 与 Seaborn (数据可视化)</h3><p>数据可视化是理解数据模式、趋势和异常的关键。</p>
<h4 id="Matplotlib-基础"><a href="#Matplotlib-基础" class="headerlink" title="Matplotlib 基础"></a>Matplotlib 基础</h4><p>Matplotlib 是 Python 的核心绘图库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单线图</span></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>)) <span class="comment"># 设置图像大小</span></span><br><span class="line">plt.plot(x, y, label=<span class="string">&#x27;sin(x)&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Sine Wave&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X-axis&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y-axis&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">x_scatter = np.random.rand(<span class="number">50</span>)</span><br><span class="line">y_scatter = np.random.rand(<span class="number">50</span>)</span><br><span class="line">colors = np.random.rand(<span class="number">50</span>)</span><br><span class="line">sizes = <span class="number">1000</span> * np.random.rand(<span class="number">50</span>)</span><br><span class="line">plt.scatter(x_scatter, y_scatter, c=colors, s=sizes, alpha=<span class="number">0.7</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.colorbar() <span class="comment"># 显示颜色条</span></span><br><span class="line">plt.title(<span class="string">&#x27;Scatter Plot&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直方图</span></span><br><span class="line">data_hist = np.random.randn(<span class="number">1000</span>)</span><br><span class="line">plt.hist(data_hist, bins=<span class="number">30</span>, edgecolor=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Histogram&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图</span></span><br><span class="line">fig, axs = plt.subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>)) <span class="comment"># 2行1列的子图</span></span><br><span class="line">axs[<span class="number">0</span>].plot(x, y, <span class="string">&#x27;b-&#x27;</span>)</span><br><span class="line">axs[<span class="number">0</span>].set_title(<span class="string">&#x27;First Subplot (Sine)&#x27;</span>)</span><br><span class="line">axs[<span class="number">1</span>].plot(x, np.cos(x), <span class="string">&#x27;g--&#x27;</span>)</span><br><span class="line">axs[<span class="number">1</span>].set_title(<span class="string">&#x27;Second Subplot (Cosine)&#x27;</span>)</span><br><span class="line">plt.tight_layout() <span class="comment"># 调整子图间距</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h4 id="Seaborn-统计可视化"><a href="#Seaborn-统计可视化" class="headerlink" title="Seaborn 统计可视化"></a>Seaborn 统计可视化</h4><p>Seaborn 基于 Matplotlib，提供了更高级的接口来绘制引人入胜的统计图形。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载示例数据集 (Pandas DataFrame)</span></span><br><span class="line">tips = sns.load_dataset(<span class="string">&#x27;tips&#x27;</span>) <span class="comment"># 内置数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 散点图 (带回归线)</span></span><br><span class="line">sns.lmplot(x=<span class="string">&#x27;total_bill&#x27;</span>, y=<span class="string">&#x27;tip&#x27;</span>, data=tips, hue=<span class="string">&#x27;smoker&#x27;</span>, markers=[<span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;x&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Tip vs Total Bill by Smoker&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分布图 (直方图 + KDE)</span></span><br><span class="line">sns.histplot(tips[<span class="string">&#x27;total_bill&#x27;</span>], kde=<span class="literal">True</span>, bins=<span class="number">20</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of Total Bill&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 箱线图</span></span><br><span class="line">sns.boxplot(x=<span class="string">&#x27;day&#x27;</span>, y=<span class="string">&#x27;total_bill&#x27;</span>, data=tips, hue=<span class="string">&#x27;time&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Total Bill by Day and Time&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 热力图</span></span><br><span class="line">correlation_matrix = tips[[<span class="string">&#x27;total_bill&#x27;</span>, <span class="string">&#x27;tip&#x27;</span>, <span class="string">&#x27;size&#x27;</span>]].corr()</span><br><span class="line">sns.heatmap(correlation_matrix, annot=<span class="literal">True</span>, cmap=<span class="string">&#x27;coolwarm&#x27;</span>, fmt=<span class="string">&quot;.2f&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Correlation Heatmap&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配对图 (Pair Plot)</span></span><br><span class="line"><span class="comment"># sns.pairplot(tips, hue=&#x27;sex&#x27;)</span></span><br><span class="line"><span class="comment"># plt.suptitle(&#x27;Pair Plot of Tips Dataset&#x27;, y=1.02)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>

<h2 id="3-Scikit-learn-简介与核心概念-1"><a href="#3-Scikit-learn-简介与核心概念-1" class="headerlink" title="3. Scikit-learn 简介与核心概念"></a>3. Scikit-learn 简介与核心概念</h2><h3 id="Scikit-learn-是什么"><a href="#Scikit-learn-是什么" class="headerlink" title="Scikit-learn 是什么?"></a>Scikit-learn 是什么?</h3><p>Scikit-learn (通常简写为 <code>sklearn</code>) 是 Python 中一个功能强大且广泛使用的开源机器学习库。它提供了简单高效的工具，用于数据挖掘和数据分析，构建在 NumPy, SciPy 和 Matplotlib 之上。</p>
<h3 id="设计哲学与-API-一致性"><a href="#设计哲学与-API-一致性" class="headerlink" title="设计哲学与 API 一致性"></a>设计哲学与 API 一致性</h3><p>Scikit-learn 的设计注重易用性、高性能、良好文档和一致的 API。</p>
<ul>
<li><strong>一致性</strong>: 所有对象共享一个通用、简洁的接口。</li>
<li><strong>检验</strong>: 所有算法参数和输入数据都经过严格检验。</li>
<li><strong>组合性</strong>: 许多机器学习任务可以表示为基本算法的序列或组合，Scikit-learn 通过 Pipeline 机制很好地支持了这一点。</li>
<li><strong>合理的默认值</strong>: 大多数参数都有合理的默认值，使得初学者可以快速上手。</li>
</ul>
<h3 id="核心对象类型"><a href="#核心对象类型" class="headerlink" title="核心对象类型"></a>核心对象类型</h3><h4 id="Estimator-估计器"><a href="#Estimator-估计器" class="headerlink" title="Estimator (估计器)"></a>Estimator (估计器)</h4><p>任何可以从数据中学习一些参数的对象都被称为估计器。例如，<code>KMeans</code> 是一个聚类估计器，<code>LinearRegression</code> 是一个回归估计器。</p>
<ul>
<li>估计本身是通过 <code>fit()</code> 方法执行的，它接受一个数据集 (通常是 2D 数组 <code>X</code>) 作为参数，对于监督学习还会接受标签或目标值 <code>y</code>。</li>
<li>所有估计器的参数都可以在实例化时设置，或者在实例化后通过其属性直接修改。</li>
</ul>
<h4 id="Transformer-转换器"><a href="#Transformer-转换器" class="headerlink" title="Transformer (转换器)"></a>Transformer (转换器)</h4><p>一种特殊类型的估计器，可以转换输入数据。例如，<code>StandardScaler</code> 用于特征标准化，<code>PCA</code> 用于降维。</p>
<ul>
<li>除了 <code>fit()</code> 方法 (用于学习转换所需的参数，如均值和标准差)，转换器还有一个 <code>transform()</code> 方法，用于将学习到的转换应用于新数据。</li>
<li>通常还有一个便捷的 <code>fit_transform()</code> 方法，它等同于先调用 <code>fit()</code> 然后调用 <code>transform()</code>。</li>
</ul>
<h4 id="Predictor-预测器"><a href="#Predictor-预测器" class="headerlink" title="Predictor (预测器)"></a>Predictor (预测器)</h4><p>一种特殊类型的估计器，可以基于学习到的模型对新数据点进行预测。</p>
<ul>
<li>除了 <code>fit()</code>，预测器还有一个 <code>predict()</code> 方法，它接受新数据 <code>X_test</code> 并返回预测值 <code>y_pred</code>。</li>
<li>还有一个 <code>score()</code> 方法，用于评估模型在给定测试数据和标签上的性能 (对于分类返回准确率，对于回归返回 R² 分数)。</li>
</ul>
<h3 id="基本-API-方法-fit-transform-predict-score"><a href="#基本-API-方法-fit-transform-predict-score" class="headerlink" title="基本 API 方法: fit(), transform(), predict(), score()"></a>基本 API 方法: <code>fit()</code>, <code>transform()</code>, <code>predict()</code>, <code>score()</code></h3><ul>
<li><code>estimator.fit(X, y=None)</code>: 从训练数据 <code>X</code> (特征) 和 <code>y</code> (目标，监督学习中) 中学习模型参数。返回估计器本身 (<code>self</code>)，允许链式调用。</li>
<li><code>transformer.transform(X)</code>: 应用学习到的转换到数据 <code>X</code>。</li>
<li><code>transformer.fit_transform(X, y=None)</code>: 学习转换参数并应用转换。</li>
<li><code>predictor.predict(X_test)</code>: 对新数据 <code>X_test</code> 进行预测。</li>
<li><code>predictor.score(X_test, y_test)</code>: 评估预测器在测试数据上的性能。</li>
</ul>
<h2 id="4-数据预处理-Preprocessing-1"><a href="#4-数据预处理-Preprocessing-1" class="headerlink" title="4. 数据预处理 (Preprocessing)"></a>4. 数据预处理 (Preprocessing)</h2><p>数据预处理是机器学习流程中至关重要的一步，它将原始数据转换为适合机器学习模型的格式。Scikit-learn 的 <code>sklearn.preprocessing</code> 模块提供了多种工具。</p>
<h3 id="4-1-标准化与归一化-Scaling-1"><a href="#4-1-标准化与归一化-Scaling-1" class="headerlink" title="4.1. 标准化与归一化 (Scaling)"></a>4.1. 标准化与归一化 (Scaling)</h3><p>许多机器学习算法对特征的尺度敏感。</p>
<h4 id="StandardScaler"><a href="#StandardScaler" class="headerlink" title="StandardScaler"></a><code>StandardScaler</code></h4><p>通过移除均值并缩放到单位方差来进行标准化 (Z-score normalization)。<br>$z &#x3D; (x - \mu) &#x2F; \sigma$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">data = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">scaled_data = scaler.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(scaled_data)</span><br><span class="line"><span class="comment"># [[-1. -1.]</span></span><br><span class="line"><span class="comment">#  [-1. -1.]</span></span><br><span class="line"><span class="comment">#  [ 1.  1.]</span></span><br><span class="line"><span class="comment">#  [ 1.  1.]]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean: <span class="subst">&#123;scaler.mean_&#125;</span>, Scale (std): <span class="subst">&#123;scaler.scale_&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="MinMaxScaler"><a href="#MinMaxScaler" class="headerlink" title="MinMaxScaler"></a><code>MinMaxScaler</code></h4><p>将特征缩放到给定的最小值和最大值之间 (通常是 [0, 1])。<br>$X_{scaled} &#x3D; (X - X_{min}) &#x2F; (X_{max} - X_{min})$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">data = np.array([[-<span class="number">1</span>, <span class="number">2</span>], [-<span class="number">0.5</span>, <span class="number">6</span>], [<span class="number">0</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">18</span>]])</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">scaled_data = scaler.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(scaled_data)</span><br></pre></td></tr></table></figure>

<h4 id="RobustScaler"><a href="#RobustScaler" class="headerlink" title="RobustScaler"></a><code>RobustScaler</code></h4><p>使用对异常值更鲁棒的统计量 (如中位数和四分位数范围) 进行缩放。<br>适用于包含许多异常值的数据集。</p>
<h3 id="4-2-编码分类特征-Encoding-Categorical-Features-1"><a href="#4-2-编码分类特征-Encoding-Categorical-Features-1" class="headerlink" title="4.2. 编码分类特征 (Encoding Categorical Features)"></a>4.2. 编码分类特征 (Encoding Categorical Features)</h3><p>机器学习算法通常需要数值输入。</p>
<h4 id="LabelEncoder"><a href="#LabelEncoder" class="headerlink" title="LabelEncoder"></a><code>LabelEncoder</code></h4><p>将分类标签 (通常是目标变量 <code>y</code>) 编码为 0 到 n_classes-1 之间的数值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">labels = [<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;mouse&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>]</span><br><span class="line">encoder = LabelEncoder()</span><br><span class="line">encoded_labels = encoder.fit_transform(labels)</span><br><span class="line"><span class="built_in">print</span>(encoded_labels) <span class="comment"># [0 1 0 2 1] (cat=0, dog=1, mouse=2)</span></span><br><span class="line"><span class="built_in">print</span>(encoder.classes_) <span class="comment"># [&#x27;cat&#x27; &#x27;dog&#x27; &#x27;mouse&#x27;]</span></span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>: <code>LabelEncoder</code> 通常不推荐用于特征 <code>X</code>，除非是序数特征，因为它引入了人为的顺序关系。</p>
<h4 id="OneHotEncoder"><a href="#OneHotEncoder" class="headerlink" title="OneHotEncoder"></a><code>OneHotEncoder</code></h4><p>将具有 k 个可能值的分类特征转换为 k 个二元特征 (0 或 1)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="comment"># 假设特征是一个二维数组，每个内层数组是一个样本的特征</span></span><br><span class="line"><span class="comment"># 注意：OneHotEncoder 期望输入是二维的</span></span><br><span class="line">features = [[<span class="string">&#x27;Male&#x27;</span>, <span class="number">1</span>], [<span class="string">&#x27;Female&#x27;</span>, <span class="number">3</span>], [<span class="string">&#x27;Female&#x27;</span>, <span class="number">2</span>]]</span><br><span class="line"><span class="comment"># 先用 OrdinalEncoder 或手动将类别转为数字（如果需要，或直接处理字符串）</span></span><br><span class="line"><span class="comment"># OneHotEncoder可以直接处理字符串类别（如果设置 categories=&#x27;auto&#x27; 或提供类别列表）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：对DataFrame中的列进行OneHotEncoding</span></span><br><span class="line">df_cat = pd.DataFrame(&#123;<span class="string">&#x27;gender&#x27;</span>: [<span class="string">&#x27;Male&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>], <span class="string">&#x27;city&#x27;</span>: [<span class="string">&#x27;NY&#x27;</span>, <span class="string">&#x27;SF&#x27;</span>, <span class="string">&#x27;NY&#x27;</span>]&#125;)</span><br><span class="line">encoder_ohe = OneHotEncoder(sparse_output=<span class="literal">False</span>) <span class="comment"># sparse_output=False 返回密集数组</span></span><br><span class="line">encoded_features = encoder_ohe.fit_transform(df_cat[[<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;city&#x27;</span>]])</span><br><span class="line"><span class="built_in">print</span>(encoded_features)</span><br><span class="line"><span class="built_in">print</span>(encoder_ohe.get_feature_names_out([<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;city&#x27;</span>]))</span><br><span class="line"><span class="comment"># [[1. 0. 1. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 1.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 1. 0.]]</span></span><br><span class="line"><span class="comment"># [&#x27;gender_Female&#x27; &#x27;gender_Male&#x27; &#x27;city_NY&#x27; &#x27;city_SF&#x27;]</span></span><br></pre></td></tr></table></figure>

<p><code>OneHotEncoder</code> 可以通过 <code>handle_unknown=&#39;ignore&#39;</code> 来处理测试集中出现的新类别，或者 <code>drop=&#39;first&#39;</code> 来避免多重共线性。</p>
<h4 id="Pandas-get-dummies"><a href="#Pandas-get-dummies" class="headerlink" title="Pandas get_dummies()"></a>Pandas <code>get_dummies()</code></h4><p>Pandas 也提供了方便的独热编码方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_cat = pd.DataFrame(&#123;<span class="string">&#x27;gender&#x27;</span>: [<span class="string">&#x27;Male&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>, <span class="string">&#x27;Female&#x27;</span>], <span class="string">&#x27;city&#x27;</span>: [<span class="string">&#x27;NY&#x27;</span>, <span class="string">&#x27;SF&#x27;</span>, <span class="string">&#x27;NY&#x27;</span>]&#125;)</span><br><span class="line">dummies_df = pd.get_dummies(df_cat, columns=[<span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;city&#x27;</span>], drop_first=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(dummies_df)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-处理缺失值-Handling-Missing-Values-1"><a href="#4-3-处理缺失值-Handling-Missing-Values-1" class="headerlink" title="4.3. 处理缺失值 (Handling Missing Values)"></a>4.3. 处理缺失值 (Handling Missing Values)</h3><h4 id="SimpleImputer"><a href="#SimpleImputer" class="headerlink" title="SimpleImputer"></a><code>SimpleImputer</code></h4><p>用提供的策略（均值、中位数、众数或常量）填充缺失值 (NaN)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line">data_missing = np.array([[<span class="number">1</span>, <span class="number">2</span>, np.nan], [<span class="number">4</span>, np.nan, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">imputer_mean = SimpleImputer(strategy=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">imputed_data = imputer_mean.fit_transform(data_missing)</span><br><span class="line"><span class="built_in">print</span>(imputed_data)</span><br></pre></td></tr></table></figure>

<h4 id="KNNImputer-更高级"><a href="#KNNImputer-更高级" class="headerlink" title="KNNImputer (更高级)"></a>KNNImputer (更高级)</h4><p>使用 K 最近邻方法来填充缺失值。每个样本的缺失值都是使用在训练集中找到的 n_neighbors 个最近邻居的值进行估算的。</p>
<h3 id="4-4-特征离散化-分箱-Discretization-Binning-1"><a href="#4-4-特征离散化-分箱-Discretization-Binning-1" class="headerlink" title="4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)"></a>4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)</h3><p>将连续特征划分为离散的区间（箱子）。</p>
<h4 id="KBinsDiscretizer"><a href="#KBinsDiscretizer" class="headerlink" title="KBinsDiscretizer"></a><code>KBinsDiscretizer</code></h4><p>将连续数据特征分箱。支持多种分箱策略 (<code>uniform</code>, <code>quantile</code>, <code>kmeans</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line">X = np.array([[ -<span class="number">2</span>, <span class="number">1</span>, -<span class="number">4</span>,   -<span class="number">1</span>],</span><br><span class="line">              [ -<span class="number">1</span>, <span class="number">2</span>, -<span class="number">3</span>, -<span class="number">0.5</span>],</span><br><span class="line">              [  <span class="number">0</span>, <span class="number">3</span>, -<span class="number">2</span>,  <span class="number">0.5</span>],</span><br><span class="line">              [  <span class="number">1</span>, <span class="number">4</span>, -<span class="number">1</span>,    <span class="number">2</span>]])</span><br><span class="line">discretizer = KBinsDiscretizer(n_bins=<span class="number">3</span>, encode=<span class="string">&#x27;ordinal&#x27;</span>, strategy=<span class="string">&#x27;quantile&#x27;</span>)</span><br><span class="line">X_binned = discretizer.fit_transform(X)</span><br><span class="line"><span class="built_in">print</span>(X_binned) <span class="comment"># [[0. 0. 0. 0.] [1. 1. 1. 1.] [2. 2. 2. 2.] [2. 2. 2. 2.]]</span></span><br></pre></td></tr></table></figure>

<h3 id="4-5-生成多项式特征-Polynomial-Features-1"><a href="#4-5-生成多项式特征-Polynomial-Features-1" class="headerlink" title="4.5. 生成多项式特征 (Polynomial Features)"></a>4.5. 生成多项式特征 (Polynomial Features)</h3><p>通过组合现有特征生成新的多项式特征和交互特征，可以帮助线性模型捕捉非线性关系。</p>
<h4 id="PolynomialFeatures"><a href="#PolynomialFeatures" class="headerlink" title="PolynomialFeatures"></a><code>PolynomialFeatures</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">X = np.arange(<span class="number">6</span>).reshape(<span class="number">3</span>, <span class="number">2</span>) <span class="comment"># [[0 1] [2 3] [4 5]]</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>) <span class="comment"># degree=2 表示最高2次</span></span><br><span class="line"><span class="comment"># include_bias=False 默认是True,会添加一列1作为偏置项</span></span><br><span class="line">X_poly = poly.fit_transform(X)</span><br><span class="line"><span class="built_in">print</span>(X_poly)</span><br><span class="line"><span class="comment"># 假设X有特征 [x1, x2]</span></span><br><span class="line"><span class="comment"># degree=2 会生成 [x1, x2, x1^2, x1*x2, x2^2]</span></span><br><span class="line"><span class="comment"># [[ 0.  1.  0.  0.  1.]</span></span><br><span class="line"><span class="comment">#  [ 2.  3.  4.  6.  9.]</span></span><br><span class="line"><span class="comment">#  [ 4.  5. 16. 20. 25.]]</span></span><br><span class="line"><span class="built_in">print</span>(poly.get_feature_names_out([<span class="string">&#x27;x1&#x27;</span>, <span class="string">&#x27;x2&#x27;</span>]))</span><br><span class="line"><span class="comment"># [&#x27;x1&#x27; &#x27;x2&#x27; &#x27;x1^2&#x27; &#x27;x1 x2&#x27; &#x27;x2^2&#x27;]</span></span><br></pre></td></tr></table></figure>

<h3 id="4-6-自定义转换器-Custom-Transformers-1"><a href="#4-6-自定义转换器-Custom-Transformers-1" class="headerlink" title="4.6. 自定义转换器 (Custom Transformers)"></a>4.6. 自定义转换器 (Custom Transformers)</h3><p>通过继承 <code>TransformerMixin</code> 和 <code>BaseEstimator</code> (或使用 <code>FunctionTransformer</code>) 可以创建自己的转换器，以便与 Scikit-learn Pipeline 集成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyFeatureAdder</span>(BaseEstimator, TransformerMixin):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, feature_index1=<span class="number">0</span>, feature_index2=<span class="number">1</span></span>): <span class="comment"># 无实际参数，只是示例</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_index1 = feature_index1</span><br><span class="line">        <span class="variable language_">self</span>.feature_index2 = feature_index2</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span> <span class="comment"># 无需学习</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 假设 X 是 NumPy 数组</span></span><br><span class="line">        sum_feature = X[:, <span class="variable language_">self</span>.feature_index1] + X[:, <span class="variable language_">self</span>.feature_index2]</span><br><span class="line">        <span class="keyword">return</span> np.c_[X, sum_feature] <span class="comment"># 添加新特征列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="comment"># adder = MyFeatureAdder()</span></span><br><span class="line"><span class="comment"># X_extended = adder.transform(X_original)</span></span><br></pre></td></tr></table></figure>

<h2 id="5-模型选择与评估-Model-Selection-and-Evaluation-1"><a href="#5-模型选择与评估-Model-Selection-and-Evaluation-1" class="headerlink" title="5. 模型选择与评估 (Model Selection and Evaluation)"></a>5. 模型选择与评估 (Model Selection and Evaluation)</h2><p><code>sklearn.model_selection</code> 模块是模型选择和评估的关键。</p>
<h3 id="5-1-数据集划分-train-test-split-1"><a href="#5-1-数据集划分-train-test-split-1" class="headerlink" title="5.1. 数据集划分 (train_test_split)"></a>5.1. 数据集划分 (<code>train_test_split</code>)</h3><p>将数据集划分为训练集和测试集，以评估模型在未见过数据上的泛化能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 假设 X 是特征， y 是目标变量</span></span><br><span class="line"><span class="comment"># X_train, X_test, y_train, y_test = train_test_split(</span></span><br><span class="line"><span class="comment">#     X, y, test_size=0.2, random_state=42, stratify=y # stratify 用于分类，确保类别比例一致</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>test_size</code>: 测试集所占比例或绝对数量。</li>
<li><code>random_state</code>: 随机数种子，确保每次划分结果一致。</li>
<li><code>stratify</code>: 对于分类问题，确保训练集和测试集中各类别的比例与原数据集相似。</li>
</ul>
<h3 id="5-2-交叉验证-Cross-Validation-1"><a href="#5-2-交叉验证-Cross-Validation-1" class="headerlink" title="5.2. 交叉验证 (Cross-Validation)"></a>5.2. 交叉验证 (Cross-Validation)</h3><p>一种更鲁棒的模型评估技术，将训练数据多次划分为训练子集和验证子集。</p>
<h4 id="cross-val-score"><a href="#cross-val-score" class="headerlink" title="cross_val_score"></a><code>cross_val_score</code></h4><p>对估计器进行交叉验证，并返回每次运行的评分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data, iris.target</span><br><span class="line">log_reg = LogisticRegression(max_iter=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5折交叉验证</span></span><br><span class="line">scores = cross_val_score(log_reg, X, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cross-validation scores: <span class="subst">&#123;scores&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Average accuracy: <span class="subst">&#123;scores.mean():<span class="number">.2</span>f&#125;</span> (+/- <span class="subst">&#123;scores.std() * <span class="number">2</span>:<span class="number">.2</span>f&#125;</span>)&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><code>scoring</code> 参数可以指定不同的评估指标。</p>
<h4 id="KFold-StratifiedKFold-等策略"><a href="#KFold-StratifiedKFold-等策略" class="headerlink" title="KFold, StratifiedKFold 等策略"></a>KFold, StratifiedKFold 等策略</h4><p><code>cv</code> 参数可以接受不同的交叉验证策略对象，如：</p>
<ul>
<li><code>KFold(n_splits=5, shuffle=True, random_state=42)</code>: K 折交叉验证。</li>
<li><code>StratifiedKFold(n_splits=5)</code>: 分层 K 折，确保每折中类别比例与整体一致，适用于分类。</li>
<li><code>LeaveOneOut()</code>: 留一法交叉验证。</li>
</ul>
<h3 id="5-3-模型评估指标-Metrics-1"><a href="#5-3-模型评估指标-Metrics-1" class="headerlink" title="5.3. 模型评估指标 (Metrics)"></a>5.3. 模型评估指标 (Metrics)</h3><p><code>sklearn.metrics</code> 模块提供了大量用于评估模型性能的函数。</p>
<h4 id="分类指标"><a href="#分类指标" class="headerlink" title="分类指标"></a>分类指标</h4><ul>
<li><strong>准确率 (<code>accuracy_score</code>)</strong>: 正确分类的样本比例。对不平衡数据集可能产生误导。</li>
<li><strong>精确率 (<code>precision_score</code>)</strong>: $TP &#x2F; (TP + FP)$。在所有被预测为正类的样本中，实际为正类的比例。</li>
<li><strong>召回率&#x2F;真正类率 (<code>recall_score</code>)</strong>: $TP &#x2F; (TP + FN)$。在所有实际为正类的样本中，被正确预测为正类的比例。</li>
<li><strong>F1 分数 (<code>f1_score</code>)</strong>: 精确率和召回率的调和平均数。$2 \cdot (Precision \cdot Recall) &#x2F; (Precision + Recall)$。</li>
<li><strong>混淆矩阵 (<code>confusion_matrix</code>)</strong>: $C_{i,j}$ 是真实类别为 $i$ 但被预测为类别 $j$ 的样本数量。</li>
<li><strong>分类报告 (<code>classification_report</code>)</strong>: 显示主要分类指标的文本报告。</li>
<li><strong>ROC 曲线与 AUC (<code>roc_curve</code>, <code>roc_auc_score</code>)</strong>:<ul>
<li>ROC 曲线 (Receiver Operating Characteristic Curve) 以不同阈值下的假正类率 (FPR) 为横轴，真正类率 (TPR) 为纵轴。</li>
<li>AUC (Area Under the ROC Curve) 是 ROC 曲线下的面积，值越接近 1 模型性能越好。</li>
</ul>
</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score</span><br><span class="line"><span class="comment"># 假设 y_true 是真实标签, y_pred 是模型预测标签, y_proba 是模型预测概率 (用于ROC AUC)</span></span><br><span class="line"><span class="comment"># accuracy = accuracy_score(y_true, y_pred)</span></span><br><span class="line"><span class="comment"># print(classification_report(y_true, y_pred))</span></span><br><span class="line"><span class="comment"># cm = confusion_matrix(y_true, y_pred)</span></span><br><span class="line"><span class="comment"># sns.heatmap(cm, annot=True, fmt=&#x27;d&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment"># auc = roc_auc_score(y_true, y_proba_positive_class) # 需要正类的概率</span></span><br></pre></td></tr></table></figure>

<h4 id="回归指标"><a href="#回归指标" class="headerlink" title="回归指标"></a>回归指标</h4><ul>
<li><strong>平均绝对误差 (MAE - <code>mean_absolute_error</code>)</strong>: $|y_{true} - y_{pred}|$ 的平均值。</li>
<li><strong>均方误差 (MSE - <code>mean_squared_error</code>)</strong>: $(y_{true} - y_{pred})^2$ 的平均值。</li>
<li><strong>均方根误差 (RMSE)</strong>: MSE 的平方根。Scikit-learn 没有直接的 RMSE 函数，但可以 <code>np.sqrt(mean_squared_error(...))</code>。</li>
<li><strong>R² 分数 (决定系数 - <code>r2_score</code>)</strong>: $1 - (SS_{res} &#x2F; SS_{tot})$。表示模型解释的数据方差比例。值越接近 1 越好，也可能为负。</li>
</ul>
<h4 id="聚类指标-简述"><a href="#聚类指标-简述" class="headerlink" title="聚类指标 (简述)"></a>聚类指标 (简述)</h4><p>如轮廓系数 (<code>silhouette_score</code>)、Calinski-Harabasz 指数 (<code>calinski_harabasz_score</code>)、调整兰德指数 (<code>adjusted_rand_score</code>) 等，用于评估聚类效果的好坏。</p>
<h3 id="5-4-超参数调优-Hyperparameter-Tuning-1"><a href="#5-4-超参数调优-Hyperparameter-Tuning-1" class="headerlink" title="5.4. 超参数调优 (Hyperparameter Tuning)"></a>5.4. 超参数调优 (Hyperparameter Tuning)</h3><p>超参数是模型在学习开始前设置的参数 (如正则化强度 <code>C</code>，核函数类型 <code>kernel</code>)。</p>
<h4 id="GridSearchCV"><a href="#GridSearchCV" class="headerlink" title="GridSearchCV"></a><code>GridSearchCV</code></h4><p>通过构建参数网格，穷举搜索所有参数组合，找到最佳参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义参数网格</span></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>], <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;gamma&#x27;</span>: [<span class="string">&#x27;scale&#x27;</span>, <span class="string">&#x27;auto&#x27;</span>, <span class="number">0.1</span>]&#125;</span><br><span class="line"><span class="comment"># 假设 X_train, y_train 已定义</span></span><br><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">3</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>, verbose=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># grid_search.fit(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(f&quot;Best parameters: &#123;grid_search.best_params_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Best cross-validation score: &#123;grid_search.best_score_:.2f&#125;&quot;)</span></span><br><span class="line"><span class="comment"># best_model = grid_search.best_estimator_</span></span><br></pre></td></tr></table></figure>

<h4 id="RandomizedSearchCV"><a href="#RandomizedSearchCV" class="headerlink" title="RandomizedSearchCV"></a><code>RandomizedSearchCV</code></h4><p>从参数的给定分布中随机采样一定数量的参数组合。对于高维参数空间，通常比网格搜索更高效。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> expon, uniform</span><br><span class="line"></span><br><span class="line">param_dist = &#123;<span class="string">&#x27;C&#x27;</span>: expon(scale=<span class="number">100</span>), <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>], <span class="string">&#x27;gamma&#x27;</span>: uniform(<span class="number">0.01</span>, <span class="number">0.5</span>)&#125;</span><br><span class="line"><span class="comment"># random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=10, cv=3, scoring=&#x27;accuracy&#x27;, random_state=42, verbose=1)</span></span><br><span class="line"><span class="comment"># random_search.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>

<h4 id="其他调优策略-如-Bayesian-Optimization-外部库"><a href="#其他调优策略-如-Bayesian-Optimization-外部库" class="headerlink" title="其他调优策略 (如 Bayesian Optimization - 外部库)"></a>其他调优策略 (如 Bayesian Optimization - 外部库)</h4><p>如 Hyperopt, Optuna, Scikit-Optimize 等库提供了更高级的贝叶斯优化等调优方法。</p>
<h3 id="5-5-学习曲线与验证曲线-Learning-and-Validation-Curves-1"><a href="#5-5-学习曲线与验证曲线-Learning-and-Validation-Curves-1" class="headerlink" title="5.5. 学习曲线与验证曲线 (Learning and Validation Curves)"></a>5.5. 学习曲线与验证曲线 (Learning and Validation Curves)</h3><ul>
<li><strong>学习曲线 (<code>learning_curve</code>)</strong>: 显示模型在不同训练集大小下的训练得分和验证得分。有助于判断模型是欠拟合、过拟合还是数据量不足。</li>
<li><strong>验证曲线 (<code>validation_curve</code>)</strong>: 显示模型在单个超参数不同取值下的训练得分和验证得分。有助于选择合适的超参数值。</li>
</ul>
<h2 id="6-监督学习-Supervised-Learning-1"><a href="#6-监督学习-Supervised-Learning-1" class="headerlink" title="6. 监督学习 (Supervised Learning)"></a>6. 监督学习 (Supervised Learning)</h2><p>监督学习是从标记数据 (即每个数据点都有一个已知的输出或目标) 中学习一个函数，该函数可以将新的输入映射到输出。</p>
<h3 id="6-1-回归-Regression-1"><a href="#6-1-回归-Regression-1" class="headerlink" title="6.1. 回归 (Regression)"></a>6.1. 回归 (Regression)</h3><p>预测连续值输出。</p>
<h4 id="线性回归-LinearRegression"><a href="#线性回归-LinearRegression" class="headerlink" title="线性回归 (LinearRegression)"></a>线性回归 (<code>LinearRegression</code>)</h4><p>拟合一个线性模型，通过找到最佳的系数来最小化真实值和预测值之间的残差平方和。<br>$y &#x3D; w_0 + w_1x_1 + … + w_nx_n$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># lin_reg = LinearRegression()</span></span><br><span class="line"><span class="comment"># lin_reg.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># print(f&quot;Coefficients: &#123;lin_reg.coef_&#125;, Intercept: &#123;lin_reg.intercept_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># y_pred_reg = lin_reg.predict(X_test)</span></span><br></pre></td></tr></table></figure>

<h4 id="岭回归-Ridge-与-Lasso-回归-Lasso"><a href="#岭回归-Ridge-与-Lasso-回归-Lasso" class="headerlink" title="岭回归 (Ridge) 与 Lasso 回归 (Lasso)"></a>岭回归 (<code>Ridge</code>) 与 Lasso 回归 (<code>Lasso</code>)</h4><p>都是线性回归的正则化版本，用于防止过拟合。</p>
<ul>
<li><strong>Ridge (L2 正则化)</strong>: 在损失函数中添加系数的平方和作为惩罚项。使得系数趋向于较小的值。<code>alpha</code> 控制正则化强度。</li>
<li><strong>Lasso (L1 正则化)</strong>: 在损失函数中添加系数的绝对值之和作为惩罚项。倾向于产生稀疏解 (一些系数变为零)，可以用于特征选择。<code>alpha</code> 控制正则化强度。</li>
<li><strong>ElasticNet</strong>: L1 和 L2 正则化的结合。</li>
</ul>
<h4 id="支持向量回归-SVR"><a href="#支持向量回归-SVR" class="headerlink" title="支持向量回归 (SVR)"></a>支持向量回归 (<code>SVR</code>)</h4><p>支持向量机 (SVM) 的回归版本。尝试在拟合数据的同时，保持尽可能多的点在指定的间隔 (由 <code>epsilon</code> 参数定义) 内。</p>
<h4 id="决策树回归-DecisionTreeRegressor"><a href="#决策树回归-DecisionTreeRegressor" class="headerlink" title="决策树回归 (DecisionTreeRegressor)"></a>决策树回归 (<code>DecisionTreeRegressor</code>)</h4><p>通过学习简单的决策规则 (基于特征值进行分裂) 来预测目标值。树的叶节点包含预测值 (通常是该叶节点训练样本的均值)。容易过拟合，通常通过剪枝或集成来改进。</p>
<h4 id="集成方法回归-RandomForestRegressor-GradientBoostingRegressor-AdaBoostRegressor-XGBoost-LightGBM"><a href="#集成方法回归-RandomForestRegressor-GradientBoostingRegressor-AdaBoostRegressor-XGBoost-LightGBM" class="headerlink" title="集成方法回归 (RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, XGBoost, LightGBM)"></a>集成方法回归 (RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, XGBoost, LightGBM)</h4><p>集成方法通过组合多个基学习器 (通常是决策树) 来获得更好的预测性能。</p>
<ul>
<li><strong>随机森林 (<code>RandomForestRegressor</code>)</strong>: 构建多个决策树，每棵树在数据的随机子集和特征的随机子集上训练。预测结果是所有树预测的平均值。</li>
<li><strong>梯度提升 (<code>GradientBoostingRegressor</code>)</strong>: 顺序构建树，每棵新树都试图纠正前一棵树的残差。</li>
<li><strong>AdaBoost (<code>AdaBoostRegressor</code>)</strong>: 顺序构建学习器，后续学习器更关注先前学习器错误分类的样本。</li>
<li><strong>XGBoost, LightGBM, CatBoost</strong>: Scikit-learn 之外的更高级、性能通常更好的梯度提升库 (但可以与 Scikit-learn API 兼容)。</li>
</ul>
<h3 id="6-2-分类-Classification-1"><a href="#6-2-分类-Classification-1" class="headerlink" title="6.2. 分类 (Classification)"></a>6.2. 分类 (Classification)</h3><p>预测离散类别标签。</p>
<h4 id="逻辑回归-LogisticRegression"><a href="#逻辑回归-LogisticRegression" class="headerlink" title="逻辑回归 (LogisticRegression)"></a>逻辑回归 (<code>LogisticRegression</code>)</h4><p>尽管名为“回归”，但它是一种线性分类模型。它使用 sigmoid 函数将线性组合的输出映射到 (0, 1) 之间的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># log_clf = LogisticRegression(solver=&#x27;liblinear&#x27;, random_state=42) # solver选择很重要</span></span><br><span class="line"><span class="comment"># log_clf.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_clf = log_clf.predict(X_test)</span></span><br><span class="line"><span class="comment"># y_proba_clf = log_clf.predict_proba(X_test) # 获取类别概率</span></span><br></pre></td></tr></table></figure>

<p><code>C</code> 参数是正则化强度的倒数 (较小的 <code>C</code> 意味着更强的正则化)。</p>
<h4 id="K-近邻-KNeighborsClassifier"><a href="#K-近邻-KNeighborsClassifier" class="headerlink" title="K 近邻 (KNeighborsClassifier)"></a>K 近邻 (<code>KNeighborsClassifier</code>)</h4><p>基于实例的学习。对新样本进行分类时，查找训练集中与其最近的 K 个邻居，并根据这些邻居的多数类别进行投票决定。<br>需要选择合适的 <code>k</code> 值和距离度量。特征缩放非常重要。</p>
<h4 id="支持向量机-SVC"><a href="#支持向量机-SVC" class="headerlink" title="支持向量机 (SVC)"></a>支持向量机 (<code>SVC</code>)</h4><p>寻找一个能将不同类别最大间隔分开的超平面。可以使用不同的核函数 (如 <code>linear</code>, <code>poly</code>, <code>rbf</code>) 来处理非线性可分数据。<br>关键超参数: <code>C</code> (正则化), <code>kernel</code>, <code>gamma</code> (对于 ‘rbf’, ‘poly’, ‘sigmoid’ 核)。特征缩放非常重要。</p>
<h4 id="决策树分类-DecisionTreeClassifier"><a href="#决策树分类-DecisionTreeClassifier" class="headerlink" title="决策树分类 (DecisionTreeClassifier)"></a>决策树分类 (<code>DecisionTreeClassifier</code>)</h4><p>与回归树类似，但叶节点包含类别标签 (或类别概率分布)。通过信息增益、基尼不纯度等标准来选择最佳分裂。</p>
<h4 id="朴素贝叶斯-GaussianNB-MultinomialNB-BernoulliNB"><a href="#朴素贝叶斯-GaussianNB-MultinomialNB-BernoulliNB" class="headerlink" title="朴素贝叶斯 (GaussianNB, MultinomialNB, BernoulliNB)"></a>朴素贝叶斯 (<code>GaussianNB</code>, <code>MultinomialNB</code>, <code>BernoulliNB</code>)</h4><p>基于贝叶斯定理的一系列分类器，假设特征之间相互独立 (朴素假设)。</p>
<ul>
<li><code>GaussianNB</code>: 适用于特征是连续高斯分布的情况。</li>
<li><code>MultinomialNB</code>: 适用于离散特征 (如文本分类中的词频)。</li>
<li><code>BernoulliNB</code>: 适用于二元特征。</li>
</ul>
<h4 id="集成方法分类-RandomForestClassifier-GradientBoostingClassifier-AdaBoostClassifier-XGBoost-LightGBM"><a href="#集成方法分类-RandomForestClassifier-GradientBoostingClassifier-AdaBoostClassifier-XGBoost-LightGBM" class="headerlink" title="集成方法分类 (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, XGBoost, LightGBM)"></a>集成方法分类 (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, XGBoost, LightGBM)</h4><p>与回归类似，但用于分类任务。预测结果通常是多数投票或平均概率。</p>
<h3 id="6-3-特征重要性-Feature-Importance-1"><a href="#6-3-特征重要性-Feature-Importance-1" class="headerlink" title="6.3. 特征重要性 (Feature Importance)"></a>6.3. 特征重要性 (Feature Importance)</h3><p>许多基于树的模型 (如决策树、随机森林、梯度提升) 可以提供特征重要性分数，指示每个特征对模型预测的贡献程度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 model 是一个训练好的 RandomForestClassifier 或 GradientBoostingClassifier</span></span><br><span class="line"><span class="comment"># importances = model.feature_importances_</span></span><br><span class="line"><span class="comment"># feature_names = X_train.columns # 如果 X_train 是 DataFrame</span></span><br><span class="line"><span class="comment"># sorted_indices = np.argsort(importances)[::-1]</span></span><br><span class="line"><span class="comment"># for i in sorted_indices:</span></span><br><span class="line"><span class="comment">#     print(f&quot;&#123;feature_names[i]&#125;: &#123;importances[i]&#125;&quot;)</span></span><br></pre></td></tr></table></figure>

<h3 id="6-4-处理不平衡数据集-Handling-Imbalanced-Datasets-1"><a href="#6-4-处理不平衡数据集-Handling-Imbalanced-Datasets-1" class="headerlink" title="6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)"></a>6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)</h3><p>当一个类别的样本数量远多于其他类别时，模型可能会偏向多数类。</p>
<ul>
<li><strong>重采样技术</strong>:<ul>
<li><strong>过采样少数类</strong>: 如 SMOTE (Synthetic Minority Over-sampling Technique)，通常使用 <code>imbalanced-learn</code> 库。</li>
<li><strong>欠采样多数类</strong>: 随机移除多数类样本。</li>
</ul>
</li>
<li><strong>调整类别权重</strong>: 许多 Scikit-learn 分类器有 <code>class_weight</code> 参数 (例如 <code>class_weight=&#39;balanced&#39;</code>)。</li>
<li><strong>使用合适的评估指标</strong>: 准确率不适用，应关注精确率、召回率、F1 分数、AUC-ROC、AUC-PR (Precision-Recall)。</li>
<li><strong>集成方法</strong>: 有些集成方法对不平衡数据更鲁棒。</li>
</ul>
<h2 id="7-无监督学习-Unsupervised-Learning-1"><a href="#7-无监督学习-Unsupervised-Learning-1" class="headerlink" title="7. 无监督学习 (Unsupervised Learning)"></a>7. 无监督学习 (Unsupervised Learning)</h2><p>无监督学习是从无标签数据中发现隐藏结构或模式。</p>
<h3 id="7-1-聚类-Clustering-1"><a href="#7-1-聚类-Clustering-1" class="headerlink" title="7.1. 聚类 (Clustering)"></a>7.1. 聚类 (Clustering)</h3><p>将数据点分组成相似的簇 (cluster)。</p>
<h4 id="K-均值聚类-KMeans"><a href="#K-均值聚类-KMeans" class="headerlink" title="K 均值聚类 (KMeans)"></a>K 均值聚类 (<code>KMeans</code>)</h4><p>迭代算法，旨在将 n 个观测值划分为 k 个簇，使得每个观测值都属于具有最近均值 (簇中心) 的簇。<br>需要预先指定簇的数量 <code>k</code>。对初始质心敏感，对特征尺度敏感。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="comment"># kmeans = KMeans(n_clusters=3, random_state=42, n_init=&#x27;auto&#x27;) # n_init=&#x27;auto&#x27; (sklearn 1.4+) or n_init=10</span></span><br><span class="line"><span class="comment"># kmeans.fit(X_scaled) # 通常在缩放后的数据上进行</span></span><br><span class="line"><span class="comment"># labels = kmeans.labels_</span></span><br><span class="line"><span class="comment"># centroids = kmeans.cluster_centers_</span></span><br></pre></td></tr></table></figure>

<p>选择 <code>k</code> 的方法包括肘部法则 (Elbow Method) 和轮廓分析 (Silhouette Analysis)。</p>
<h4 id="DBSCAN-DBSCAN"><a href="#DBSCAN-DBSCAN" class="headerlink" title="DBSCAN (DBSCAN)"></a>DBSCAN (<code>DBSCAN</code>)</h4><p>基于密度的聚类算法。可以发现任意形状的簇，并且能够识别噪声点。<br>主要参数: <code>eps</code> (邻域半径) 和 <code>min_samples</code> (形成核心对象的最小样本数)。不需要预先指定簇的数量。</p>
<h4 id="层次聚类-AgglomerativeClustering"><a href="#层次聚类-AgglomerativeClustering" class="headerlink" title="层次聚类 (AgglomerativeClustering)"></a>层次聚类 (<code>AgglomerativeClustering</code>)</h4><p>构建一个簇的层次结构 (树状图)。可以是凝聚的 (自底向上) 或分裂的 (自顶向下)。Scikit-learn 实现的是凝聚的。<br>可以指定簇的数量或切割树状图的距离阈值。</p>
<h4 id="评估聚类性能-轮廓系数、Calinski-Harabasz-指数等"><a href="#评估聚类性能-轮廓系数、Calinski-Harabasz-指数等" class="headerlink" title="评估聚类性能 (轮廓系数、Calinski-Harabasz 指数等)"></a>评估聚类性能 (轮廓系数、Calinski-Harabasz 指数等)</h4><ul>
<li><strong>轮廓系数 (<code>silhouette_score</code>)</strong>:衡量一个样本与其自身簇的紧密程度相对于其他簇的分离程度。值在 -1 到 1 之间，越接近 1 越好。</li>
<li><strong>Calinski-Harabasz 指数 (<code>calinski_harabasz_score</code>)</strong>: 基于簇间离散度和簇内离散度的比率，分数越高越好。</li>
<li>如果真实标签已知 (通常用于评估)，可以使用调整兰德指数 (<code>adjusted_rand_score</code>)、互信息 (<code>normalized_mutual_info_score</code>) 等。</li>
</ul>
<h3 id="7-2-降维-Dimensionality-Reduction-1"><a href="#7-2-降维-Dimensionality-Reduction-1" class="headerlink" title="7.2. 降维 (Dimensionality Reduction)"></a>7.2. 降维 (Dimensionality Reduction)</h3><p>减少数据中的特征数量，同时保留重要信息。</p>
<ul>
<li><strong>目的</strong>: 缓解维度灾难、减少计算复杂度、数据可视化、去除冗余特征。</li>
</ul>
<h4 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析 (PCA)"></a>主成分分析 (<code>PCA</code>)</h4><p>一种线性降维技术，通过正交变换将数据投影到由最大方差方向定义的一系列新的低维坐标系 (主成分) 上。<br>需要选择保留的主成分数量，可以通过解释方差比 (<code>explained_variance_ratio_</code>) 来决定。特征缩放很重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="comment"># pca = PCA(n_components=0.95, random_state=42) # 保留 95% 的方差，或指定整数 n_components</span></span><br><span class="line"><span class="comment"># X_pca = pca.fit_transform(X_scaled) # 通常在缩放后的数据上进行</span></span><br><span class="line"><span class="comment"># print(f&quot;Explained variance ratio: &#123;pca.explained_variance_ratio_&#125;&quot;)</span></span><br><span class="line"><span class="comment"># print(f&quot;Number of components chosen: &#123;pca.n_components_&#125;&quot;)</span></span><br></pre></td></tr></table></figure>

<h4 id="t-SNE-TSNE-主要用于可视化"><a href="#t-SNE-TSNE-主要用于可视化" class="headerlink" title="t-SNE (TSNE - 主要用于可视化)"></a>t-SNE (<code>TSNE</code> - 主要用于可视化)</h4><p><code>sklearn.manifold.TSNE</code> 是一种非线性降维技术，特别擅长于高维数据的可视化。它将高维数据点之间的相似性映射到低维空间 (通常是 2D 或 3D)，使得相似的点在低维空间中也彼此靠近。计算成本较高。</p>
<h4 id="其他降维方法-LDA-NMF"><a href="#其他降维方法-LDA-NMF" class="headerlink" title="其他降维方法 (LDA, NMF)"></a>其他降维方法 (LDA, NMF)</h4><ul>
<li><strong>线性判别分析 (LDA - <code>LinearDiscriminantAnalysis</code>)</strong>: 一种监督降维技术 (也可用作分类器)，旨在找到最大化类间可分性的特征子空间。</li>
<li><strong>非负矩阵分解 (NMF - <code>NMF</code>)</strong>: 将数据分解为两个非负矩阵的乘积，常用于特征提取 (如文本主题建模)。</li>
</ul>
<h3 id="7-3-异常检测-Anomaly-Detection-简述-1"><a href="#7-3-异常检测-Anomaly-Detection-简述-1" class="headerlink" title="7.3. 异常检测 (Anomaly Detection - 简述)"></a>7.3. 异常检测 (Anomaly Detection - 简述)</h3><p>识别与大多数数据显著不同的数据点。</p>
<ul>
<li><strong>基于统计的方法</strong>: 如 Z-score, IQR。</li>
<li><strong>基于距离的方法</strong>: 如 K-NN。</li>
<li><strong>基于密度的方法</strong>: 如 LOF (Local Outlier Factor)。</li>
<li><strong>Scikit-learn 中的工具</strong>:<ul>
<li><code>OneClassSVM</code>: 学习一个边界来包围“正常”数据。</li>
<li><code>IsolationForest</code>: 通过随机分割来隔离异常点。</li>
<li><code>EllipticEnvelope</code>: 假设数据服从高斯分布，拟合一个椭圆。</li>
</ul>
</li>
</ul>
<h2 id="8-构建-Pipeline-Pipelines-1"><a href="#8-构建-Pipeline-Pipelines-1" class="headerlink" title="8. 构建 Pipeline (Pipelines)"></a>8. 构建 Pipeline (Pipelines)</h2><p><code>sklearn.pipeline.Pipeline</code> 允许将多个处理步骤 (转换器和最终的估计器) 串联起来，形成一个单一的 Scikit-learn 估计器。</p>
<h3 id="使用-Pipeline-串联转换器和估计器"><a href="#使用-Pipeline-串联转换器和估计器" class="headerlink" title="使用 Pipeline 串联转换器和估计器"></a>使用 <code>Pipeline</code> 串联转换器和估计器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个包含缩放、PCA 和 SVC 的 Pipeline</span></span><br><span class="line"><span class="comment"># num_pipeline = Pipeline([</span></span><br><span class="line"><span class="comment">#     (&#x27;scaler&#x27;, StandardScaler()),</span></span><br><span class="line"><span class="comment">#     (&#x27;pca&#x27;, PCA(n_components=2)),</span></span><br><span class="line"><span class="comment">#     (&#x27;svc&#x27;, SVC(kernel=&#x27;rbf&#x27;, C=1, random_state=42))</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 像普通估计器一样使用 Pipeline</span></span><br><span class="line"><span class="comment"># num_pipeline.fit(X_train, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_pipeline = num_pipeline.predict(X_test)</span></span><br><span class="line"><span class="comment"># print(f&quot;Pipeline accuracy: &#123;accuracy_score(y_test, y_pred_pipeline)&#125;&quot;)</span></span><br></pre></td></tr></table></figure>

<p><strong>优点</strong>:</p>
<ul>
<li>方便：只需调用一次 <code>fit</code> 和 <code>predict</code>。</li>
<li>避免数据泄露：确保交叉验证的每一折都正确地在训练数据上 <code>fit</code> 转换器，然后 <code>transform</code> 训练和验证数据。</li>
<li>超参数调优：可以对 Pipeline 中的所有步骤的参数进行网格搜索 (参数名称格式为 <code>stepname__parametername</code>)。</li>
</ul>
<h3 id="使用-ColumnTransformer-对不同列应用不同转换"><a href="#使用-ColumnTransformer-对不同列应用不同转换" class="headerlink" title="使用 ColumnTransformer 对不同列应用不同转换"></a>使用 <code>ColumnTransformer</code> 对不同列应用不同转换</h3><p><code>sklearn.compose.ColumnTransformer</code> 允许对 DataFrame 的不同列应用不同的转换序列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 df 有数值列 &#x27;num_col1&#x27;, &#x27;num_col2&#x27; 和分类列 &#x27;cat_col1&#x27;</span></span><br><span class="line"><span class="comment"># numeric_features = [&#x27;num_col1&#x27;, &#x27;num_col2&#x27;]</span></span><br><span class="line"><span class="comment"># categorical_features = [&#x27;cat_col1&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># numeric_transformer = Pipeline(steps=[</span></span><br><span class="line"><span class="comment">#     (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;median&#x27;)),</span></span><br><span class="line"><span class="comment">#     (&#x27;scaler&#x27;, StandardScaler())])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># categorical_transformer = Pipeline(steps=[</span></span><br><span class="line"><span class="comment">#     (&#x27;imputer&#x27;, SimpleImputer(strategy=&#x27;most_frequent&#x27;)), # 或 &#x27;constant&#x27;, fill_value=&#x27;missing&#x27;</span></span><br><span class="line"><span class="comment">#     (&#x27;onehot&#x27;, OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessor = ColumnTransformer(</span></span><br><span class="line"><span class="comment">#     transformers=[</span></span><br><span class="line"><span class="comment">#         (&#x27;num&#x27;, numeric_transformer, numeric_features),</span></span><br><span class="line"><span class="comment">#         (&#x27;cat&#x27;, categorical_transformer, categorical_features)],</span></span><br><span class="line"><span class="comment">#     remainder=&#x27;passthrough&#x27; # 其他未指定列的处理方式 (&#x27;drop&#x27; 或 &#x27;passthrough&#x27;)</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># full_pipeline = Pipeline(steps=[(&#x27;preprocessor&#x27;, preprocessor),</span></span><br><span class="line"><span class="comment">#                               (&#x27;classifier&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># full_pipeline.fit(X_train_df, y_train)</span></span><br><span class="line"><span class="comment"># y_pred_full = full_pipeline.predict(X_test_df)</span></span><br></pre></td></tr></table></figure>

<h3 id="Pipeline-与-GridSearchCV-RandomizedSearchCV-结合"><a href="#Pipeline-与-GridSearchCV-RandomizedSearchCV-结合" class="headerlink" title="Pipeline 与 GridSearchCV&#x2F;RandomizedSearchCV 结合"></a>Pipeline 与 GridSearchCV&#x2F;RandomizedSearchCV 结合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># param_grid_pipeline = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;preprocessor__num__imputer__strategy&#x27;: [&#x27;mean&#x27;, &#x27;median&#x27;],</span></span><br><span class="line"><span class="comment">#     &#x27;pca__n_components&#x27;: [2, 3, 5], # 假设 Pipeline 中有名为 &#x27;pca&#x27; 的步骤</span></span><br><span class="line"><span class="comment">#     &#x27;classifier__C&#x27;: [0.1, 1, 10]    # 假设 Pipeline 中有名为 &#x27;classifier&#x27; 的步骤 (SVC)</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># grid_search_pipeline = GridSearchCV(full_pipeline, param_grid_pipeline, cv=5)</span></span><br><span class="line"><span class="comment"># grid_search_pipeline.fit(X_train_df, y_train)</span></span><br></pre></td></tr></table></figure>

<h2 id="9-模型持久化-Model-Persistence-1"><a href="#9-模型持久化-Model-Persistence-1" class="headerlink" title="9. 模型持久化 (Model Persistence)"></a>9. 模型持久化 (Model Persistence)</h2><p>训练好的模型可以保存到磁盘，以便将来加载和重用，而无需重新训练。</p>
<h3 id="使用-joblib"><a href="#使用-joblib" class="headerlink" title="使用 joblib"></a>使用 <code>joblib</code></h3><p><code>joblib</code> 是 Scikit-learn 推荐的持久化方法，尤其对于包含大型 NumPy 数组的对象更高效。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="comment"># 假设 model 是一个训练好的 Scikit-learn 模型</span></span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line"><span class="comment"># joblib.dump(model, &#x27;my_model.joblib&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="comment"># loaded_model = joblib.load(&#x27;my_model.joblib&#x27;)</span></span><br><span class="line"><span class="comment"># predictions = loaded_model.predict(X_new_data)</span></span><br></pre></td></tr></table></figure>

<h3 id="使用-pickle-注意事项"><a href="#使用-pickle-注意事项" class="headerlink" title="使用 pickle (注意事项)"></a>使用 <code>pickle</code> (注意事项)</h3><p>Python 内置的 <code>pickle</code> 模块也可以用于序列化和反序列化对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line"><span class="comment"># with open(&#x27;my_model.pkl&#x27;, &#x27;wb&#x27;) as f:</span></span><br><span class="line"><span class="comment">#     pickle.dump(model, f)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="comment"># with open(&#x27;my_model.pkl&#x27;, &#x27;rb&#x27;) as f:</span></span><br><span class="line"><span class="comment">#     loaded_model_pkl = pickle.load(f)</span></span><br></pre></td></tr></table></figure>

<p><strong>安全警告</strong>: <code>pickle</code> 模块不安全，不要反序列化来自不受信任或未经身份验证来源的数据。<code>joblib</code> 在内部使用 <code>pickle</code>，但主要针对 NumPy 数组进行了优化。对于 Scikit-learn 模型，两者通常都可以工作，但 <code>joblib</code> 更受推荐。</p>
<h2 id="10-实践案例：一个完整的数据科学项目流程示例-1"><a href="#10-实践案例：一个完整的数据科学项目流程示例-1" class="headerlink" title="10. 实践案例：一个完整的数据科学项目流程示例"></a>10. 实践案例：一个完整的数据科学项目流程示例</h2><p>这里概述一个简化的流程，使用 Scikit-learn 内置的 <code>iris</code> 数据集。</p>
<h3 id="10-1-问题定义与数据加载-1"><a href="#10-1-问题定义与数据加载-1" class="headerlink" title="10.1. 问题定义与数据加载"></a>10.1. 问题定义与数据加载</h3><p><strong>问题</strong>: 根据鸢尾花的花萼和花瓣的测量值，预测其种类。这是一个多分类问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">y = pd.Series(iris.target, name=<span class="string">&#x27;species&#x27;</span>) <span class="comment"># 目标是数字 (0, 1, 2)</span></span><br><span class="line">target_names = iris.target_names <span class="comment"># [&#x27;setosa&#x27;, &#x27;versicolor&#x27;, &#x27;virginica&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br><span class="line"><span class="built_in">print</span>(y.value_counts())</span><br><span class="line"><span class="built_in">print</span>(target_names)</span><br></pre></td></tr></table></figure>

<h3 id="10-2-EDA-与数据预处理-1"><a href="#10-2-EDA-与数据预处理-1" class="headerlink" title="10.2. EDA 与数据预处理"></a>10.2. EDA 与数据预处理</h3><p>对于 iris 数据集，通常不需要太多预处理，但我们会进行标准化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 简单的 EDA: 查看特征分布 (仅示例一个)</span></span><br><span class="line"><span class="comment"># sns.pairplot(pd.concat([X, y.map(&#123;i:name for i, name in enumerate(target_names)&#125;)], axis=1), hue=&#x27;species&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>, stratify=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化 (仅对训练集 fit，然后对训练集和测试集 transform)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train)</span><br><span class="line">X_test_scaled = scaler.transform(X_test) <span class="comment"># 使用训练集的均值和标准差</span></span><br></pre></td></tr></table></figure>

<h3 id="10-3-特征工程-1"><a href="#10-3-特征工程-1" class="headerlink" title="10.3. 特征工程"></a>10.3. 特征工程</h3><p>Iris 数据集特征已经很好，此步骤可以跳过或简化。在实际项目中，这可能包括创建交互特征、多项式特征、文本特征转换等。</p>
<h3 id="10-4-模型训练与评估-1"><a href="#10-4-模型训练与评估-1" class="headerlink" title="10.4. 模型训练与评估"></a>10.4. 模型训练与评估</h3><p>选择一个分类模型，例如逻辑回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line">model = LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>, multi_class=<span class="string">&#x27;ovr&#x27;</span>, random_state=<span class="number">42</span>) <span class="comment"># One-vs-Rest</span></span><br><span class="line">model.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy: <span class="subst">&#123;accuracy_score(y_test, y_pred):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report:\n&quot;</span>, classification_report(y_test, y_pred, target_names=target_names))</span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line"><span class="comment"># sns.heatmap(cm, annot=True, fmt=&#x27;d&#x27;, xticklabels=target_names, yticklabels=target_names)</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;Predicted&#x27;)</span></span><br><span class="line"><span class="comment"># plt.ylabel(&#x27;True&#x27;)</span></span><br><span class="line"><span class="comment"># plt.title(&#x27;Confusion Matrix&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br></pre></td></tr></table></figure>

<h3 id="10-5-超参数调优-1"><a href="#10-5-超参数调优-1" class="headerlink" title="10.5. 超参数调优"></a>10.5. 超参数调优</h3><p>使用 GridSearchCV 尝试改进逻辑回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = &#123;<span class="string">&#x27;C&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">&#x27;penalty&#x27;</span>: [<span class="string">&#x27;l1&#x27;</span>, <span class="string">&#x27;l2&#x27;</span>]&#125;</span><br><span class="line">grid_search = GridSearchCV(LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>, multi_class=<span class="string">&#x27;ovr&#x27;</span>, random_state=<span class="number">42</span>),</span><br><span class="line">                           param_grid, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">grid_search.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best parameters: <span class="subst">&#123;grid_search.best_params_&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Best cross-validation accuracy: <span class="subst">&#123;grid_search.best_score_:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">best_model = grid_search.best_estimator_</span><br></pre></td></tr></table></figure>

<h3 id="10-6-最终模型与预测-1"><a href="#10-6-最终模型与预测-1" class="headerlink" title="10.6. 最终模型与预测"></a>10.6. 最终模型与预测</h3><p>使用找到的最佳模型进行最终评估或预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_pred_best = best_model.predict(X_test_scaled)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy (Best Model): <span class="subst">&#123;accuracy_score(y_test, y_pred_best):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nClassification Report (Best Model):\n&quot;</span>, classification_report(y_test, y_pred_best, target_names=target_names))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以保存 best_model</span></span><br><span class="line"><span class="comment"># import joblib</span></span><br><span class="line"><span class="comment"># joblib.dump(best_model, &#x27;iris_logreg_model.joblib&#x27;)</span></span><br></pre></td></tr></table></figure>

<h2 id="11-总结与进阶学习-1"><a href="#11-总结与进阶学习-1" class="headerlink" title="11. 总结与进阶学习"></a>11. 总结与进阶学习</h2><p>本指南详细介绍了 Python 在数据科学中的核心库 NumPy, Pandas, Matplotlib&#x2F;Seaborn 的基础用法，并深入探讨了 Scikit-learn 库的各个方面，包括数据预处理、模型选择、监督学习算法、无监督学习算法、Pipeline 构建和模型持久化。</p>
<p><strong>进阶学习方向</strong>:</p>
<ul>
<li><strong>深度学习框架</strong>: TensorFlow, Keras, PyTorch。</li>
<li><strong>更高级的统计建模</strong>: Statsmodels 库。</li>
<li><strong>特定领域的库</strong>: 如 NLTK, spaCy (自然语言处理), OpenCV (计算机视觉)。</li>
<li><strong>大数据工具</strong>: PySpark。</li>
<li><strong>模型可解释性</strong>: SHAP, LIME。</li>
<li><strong>MLOps</strong>: 模型部署、监控、版本控制 (如 MLflow, Kubeflow)。</li>
<li><strong>贝叶斯方法与概率编程</strong>: PyMC3, Stan。</li>
<li><strong>时间序列分析</strong>: Prophet, <code>statsmodels.tsa</code>。</li>
</ul>
<p>不断实践、参与 Kaggle 等竞赛、阅读论文和博客是提升数据科学技能的关键。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Jackey Zhou</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/')">数据科学与机器学习</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=数据科学与机器学习&amp;url=http://example.com/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Welcome To LifeTech's Blog</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>python相关学习<span class="tagsPageCount">4</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">pytorch基础知识</div></div></a></div><div class="next-post pull-right"><a href="/2025/05/12/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">模型部署</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2025/05/12/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="python基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">python基础知识</div></div></a></div><div><a href="/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="pytorch基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">pytorch基础知识</div></div></a></div><div><a href="/2025/05/12/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" title="模型部署"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">模型部署</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description">记录技术成长的个人博客</div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Python-%E5%9C%A8%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">1. Python 在数据科学中的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A0%B8%E5%BF%83%E5%BA%93%E5%AE%9E%E8%B7%B5-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">2. 核心库实践 - 数据处理与可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-NumPy-Numerical-Python"><span class="toc-number">2.1.</span> <span class="toc-text">2.1. NumPy (Numerical Python)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Pandas-Python-Data-Analysis-Library"><span class="toc-number">2.2.</span> <span class="toc-text">2.2. Pandas (Python Data Analysis Library)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Matplotlib-%E4%B8%8E-Seaborn-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">2.3.</span> <span class="toc-text">2.3. Matplotlib 与 Seaborn (数据可视化)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Scikit-learn-%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-number">3.</span> <span class="toc-text">3. Scikit-learn 简介与核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-Preprocessing"><span class="toc-number">4.</span> <span class="toc-text">4. 数据预处理 (Preprocessing)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96-Scaling"><span class="toc-number">4.1.</span> <span class="toc-text">4.1. 标准化与归一化 (Scaling)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BC%96%E7%A0%81%E5%88%86%E7%B1%BB%E7%89%B9%E5%BE%81-Encoding-Categorical-Features"><span class="toc-number">4.2.</span> <span class="toc-text">4.2. 编码分类特征 (Encoding Categorical Features)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC-Handling-Missing-Values"><span class="toc-number">4.3.</span> <span class="toc-text">4.3. 处理缺失值 (Handling Missing Values)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96-%E5%88%86%E7%AE%B1-Discretization-Binning"><span class="toc-number">4.4.</span> <span class="toc-text">4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E7%94%9F%E6%88%90%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81-Polynomial-Features"><span class="toc-number">4.5.</span> <span class="toc-text">4.5. 生成多项式特征 (Polynomial Features)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BD%AC%E6%8D%A2%E5%99%A8-Custom-Transformers"><span class="toc-number">4.6.</span> <span class="toc-text">4.6. 自定义转换器 (Custom Transformers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%AF%84%E4%BC%B0-Model-Selection-and-Evaluation"><span class="toc-number">5.</span> <span class="toc-text">5. 模型选择与评估 (Model Selection and Evaluation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86-train-test-split"><span class="toc-number">5.1.</span> <span class="toc-text">5.1. 数据集划分 (train_test_split)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-Cross-Validation"><span class="toc-number">5.2.</span> <span class="toc-text">5.2. 交叉验证 (Cross-Validation)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-Metrics"><span class="toc-number">5.3.</span> <span class="toc-text">5.3. 模型评估指标 (Metrics)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98-Hyperparameter-Tuning"><span class="toc-number">5.4.</span> <span class="toc-text">5.4. 超参数调优 (Hyperparameter Tuning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E4%B8%8E%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF-Learning-and-Validation-Curves"><span class="toc-number">5.5.</span> <span class="toc-text">5.5. 学习曲线与验证曲线 (Learning and Validation Curves)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Supervised-Learning"><span class="toc-number">6.</span> <span class="toc-text">6. 监督学习 (Supervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%9B%9E%E5%BD%92-Regression"><span class="toc-number">6.1.</span> <span class="toc-text">6.1. 回归 (Regression)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%88%86%E7%B1%BB-Classification"><span class="toc-number">6.2.</span> <span class="toc-text">6.2. 分类 (Classification)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7-Feature-Importance"><span class="toc-number">6.3.</span> <span class="toc-text">6.3. 特征重要性 (Feature Importance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E5%A4%84%E7%90%86%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86-Handling-Imbalanced-Datasets"><span class="toc-number">6.4.</span> <span class="toc-text">6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Unsupervised-Learning"><span class="toc-number">7.</span> <span class="toc-text">7. 无监督学习 (Unsupervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%81%9A%E7%B1%BB-Clustering"><span class="toc-number">7.1.</span> <span class="toc-text">7.1. 聚类 (Clustering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E9%99%8D%E7%BB%B4-Dimensionality-Reduction"><span class="toc-number">7.2.</span> <span class="toc-text">7.2. 降维 (Dimensionality Reduction)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Anomaly-Detection-%E7%AE%80%E8%BF%B0"><span class="toc-number">7.3.</span> <span class="toc-text">7.3. 异常检测 (Anomaly Detection - 简述)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%9E%84%E5%BB%BA-Pipeline-Pipelines"><span class="toc-number">8.</span> <span class="toc-text">8. 构建 Pipeline (Pipelines)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%A8%A1%E5%9E%8B%E6%8C%81%E4%B9%85%E5%8C%96-Model-Persistence"><span class="toc-number">9.</span> <span class="toc-text">9. 模型持久化 (Model Persistence)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">10.</span> <span class="toc-text">10. 实践案例：一个完整的数据科学项目流程示例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">10.1.</span> <span class="toc-text">10.1. 问题定义与数据加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-EDA-%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">10.2.</span> <span class="toc-text">10.2. EDA 与数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">10.3.</span> <span class="toc-text">10.3. 特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="toc-number">10.4.</span> <span class="toc-text">10.4. 模型训练与评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-number">10.5.</span> <span class="toc-text">10.5. 超参数调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-6-%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="toc-number">10.6.</span> <span class="toc-text">10.6. 最终模型与预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">11. 总结与进阶学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Python-%E5%9C%A8%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8-1"><span class="toc-number">12.</span> <span class="toc-text">1. Python 在数据科学中的作用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9-Python-%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6"><span class="toc-number">12.1.</span> <span class="toc-text">为什么选择 Python 进行数据科学?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%BA%93%E6%A6%82%E8%A7%88"><span class="toc-number">12.2.</span> <span class="toc-text">核心数据科学库概览</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A0%B8%E5%BF%83%E5%BA%93%E5%AE%9E%E8%B7%B5-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96-1"><span class="toc-number">13.</span> <span class="toc-text">2. 核心库实践 - 数据处理与可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-NumPy-Numerical-Python-1"><span class="toc-number">13.1.</span> <span class="toc-text">2.1. NumPy (Numerical Python)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NumPy-%E6%95%B0%E7%BB%84-ndarray"><span class="toc-number">13.1.1.</span> <span class="toc-text">NumPy 数组 (ndarray)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E5%88%9B%E5%BB%BA"><span class="toc-number">13.1.2.</span> <span class="toc-text">数组创建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97"><span class="toc-number">13.1.3.</span> <span class="toc-text">数组操作与数学运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%88%87%E7%89%87"><span class="toc-number">13.1.4.</span> <span class="toc-text">索引与切片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD-Broadcasting"><span class="toc-number">13.1.5.</span> <span class="toc-text">广播 (Broadcasting)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Pandas-Python-Data-Analysis-Library-1"><span class="toc-number">13.2.</span> <span class="toc-text">2.2. Pandas (Python Data Analysis Library)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Series-%E4%B8%8E-DataFrame"><span class="toc-number">13.2.1.</span> <span class="toc-text">Series 与 DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA-CSV-Excel-%E7%AD%89"><span class="toc-number">13.2.2.</span> <span class="toc-text">数据导入与导出 (CSV, Excel 等)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9F%A5%E7%9C%8B%E4%B8%8E%E6%A3%80%E6%B5%8B"><span class="toc-number">13.2.3.</span> <span class="toc-text">数据查看与检测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%80%89%E6%8B%A9%E4%B8%8E%E7%B4%A2%E5%BC%95-loc-iloc"><span class="toc-number">13.2.4.</span> <span class="toc-text">数据选择与索引 (loc, iloc)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC%E3%80%81%E9%87%8D%E5%A4%8D%E5%80%BC"><span class="toc-number">13.2.5.</span> <span class="toc-text">数据清洗 (处理缺失值、重复值)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E4%B8%8E%E6%93%8D%E4%BD%9C-apply-map"><span class="toc-number">13.2.6.</span> <span class="toc-text">数据转换与操作 (apply, map)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E4%B8%8E%E8%81%9A%E5%90%88-groupby"><span class="toc-number">13.2.7.</span> <span class="toc-text">分组与聚合 (groupby)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E4%B8%8E%E8%BF%9E%E6%8E%A5-merge-concat-join"><span class="toc-number">13.2.8.</span> <span class="toc-text">合并与连接 (merge, concat, join)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Matplotlib-%E4%B8%8E-Seaborn-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-1"><span class="toc-number">13.3.</span> <span class="toc-text">2.3. Matplotlib 与 Seaborn (数据可视化)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Matplotlib-%E5%9F%BA%E7%A1%80"><span class="toc-number">13.3.1.</span> <span class="toc-text">Matplotlib 基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Seaborn-%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">13.3.2.</span> <span class="toc-text">Seaborn 统计可视化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Scikit-learn-%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5-1"><span class="toc-number">14.</span> <span class="toc-text">3. Scikit-learn 简介与核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scikit-learn-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">14.1.</span> <span class="toc-text">Scikit-learn 是什么?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%93%B2%E5%AD%A6%E4%B8%8E-API-%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">14.2.</span> <span class="toc-text">设计哲学与 API 一致性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%AF%B9%E8%B1%A1%E7%B1%BB%E5%9E%8B"><span class="toc-number">14.3.</span> <span class="toc-text">核心对象类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Estimator-%E4%BC%B0%E8%AE%A1%E5%99%A8"><span class="toc-number">14.3.1.</span> <span class="toc-text">Estimator (估计器)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer-%E8%BD%AC%E6%8D%A2%E5%99%A8"><span class="toc-number">14.3.2.</span> <span class="toc-text">Transformer (转换器)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Predictor-%E9%A2%84%E6%B5%8B%E5%99%A8"><span class="toc-number">14.3.3.</span> <span class="toc-text">Predictor (预测器)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC-API-%E6%96%B9%E6%B3%95-fit-transform-predict-score"><span class="toc-number">14.4.</span> <span class="toc-text">基本 API 方法: fit(), transform(), predict(), score()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-Preprocessing-1"><span class="toc-number">15.</span> <span class="toc-text">4. 数据预处理 (Preprocessing)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A0%87%E5%87%86%E5%8C%96%E4%B8%8E%E5%BD%92%E4%B8%80%E5%8C%96-Scaling-1"><span class="toc-number">15.1.</span> <span class="toc-text">4.1. 标准化与归一化 (Scaling)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#StandardScaler"><span class="toc-number">15.1.1.</span> <span class="toc-text">StandardScaler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MinMaxScaler"><span class="toc-number">15.1.2.</span> <span class="toc-text">MinMaxScaler</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RobustScaler"><span class="toc-number">15.1.3.</span> <span class="toc-text">RobustScaler</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BC%96%E7%A0%81%E5%88%86%E7%B1%BB%E7%89%B9%E5%BE%81-Encoding-Categorical-Features-1"><span class="toc-number">15.2.</span> <span class="toc-text">4.2. 编码分类特征 (Encoding Categorical Features)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LabelEncoder"><span class="toc-number">15.2.1.</span> <span class="toc-text">LabelEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OneHotEncoder"><span class="toc-number">15.2.2.</span> <span class="toc-text">OneHotEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pandas-get-dummies"><span class="toc-number">15.2.3.</span> <span class="toc-text">Pandas get_dummies()</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC-Handling-Missing-Values-1"><span class="toc-number">15.3.</span> <span class="toc-text">4.3. 处理缺失值 (Handling Missing Values)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SimpleImputer"><span class="toc-number">15.3.1.</span> <span class="toc-text">SimpleImputer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KNNImputer-%E6%9B%B4%E9%AB%98%E7%BA%A7"><span class="toc-number">15.3.2.</span> <span class="toc-text">KNNImputer (更高级)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96-%E5%88%86%E7%AE%B1-Discretization-Binning-1"><span class="toc-number">15.4.</span> <span class="toc-text">4.4. 特征离散化&#x2F;分箱 (Discretization&#x2F;Binning)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#KBinsDiscretizer"><span class="toc-number">15.4.1.</span> <span class="toc-text">KBinsDiscretizer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E7%94%9F%E6%88%90%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81-Polynomial-Features-1"><span class="toc-number">15.5.</span> <span class="toc-text">4.5. 生成多项式特征 (Polynomial Features)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#PolynomialFeatures"><span class="toc-number">15.5.1.</span> <span class="toc-text">PolynomialFeatures</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BD%AC%E6%8D%A2%E5%99%A8-Custom-Transformers-1"><span class="toc-number">15.6.</span> <span class="toc-text">4.6. 自定义转换器 (Custom Transformers)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%AF%84%E4%BC%B0-Model-Selection-and-Evaluation-1"><span class="toc-number">16.</span> <span class="toc-text">5. 模型选择与评估 (Model Selection and Evaluation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86-train-test-split-1"><span class="toc-number">16.1.</span> <span class="toc-text">5.1. 数据集划分 (train_test_split)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-Cross-Validation-1"><span class="toc-number">16.2.</span> <span class="toc-text">5.2. 交叉验证 (Cross-Validation)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cross-val-score"><span class="toc-number">16.2.1.</span> <span class="toc-text">cross_val_score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KFold-StratifiedKFold-%E7%AD%89%E7%AD%96%E7%95%A5"><span class="toc-number">16.2.2.</span> <span class="toc-text">KFold, StratifiedKFold 等策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-Metrics-1"><span class="toc-number">16.3.</span> <span class="toc-text">5.3. 模型评估指标 (Metrics)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87"><span class="toc-number">16.3.1.</span> <span class="toc-text">分类指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%8C%87%E6%A0%87"><span class="toc-number">16.3.2.</span> <span class="toc-text">回归指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E6%8C%87%E6%A0%87-%E7%AE%80%E8%BF%B0"><span class="toc-number">16.3.3.</span> <span class="toc-text">聚类指标 (简述)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98-Hyperparameter-Tuning-1"><span class="toc-number">16.4.</span> <span class="toc-text">5.4. 超参数调优 (Hyperparameter Tuning)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GridSearchCV"><span class="toc-number">16.4.1.</span> <span class="toc-text">GridSearchCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RandomizedSearchCV"><span class="toc-number">16.4.2.</span> <span class="toc-text">RandomizedSearchCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E8%B0%83%E4%BC%98%E7%AD%96%E7%95%A5-%E5%A6%82-Bayesian-Optimization-%E5%A4%96%E9%83%A8%E5%BA%93"><span class="toc-number">16.4.3.</span> <span class="toc-text">其他调优策略 (如 Bayesian Optimization - 外部库)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E4%B8%8E%E9%AA%8C%E8%AF%81%E6%9B%B2%E7%BA%BF-Learning-and-Validation-Curves-1"><span class="toc-number">16.5.</span> <span class="toc-text">5.5. 学习曲线与验证曲线 (Learning and Validation Curves)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Supervised-Learning-1"><span class="toc-number">17.</span> <span class="toc-text">6. 监督学习 (Supervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%9B%9E%E5%BD%92-Regression-1"><span class="toc-number">17.1.</span> <span class="toc-text">6.1. 回归 (Regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-LinearRegression"><span class="toc-number">17.1.1.</span> <span class="toc-text">线性回归 (LinearRegression)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92-Ridge-%E4%B8%8E-Lasso-%E5%9B%9E%E5%BD%92-Lasso"><span class="toc-number">17.1.2.</span> <span class="toc-text">岭回归 (Ridge) 与 Lasso 回归 (Lasso)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92-SVR"><span class="toc-number">17.1.3.</span> <span class="toc-text">支持向量回归 (SVR)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%9B%9E%E5%BD%92-DecisionTreeRegressor"><span class="toc-number">17.1.4.</span> <span class="toc-text">决策树回归 (DecisionTreeRegressor)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E5%9B%9E%E5%BD%92-RandomForestRegressor-GradientBoostingRegressor-AdaBoostRegressor-XGBoost-LightGBM"><span class="toc-number">17.1.5.</span> <span class="toc-text">集成方法回归 (RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, XGBoost, LightGBM)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%88%86%E7%B1%BB-Classification-1"><span class="toc-number">17.2.</span> <span class="toc-text">6.2. 分类 (Classification)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-LogisticRegression"><span class="toc-number">17.2.1.</span> <span class="toc-text">逻辑回归 (LogisticRegression)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#K-%E8%BF%91%E9%82%BB-KNeighborsClassifier"><span class="toc-number">17.2.2.</span> <span class="toc-text">K 近邻 (KNeighborsClassifier)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVC"><span class="toc-number">17.2.3.</span> <span class="toc-text">支持向量机 (SVC)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB-DecisionTreeClassifier"><span class="toc-number">17.2.4.</span> <span class="toc-text">决策树分类 (DecisionTreeClassifier)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-GaussianNB-MultinomialNB-BernoulliNB"><span class="toc-number">17.2.5.</span> <span class="toc-text">朴素贝叶斯 (GaussianNB, MultinomialNB, BernoulliNB)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB-RandomForestClassifier-GradientBoostingClassifier-AdaBoostClassifier-XGBoost-LightGBM"><span class="toc-number">17.2.6.</span> <span class="toc-text">集成方法分类 (RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, XGBoost, LightGBM)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7-Feature-Importance-1"><span class="toc-number">17.3.</span> <span class="toc-text">6.3. 特征重要性 (Feature Importance)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E5%A4%84%E7%90%86%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86-Handling-Imbalanced-Datasets-1"><span class="toc-number">17.4.</span> <span class="toc-text">6.4. 处理不平衡数据集 (Handling Imbalanced Datasets)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Unsupervised-Learning-1"><span class="toc-number">18.</span> <span class="toc-text">7. 无监督学习 (Unsupervised Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E8%81%9A%E7%B1%BB-Clustering-1"><span class="toc-number">18.1.</span> <span class="toc-text">7.1. 聚类 (Clustering)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB-KMeans"><span class="toc-number">18.1.1.</span> <span class="toc-text">K 均值聚类 (KMeans)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DBSCAN-DBSCAN"><span class="toc-number">18.1.2.</span> <span class="toc-text">DBSCAN (DBSCAN)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB-AgglomerativeClustering"><span class="toc-number">18.1.3.</span> <span class="toc-text">层次聚类 (AgglomerativeClustering)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD-%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0%E3%80%81Calinski-Harabasz-%E6%8C%87%E6%95%B0%E7%AD%89"><span class="toc-number">18.1.4.</span> <span class="toc-text">评估聚类性能 (轮廓系数、Calinski-Harabasz 指数等)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E9%99%8D%E7%BB%B4-Dimensionality-Reduction-1"><span class="toc-number">18.2.</span> <span class="toc-text">7.2. 降维 (Dimensionality Reduction)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA"><span class="toc-number">18.2.1.</span> <span class="toc-text">主成分分析 (PCA)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#t-SNE-TSNE-%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">18.2.2.</span> <span class="toc-text">t-SNE (TSNE - 主要用于可视化)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95-LDA-NMF"><span class="toc-number">18.2.3.</span> <span class="toc-text">其他降维方法 (LDA, NMF)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-Anomaly-Detection-%E7%AE%80%E8%BF%B0-1"><span class="toc-number">18.3.</span> <span class="toc-text">7.3. 异常检测 (Anomaly Detection - 简述)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%9E%84%E5%BB%BA-Pipeline-Pipelines-1"><span class="toc-number">19.</span> <span class="toc-text">8. 构建 Pipeline (Pipelines)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Pipeline-%E4%B8%B2%E8%81%94%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8"><span class="toc-number">19.1.</span> <span class="toc-text">使用 Pipeline 串联转换器和估计器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-ColumnTransformer-%E5%AF%B9%E4%B8%8D%E5%90%8C%E5%88%97%E5%BA%94%E7%94%A8%E4%B8%8D%E5%90%8C%E8%BD%AC%E6%8D%A2"><span class="toc-number">19.2.</span> <span class="toc-text">使用 ColumnTransformer 对不同列应用不同转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pipeline-%E4%B8%8E-GridSearchCV-RandomizedSearchCV-%E7%BB%93%E5%90%88"><span class="toc-number">19.3.</span> <span class="toc-text">Pipeline 与 GridSearchCV&#x2F;RandomizedSearchCV 结合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%A8%A1%E5%9E%8B%E6%8C%81%E4%B9%85%E5%8C%96-Model-Persistence-1"><span class="toc-number">20.</span> <span class="toc-text">9. 模型持久化 (Model Persistence)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-joblib"><span class="toc-number">20.1.</span> <span class="toc-text">使用 joblib</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-pickle-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">20.2.</span> <span class="toc-text">使用 pickle (注意事项)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B%EF%BC%9A%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E9%A1%B9%E7%9B%AE%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">21.</span> <span class="toc-text">10. 实践案例：一个完整的数据科学项目流程示例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD-1"><span class="toc-number">21.1.</span> <span class="toc-text">10.1. 问题定义与数据加载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-EDA-%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-1"><span class="toc-number">21.2.</span> <span class="toc-text">10.2. EDA 与数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-1"><span class="toc-number">21.3.</span> <span class="toc-text">10.3. 特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-4-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0-1"><span class="toc-number">21.4.</span> <span class="toc-text">10.4. 模型训练与评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-5-%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98-1"><span class="toc-number">21.5.</span> <span class="toc-text">10.5. 超参数调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-6-%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B%E4%B8%8E%E9%A2%84%E6%B5%8B-1"><span class="toc-number">21.6.</span> <span class="toc-text">10.6. 最终模型与预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0-1"><span class="toc-number">22.</span> <span class="toc-text">11. 总结与进阶学习</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/15/tauri%E5%89%8D%E5%90%8E%E7%AB%AF%E4%BA%A4%E4%BA%92/" title="tauri前后端交互">tauri前后端交互</a><time datetime="2025-05-14T16:28:49.000Z" title="发表于 2025-05-15 00:28:49">2025-05-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/vue3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="vue3基础知识">vue3基础知识</a><time datetime="2025-05-12T15:28:53.000Z" title="发表于 2025-05-12 23:28:53">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" title="模型部署">模型部署</a><time datetime="2025-05-11T18:17:36.000Z" title="发表于 2025-05-12 02:17:36">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="数据科学与机器学习">数据科学与机器学习</a><time datetime="2025-05-11T18:17:28.000Z" title="发表于 2025-05-12 02:17:28">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="pytorch基础知识">pytorch基础知识</a><time datetime="2025-05-11T18:17:15.000Z" title="发表于 2025-05-12 02:17:15">2025-05-12</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Jackey Zhou" target="_blank">Jackey Zhou</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/CSharp%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">CSharp学习<sup>3</sup></a><a href="/tags/IDE%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 0.88rem;">IDE常用快捷键<sup>2</sup></a><a href="/tags/Rust%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 0.88rem;">Rust基础知识<sup>21</sup></a><a href="/tags/csharp%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">csharp相关学习<sup>5</sup></a><a href="/tags/docker%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">docker学习<sup>1</sup></a><a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/" style="font-size: 0.88rem;">hexo博客常见命令<sup>1</sup></a><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">python相关学习<sup>4</sup></a><a href="/tags/tauri%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 0.88rem;">tauri基础知识<sup>1</sup></a><a href="/tags/xmake%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">xmake学习<sup>3</sup></a><a href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">个人学习<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">前端学习<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 0.88rem;">数据分析<sup>1</sup></a><a href="/tags/%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95/" style="font-size: 0.88rem;">无线调试<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">深度学习<sup>1</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" style="font-size: 0.88rem;">系统常用命令<sup>2</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 0.88rem;">系统常用快捷键<sup>2</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Jackey Zhou 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>