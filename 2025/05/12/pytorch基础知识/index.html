<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>pytorch基础知识 | Welcome To LifeTech's Blog</title><meta name="keywords" content="python相关学习"><meta name="author" content="Jackey Zhou"><meta name="copyright" content="Jackey Zhou"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="pytorch基础知识"><meta name="application-name" content="pytorch基础知识"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="pytorch基础知识"><meta property="og:url" content="http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/index.html"><meta property="og:site_name" content="Welcome To LifeTech's Blog"><meta property="og:description" content="目录 1. PyTorch 简介 1.1. 什么是 PyTorch? 1.2. PyTorch 的核心特性 1.3. 为什么选择 PyTorch?  2. 安装与环境配置3. PyTorch 核心：张量 (Tensors) 3.1. 张量基础 3.2. 创建张量 3.3. 张量属性 3.4. 张量操"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="Jackey Zhou"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="目录 1. PyTorch 简介 1.1. 什么是 PyTorch? 1.2. PyTorch 的核心特性 1.3. 为什么选择 PyTorch?  2. 安装与环境配置3. PyTorch 核心：张量 (Tensors) 3.1. 张量基础 3.2. 创建张量 3.3. 张量属性 3.4. 张量操"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2024/07/27/125766904/ba62475f396df9de3316a08ed9e65d86_5680958632268053399..png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Jackey Zhou","link":"链接: ","source":"来源: Welcome To LifeTech's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Welcome To LifeTech's Blog',
  title: 'pytorch基础知识',
  postAI: '',
  pageFillDescription: ', , , , , , , , , , , , , 1. PyTorch 简介, 1.1. 什么是 PyTorch?, 1.2. PyTorch 的核心特性, 1.3. 为什么选择 PyTorch?, 2. 安装与环境配置, 3. PyTorch 核心：张量 (Tensors), 3.1. 张量基础, 3.2. 创建张量, 3.3. 张量属性, 3.4. 张量操作, 算术运算, 索引、切片、连接与变形, 数学运算 (聚合、比较等), 广播机制, 3.5. 与 NumPy 的互操作, 3.6. GPU 加速, 4. Autograd：自动微分, 4.1. 计算图, 4.2. requires_grad 属性, 4.3. 梯度计算 (backward() 和 .grad), 4.4. 禁用梯度跟踪 (torch.no_grad() .detach()), 4.5. 梯度累积, 4.6. 计算图与叶子节点, 5. 构建神经网络 (torch.nn), 5.1. nn.Module 模型基类, 5.2. 定义网络结构, 5.3. 常用层 (Layers), 线性层 (nn.Linear), 卷积层 (nn.Conv1d nn.Conv2d nn.Conv3d), 池化层 (nn.MaxPool2d nn.AvgPool2d nn.AdaptiveAvgPool2d), 循环层 (nn.RNN nn.LSTM nn.GRU), Transformer 层 (nn.Transformer), 激活函数 (nn.ReLU nn.Sigmoid nn.Tanh nn.Softmax etc.), 正则化层 (nn.Dropout nn.BatchNorm1d/2d/3d), 5.4. 损失函数 (Loss Functions), 5.5. 容器 (Containers), nn.Sequential, nn.ModuleList, nn.ModuleDict, 5.6. 初始化权重, 5.7. 访问模型参数, 6. 优化器 (torch.optim), 6.1. 优化器基础, 6.2. 常用优化器, 6.3. 使用优化器, 6.4. 学习率调度 (lr_scheduler), 7. 数据加载与处理 (torch.utils.data), 7.1. Dataset, 7.2. DataLoader, 7.3. torchvision.datasets 与 torchvision.transforms, 8. 模型训练标准流程, 8.1. 训练循环, 8.2. 验证x2F测试循环, 8.3. 设备管理 (CPUx2FGPU), 9. 模型保存与加载, 9.1. 保存和加载整个模型, 9.2. 保存和加载模型参数 (state_dict - 推荐), 9.3. 保存和加载检查点 (Checkpointing), 10. GPU 并行与分布式训练, 10.1. nn.DataParallel (单机多卡简单但不推荐用于大规模), 10.2. nn.parallel.DistributedDataParallel (推荐单机x2F多机多卡), 11. PyTorch 生态与工具, 11.1. torchvision, 11.2. torchaudio, 11.3. torchtext, 11.4. TensorBoard 与 PyTorch, 11.5. Torch Hub, 11.6. PyTorch JIT (torch.jit) 与 TorchScript, 11.7. torch.compile() (PyTorch 2.0+), 12. 实践技巧与注意事项, 13. 总结与进阶学习目录简介什么是的核心特性为什么选择安装与环境配置核心张量张量基础创建张量张量属性张量操作算术运算索引切片连接与变形数学运算聚合比较等广播机制与的互操作加速自动微分计算图属性梯度计算和禁用梯度跟踪梯度累积计算图与叶子节点构建神经网络模型基类定义网络结构常用层线性层卷积层池化层循环层层激活函数正则化层损失函数容器初始化权重访问模型参数优化器优化器基础常用优化器使用优化器学习率调度数据加载与处理与模型训练标准流程训练循环验证测试循环设备管理模型保存与加载保存和加载整个模型保存和加载模型参数推荐保存和加载检查点并行与分布式训练单机多卡简单但不推荐用于大规模推荐单机多机多卡生态与工具与与实践技巧与注意事项总结与进阶学习简介什么是是一个由人工智能研究实验室首推的开源机器学习库基于库开发它主要用于构建和训练深度神经网络并广泛应用于计算机视觉自然语言处理等领域的核心特性动态计算图与的静态图不同的计算图在运行时动态构建这使得调试更加直观并且更容易处理动态结构的网络如设计简洁直观与语言和等库的风格高度一致易于上手和使用强大的加速通过提供高效的计算支持丰富的工具和库拥有庞大的生态系统包括等以及大量的第三方扩展和预训练模型灵活性与可扩展性允许研究人员和开发者轻松实现自定义层和算法易于调试由于其动态性调试代码通常比调试静态图框架更简单强大的社区拥有活跃的学术和工业界社区支持为什么选择研究友好动态图和的特性使其成为学术研究的首选快速原型易于快速实现和测试新的想法生产部署随着支持以及的发展在生产环境中的部署能力也越来越强易学性对于熟悉和的开发者来说学习曲线相对平缓安装与环境配置的安装通常通过或完成官方网站提供了根据你的操作系统包管理器版本和版本如果需要支持生成安装命令的便捷工具示例验证安装检查是否可用强烈建议在虚拟环境如或中安装以避免包冲突核心张量张量基础张量是中的核心数据结构类似于的但具有额外的功能如计算和自动微分张量可以表示标量向量矩阵或更高维度的数组图片来源官方教程创建张量从列表创建从数组创建共享内存除非显式复制默认继承数组的数据类型创建特定形状和类型的张量均匀分布标准正态分布单位矩阵类似线性间隔复制另一个张量的属性形状数据类型形状和数据类型与相同但值为形状与相同但数据类型和值不同张量属性或张量的形状一个元组张量中元素的数据类型如张量所在的设备如张量的维度数量张量中元素的总数张量操作算术运算支持逐元素运算或或或或或修改原始张量通常带有下划线后缀索引切片连接与变形索引与切片类似于的数字矩阵用于获取单元素张量的数字布尔索引连接沿指定维度连接张量序列沿新维度堆叠张量序列新维度在最前变形返回具有新形状的张量与原张量共享数据如果可能新形状必须与原形状元素总数兼容表示该维度大小由其他维度推断功能类似但可能返回副本或视图更灵活移除所有大小为的维度在指定维度插入一个大小为的新维度重新排列张量的维度数学运算聚合比较等沿维度矩阵乘法比较广播机制与类似也支持广播允许对不同形状的张量进行操作与的互操作张量和数组可以高效地相互转换数组张量张量上的数组重要转换后的张量数组与原始数据共享内存如果张量在上修改一个会影响另一个也被修改了也被修改了如果张量在上需要先移到加速可以利用通过来加速计算检查是否可用创建张量并移动到设备如果是模型也可以移动到设备确保所有参与运算的张量都在同一个设备上将结果移回例如用于打印或与交互自动微分是的自动微分引擎它为神经网络训练提供了强大的支持计算图使用动态计算图当对张量执行操作时如果张量的为会自动构建一个记录这些操作的计算图这个图由张量节点和函数边表示操作组成属性张量的属性默认为如果一个张量需要计算梯度例如模型的可学习参数则应将其设置为如果一个操作的任何输入张量则其输出张量也会自动叶子节点需要梯度是由需要梯度的计算得到的梯度计算和当在计算图的最终输出标量张量例如损失上调用方法时会自动计算图中所有的叶子张量相对于该输出的梯度梯度会累积到相应张量的属性中续上例计算梯度注意只能对标量输出调用如果输出是多维张量需要提供一个与输出形状相同的参数通常是全的张量相当于对输出求和后再求导默认情况下梯度是累积的在每次优化步骤之前通常需要使用或手动将参数的属性清零禁用梯度跟踪在某些情况下如模型评估推理或更新不需要梯度的参数时需要临时禁用梯度计算以提高效率和减少内存消耗上下文管理器在其内部执行的所有操作都不会被跟踪梯度创建一个与原张量共享数据但不参与梯度计算的新张量它从计算图中分离出来对的操作不会影响的梯度计算梯度累积如前所述调用时梯度会累积到属性中这在某些场景下很有用如梯度裁剪前的梯度总和或模拟大批量训练但在典型的训练循环中每次迭代开始时都需要清零梯度假设和已定义清零梯度计算梯度累积到更新参数计算图与叶子节点叶子节点由用户直接创建的张量例如或且这些是计算梯度的目标非叶子节点计算图中的中间张量它们是操作的结果默认情况下即使它们执行后它们的属性也会被释放以节省内存如果需要保留中间节点的梯度可以使用构建神经网络模块是构建神经网络的核心它提供了各种层激活函数损失函数和容器模型基类所有神经网络模型都应该继承自子类必须实现方法该方法定义了数据如何通过网络传播会自动跟踪其子模块其他实例在中定义为属性和参数实例定义网络结构通常包含无状态的操作如激活函数池化函数调用父类构造函数全连接层激活函数全连接层输入张量通常分类任务的最后一层不直接加而是由损失函数如处理实例化模型例如图像个类别常用层线性层也称为全连接层或密集层对输入数据应用线性变换输入样本的大小特征数量输出样本的大小卷积层用于处理网格状数据如图像和序列输入图像的通道数如图像为输出特征图的通道数卷积核的数量卷积核的大小可以是一个整数或元组步长填充池化层用于降低特征图的空间维度减少参数数量并控制过拟合最大池化平均池化自适应平均池化输出固定大小的特征图循环层用于处理序列数据如文本或时间序列基本循环神经网络长短期记忆网络能更好地处理长序列依赖门控循环单元的一种变体参数更少表示输入和输出张量的第一维是批次大小层基于自注意力机制的模型在和其他序列到序列任务中表现出色它由和组成每个又由多个和堆叠而成激活函数为神经网络引入非线性使其能够学习更复杂的模式输出范围常用于二分类输出层输出范围将输出转换为概率分布总和为常用于多分类输出层后取对数常与结合使用也可以使用中的函数版本如正则化层在训练期间以概率随机将输入张量中的部分元素置零防止过拟合在评估模式下不生效批归一化对小批量数据的每个通道进行归一化有助于加速训练提高稳定性并具有一定的正则化效果在训练和评估模式下行为不同损失函数衡量模型输出与真实目标之间的差异训练的目标是最小化损失函数定义在模块中均方误差用于回归平均绝对误差用于回归交叉熵损失常用于多分类问题它内部结合了和因此模型输出层不应再接期望输入是原始负对数似然损失通常与层一起使用二元交叉熵损失用于二分类问题目标是或模型输出应是激活后的概率结合了层和数值上更稳定期望输入是原始包含类别索引容器用于组织多个层一个有序的模块容器数据会按照定义的顺序依次通过包含的模块如果损失函数是将子模块存储在列表中可以像普通列表一样索引模块但这些模块会被正确注册以便模型可以跟踪它们的参数将子模块存储在字典中初始化权重中的参数权重和偏置会有默认的初始化有时需要自定义初始化初始化初始化递归地将函数应用于每个子模块访问模型参数返回一个包含模型所有可学习参数权重和偏置的迭代器返回参数名和参数本身的迭代器优化器优化器基础优化器实现了各种优化算法用于根据计算得到的梯度来更新模型的参数权重以最小化损失函数常用优化器随机梯度下降是学习率和正则化是常用选项优化器一种自适应学习率方法通常效果良好且收敛快的一个变体改进了权重衰减的处理方式通常比配合权重衰减效果更好优化器优化器使用优化器典型的优化步骤清除先前计算的梯度必须在之前或之后但在之前调用通常在每次迭代开始时调用计算损失相对于模型参数的梯度根据梯度更新模型参数假设已定义学习率调度模块提供了多种在训练过程中调整学习率的方法每隔几个将学习率乘以一个因子在指定的将学习率乘以一个因子当某个指标如验证损失停止改善时降低学习率使用余弦退火调度学习率数据加载与处理高效的数据加载和预处理对训练至关重要是一个抽象类代表一个数据集自定义数据集应继承它并重写以下方法返回数据集的大小返回索引为的一个样本通常是元组应用数据增强转换将包装成一个可迭代对象提供以下功能批量将数据组织成小批量打乱在每个开始时打乱数据顺序并行加载使用多个工作进程并行加载数据打乱训练数据使用个子进程加载数据如果使用可以加速数据传输和已经是小批量数据将数据移到设备训练步骤与库为计算机视觉任务提供了常用的数据集模型架构和图像转换工具包含等常用数据集的加载器包含常用的图像预处理操作如缩放裁剪归一化数据增强等可以使用将多个转换串联起来定义转换将或转换为张量并缩放到均值和标准差加载数据集模型训练标准流程训练循环设置为训练模式启用更新等移动数据到设备前向传播计算损失反向传播与优化清零梯度计算梯度更新权重统计损失和准确率每个打印一次可选学习率调度或者验证测试循环在每个结束或训练完成后在验证集或测试集上评估模型性能设置为评估模式禁用使用运行统计等在评估期间不计算梯度假设已定义设备管理确保模型和数据都在同一个设备上模型保存与加载保存和加载整个模型保存整个模型包括结构和参数不推荐因为序列化代码可能在不同版本或不同项目间不兼容保存和加载模型参数推荐只保存模型的可学习参数状态字典更灵活更推荐保存加载先创建模型实例确保加载后模型在正确设备上保存和加载检查点保存训练过程中的更多信息如优化器状态损失等以便恢复训练保存检查点加载检查点并行与分布式训练单机多卡简单但不推荐用于大规模可以简单地将模型包装起来在多个上并行处理一个小批量数据它会将输入数据划分到不同复制模型到每个进行前向传播然后将结果收集到主计算损失缺点主负载较重可能存在负载不均且通常比慢将模型包装起来应该是主如推荐单机多机多卡是进行分布式训练包括单机多卡和多机多卡的推荐方式它为每个创建一个进程模型在每个进程中独立复制梯度在反向传播期间进行同步和平均通常性能更好负载更均衡设置相对复杂需要配置进程组和使用生态与工具提供计算机视觉相关的流行数据集模型架构如和通用的图像转换提供音频处理相关的工具数据集和模型注意的和维护状态在近几年有较大变化社区转向更现代的库如但它仍提供一些基础功能数据集和词嵌入与可以通过与集成用于可视化训练过程如损失曲线准确率网络图图像等日志会保存在在训练循环中记录记录模型图在终端运行一个包含预训练模型如图像分类分割目标检测文本模型的中心仓库方便通过加载与编译器可以将模型转换为格式是一种可以在非环境如中运行的模型表示通过传递示例输入来记录模型执行的操作生成静态图不适用于包含控制流如语句的模型直接分析和编译模型代码可以处理控制流如果模型有控制流是引入的一个重要特性旨在通过将程序编译成更优化的内核来显著加速模型训练和推理而无需大量代码更改默认使用后端之后像往常一样使用它支持多种后端和优化模式实践技巧与注意事项设置随机种子为了实验的可复现性在开始时设置随机种子从只包含一个元素的张量中获取数字张量形状时刻关注张量的形状使用调试维度不匹配是常见错误来源将模型设置为评估模式影响等层的行为禁用梯度计算用于推理或不需要梯度的计算以节省内存和计算通常在评估时两者都使用内存管理对于大张量或在上操作时注意及时删除不再需要的张量并调用尽管后者不一定立即释放内存给调试使用语句调试器或调试器由于动态图逐行调试通常很有效总结与进阶学习是一个强大灵活且对开发者友好的深度学习框架本指南涵盖了其核心概念和常用功能为构建和训练神经网络奠定了基础进阶学习方向特定应用领域深入计算机视觉如目标检测分割生成模型自然语言处理变体大型语言模型强化学习图神经网络等高级优化技术更复杂的学习率调度器自定义优化器模型部署分布式训练深入理解和后端底层机制进一步理解的工作原理编程基础生态中的新库如阅读论文并尝试复现这是提升理解和实践能力的绝佳方式持续学习和实践是掌握并将其应用于解决复杂问题的关键官方文档教程和活跃的社区是宝贵的资源',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-12 02:29:36',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">Welcome To LifeTech's Blog</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/CSharp%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">CSharp学习<sup>3</sup></a><a href="/tags/IDE%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 1.05rem;">IDE常用快捷键<sup>2</sup></a><a href="/tags/Rust%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 1.05rem;">Rust基础知识<sup>21</sup></a><a href="/tags/csharp%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">csharp相关学习<sup>5</sup></a><a href="/tags/docker%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">docker学习<sup>1</sup></a><a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/" style="font-size: 1.05rem;">hexo博客常见命令<sup>1</sup></a><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">python相关学习<sup>4</sup></a><a href="/tags/tauri%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 1.05rem;">tauri基础知识<sup>1</sup></a><a href="/tags/xmake%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">xmake学习<sup>3</sup></a><a href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">个人学习<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">前端学习<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 1.05rem;">数据分析<sup>1</sup></a><a href="/tags/%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95/" style="font-size: 1.05rem;">无线调试<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">深度学习<sup>1</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" style="font-size: 1.05rem;">系统常用命令<sup>2</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 1.05rem;">系统常用快捷键<sup>2</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">32</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/11/"><span class="card-archive-list-date">十一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"><a class="article-meta__tags" href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>python相关学习</span></a></span></div></div><h1 class="post-title" itemprop="name headline">pytorch基础知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-05-11T18:17:15.000Z" title="发表于 2025-05-12 02:17:15">2025-05-12</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-05-11T18:29:36.543Z" title="更新于 2025-05-12 02:29:36">2025-05-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为无锡"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>无锡</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><header><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">python相关学习</a><h1 id="CrawlerTitle" itemprop="name headline">pytorch基础知识</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Jackey Zhou</span><time itemprop="dateCreated datePublished" datetime="2025-05-11T18:17:15.000Z" title="发表于 2025-05-12 02:17:15">2025-05-12</time><time itemprop="dateCreated datePublished" datetime="2025-05-11T18:29:36.543Z" title="更新于 2025-05-12 02:29:36">2025-05-12</time></header><p><strong>目录</strong></p>
<h2 id="1-PyTorch-简介"><a href="#1-PyTorch-简介" class="headerlink" title="1. PyTorch 简介"></a><a href="#1-pytorch-%E7%AE%80%E4%BB%8B">1. PyTorch 简介</a></h2><ul>
<li><a href="#11-%E4%BB%80%E4%B9%88%E6%98%AF-pytorch">1.1. 什么是 PyTorch?</a></li>
<li><a href="#12-pytorch-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%89%B9%E6%80%A7">1.2. PyTorch 的核心特性</a></li>
<li><a href="#13-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9-pytorch">1.3. 为什么选择 PyTorch?</a></li>
</ul>
<h2 id="2-安装与环境配置"><a href="#2-安装与环境配置" class="headerlink" title="2. 安装与环境配置"></a><a href="#2-%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE">2. 安装与环境配置</a></h2><h2 id="3-PyTorch-核心：张量-Tensors"><a href="#3-PyTorch-核心：张量-Tensors" class="headerlink" title="3. PyTorch 核心：张量 (Tensors)"></a><a href="#3-pytorch-%E6%A0%B8%E5%BF%83%E5%BC%A0%E9%87%8F-tensors">3. PyTorch 核心：张量 (Tensors)</a></h2><ul>
<li><a href="#31-%E5%BC%A0%E9%87%8F%E5%9F%BA%E7%A1%80">3.1. 张量基础</a></li>
<li><a href="#32-%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">3.2. 创建张量</a></li>
<li><a href="#33-%E5%BC%A0%E9%87%8F%E5%B1%9E%E6%80%A7">3.3. 张量属性</a></li>
<li><a href="#34-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C">3.4. 张量操作</a><ul>
<li><a href="#%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97">算术运算</a></li>
<li><a href="#%E7%B4%A2%E5%BC%95%E5%88%87%E7%89%87%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%8F%98%E5%BD%A2">索引、切片、连接与变形</a></li>
<li><a href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97-%E8%81%9A%E5%90%88%E6%AF%94%E8%BE%83%E7%AD%89">数学运算 (聚合、比较等)</a></li>
<li><a href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6">广播机制</a></li>
</ul>
</li>
<li><a href="#35-%E4%B8%8E-numpy-%E7%9A%84%E4%BA%92%E6%93%8D%E4%BD%9C">3.5. 与 NumPy 的互操作</a></li>
<li><a href="#36-gpu-%E5%8A%A0%E9%80%9F">3.6. GPU 加速</a></li>
</ul>
<h2 id="4-Autograd：自动微分"><a href="#4-Autograd：自动微分" class="headerlink" title="4. Autograd：自动微分"></a><a href="#4-autograd%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86">4. Autograd：自动微分</a></h2><ul>
<li><a href="#41-%E8%AE%A1%E7%AE%97%E5%9B%BE">4.1. 计算图</a></li>
<li><a href="#42-requires_grad-%E5%B1%9E%E6%80%A7">4.2. <code>requires_grad</code> 属性</a></li>
<li><a href="#43-%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97-backward-%E5%92%8C-grad">4.3. 梯度计算 (<code>backward()</code> 和 <code>.grad</code>)</a></li>
<li><a href="#44-%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%B7%9F%E8%B8%AA-torchnograd-detach">4.4. 禁用梯度跟踪 (<code>torch.no_grad()</code>, <code>.detach()</code>)</a></li>
<li><a href="#45-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF">4.5. 梯度累积</a></li>
<li><a href="#46-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9">4.6. 计算图与叶子节点</a></li>
</ul>
<h2 id="5-构建神经网络-torch-nn"><a href="#5-构建神经网络-torch-nn" class="headerlink" title="5. 构建神经网络 (torch.nn)"></a><a href="#5-%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-torchnn">5. 构建神经网络 (<code>torch.nn</code>)</a></h2><ul>
<li><a href="#51-nnmodule-%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%B1%BB">5.1. <code>nn.Module</code>: 模型基类</a></li>
<li><a href="#52-%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">5.2. 定义网络结构</a></li>
<li><a href="#53-%E5%B8%B8%E7%94%A8%E5%B1%82-layers">5.3. 常用层 (Layers)</a><ul>
<li><a href="#%E7%BA%BF%E6%80%A7%E5%B1%82-nnlinear">线性层 (<code>nn.Linear</code>)</a></li>
<li><a href="#%E5%8D%B7%E7%A7%AF%E5%B1%82-nnconv1d-nnconv2d-nnconv3d">卷积层 (<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>)</a></li>
<li><a href="#%E6%B1%A0%E5%8C%96%E5%B1%82-nnmaxpool2d-nnavgpool2d-nnadaptiveavgpool2d">池化层 (<code>nn.MaxPool2d</code>, <code>nn.AvgPool2d</code>, <code>nn.AdaptiveAvgPool2d</code>)</a></li>
<li><a href="#%E5%BE%AA%E7%8E%AF%E5%B1%82-nnrnn-nnlstm-nngru">循环层 (<code>nn.RNN</code>, <code>nn.LSTM</code>, <code>nn.GRU</code>)</a></li>
<li><a href="#transformer-%E5%B1%82-nntransformer">Transformer 层 (<code>nn.Transformer</code>)</a></li>
<li><a href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-nnrelu-nnsigmoid-nntanh-nnsoftmax-etc">激活函数 (<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>, <code>nn.Softmax</code>, etc.)</a></li>
<li><a href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82-nndropout-nnbatchnorm1d2d3d">正则化层 (<code>nn.Dropout</code>, <code>nn.BatchNorm1d/2d/3d</code>)</a></li>
</ul>
</li>
<li><a href="#54-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss-functions">5.4. 损失函数 (Loss Functions)</a></li>
<li><a href="#55-%E5%AE%B9%E5%99%A8-containers">5.5. 容器 (Containers)</a><ul>
<li><a href="#nnsequential"><code>nn.Sequential</code></a></li>
<li><a href="#nnmodulelist"><code>nn.ModuleList</code></a></li>
<li><a href="#nnmoduledict"><code>nn.ModuleDict</code></a></li>
</ul>
</li>
<li><a href="#56-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D">5.6. 初始化权重</a></li>
<li><a href="#57-%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0">5.7. 访问模型参数</a></li>
</ul>
<h2 id="6-优化器-torch-optim"><a href="#6-优化器-torch-optim" class="headerlink" title="6. 优化器 (torch.optim)"></a><a href="#6-%E4%BC%98%E5%8C%96%E5%99%A8-torchoptim">6. 优化器 (<code>torch.optim</code>)</a></h2><ul>
<li><a href="#61-%E4%BC%98%E5%8C%96%E5%99%A8%E5%9F%BA%E7%A1%80">6.1. 优化器基础</a></li>
<li><a href="#62-%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8">6.2. 常用优化器</a></li>
<li><a href="#63-%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8">6.3. 使用优化器</a></li>
<li><a href="#64-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6-lr_scheduler">6.4. 学习率调度 (<code>lr_scheduler</code>)</a></li>
</ul>
<h2 id="7-数据加载与处理-torch-utils-data"><a href="#7-数据加载与处理-torch-utils-data" class="headerlink" title="7. 数据加载与处理 (torch.utils.data)"></a><a href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%A4%84%E7%90%86-torchutilsdata">7. 数据加载与处理 (<code>torch.utils.data</code>)</a></h2><ul>
<li><a href="#71-dataset">7.1. <code>Dataset</code></a></li>
<li><a href="#72-dataloader">7.2. <code>DataLoader</code></a></li>
<li><a href="#73-torchvisiondatasets-%E4%B8%8E-torchvisiontransforms">7.3. <code>torchvision.datasets</code> 与 <code>torchvision.transforms</code></a></li>
</ul>
<h2 id="8-模型训练标准流程"><a href="#8-模型训练标准流程" class="headerlink" title="8. 模型训练标准流程"></a><a href="#8-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B">8. 模型训练标准流程</a></h2><ul>
<li><a href="#81-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF">8.1. 训练循环</a></li>
<li><a href="#82-%E9%AA%8C%E8%AF%81%E6%B5%8B%E8%AF%95%E5%BE%AA%E7%8E%AF">8.2. 验证&#x2F;测试循环</a></li>
<li><a href="#83-%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86-cpugpu">8.3. 设备管理 (CPU&#x2F;GPU)</a></li>
</ul>
<h2 id="9-模型保存与加载"><a href="#9-模型保存与加载" class="headerlink" title="9. 模型保存与加载"></a><a href="#9-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD">9. 模型保存与加载</a></h2><ul>
<li><a href="#91-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%95%B4%E4%B8%AA%E6%A8%A1%E5%9E%8B">9.1. 保存和加载整个模型</a></li>
<li><a href="#92-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-state_dict---%E6%8E%A8%E8%8D%90">9.2. 保存和加载模型参数 (<code>state_dict</code> - 推荐)</a></li>
<li><a href="#93-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A3%80%E6%9F%A5%E7%82%B9-checkpointing">9.3. 保存和加载检查点 (Checkpointing)</a></li>
</ul>
<h2 id="10-GPU-并行与分布式训练"><a href="#10-GPU-并行与分布式训练" class="headerlink" title="10. GPU 并行与分布式训练"></a><a href="#10-gpu-%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83">10. GPU 并行与分布式训练</a></h2><ul>
<li><a href="#101-nndataparallel-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E7%AE%80%E5%8D%95%E4%BD%86%E4%B8%8D%E6%8E%A8%E8%8D%90%E7%94%A8%E4%BA%8E%E5%A4%A7%E8%A7%84%E6%A8%A1">10.1. <code>nn.DataParallel</code> (单机多卡，简单但不推荐用于大规模)</a></li>
<li><a href="#102-nnparalleldistributeddataparallel-%E6%8E%A8%E8%8D%90%E5%8D%95%E6%9C%BA%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1">10.2. <code>nn.parallel.DistributedDataParallel</code> (推荐，单机&#x2F;多机多卡)</a></li>
</ul>
<h2 id="11-PyTorch-生态与工具"><a href="#11-PyTorch-生态与工具" class="headerlink" title="11. PyTorch 生态与工具"></a><a href="#11-pytorch-%E7%94%9F%E6%80%81%E4%B8%8E%E5%B7%A5%E5%85%B7">11. PyTorch 生态与工具</a></h2><ul>
<li><a href="#111-torchvision">11.1. <code>torchvision</code></a></li>
<li><a href="#112-torchaudio">11.2. <code>torchaudio</code></a></li>
<li><a href="#113-torchtext">11.3. <code>torchtext</code></a></li>
<li><a href="#114-tensorboard-%E4%B8%8E-pytorch">11.4. TensorBoard 与 PyTorch</a></li>
<li><a href="#115-torch-hub">11.5. Torch Hub</a></li>
<li><a href="#116-pytorch-jit-torchjit-%E4%B8%8E-torchscript">11.6. PyTorch JIT (<code>torch.jit</code>) 与 TorchScript</a></li>
<li><a href="#117-torchcompile-pytorch-20">11.7. <code>torch.compile()</code> (PyTorch 2.0+)</a></li>
</ul>
<h2 id="12-实践技巧与注意事项"><a href="#12-实践技巧与注意事项" class="headerlink" title="12. 实践技巧与注意事项"></a><a href="#12-%E5%AE%9E%E8%B7%B5%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">12. 实践技巧与注意事项</a></h2><h2 id="13-总结与进阶学习"><a href="#13-总结与进阶学习" class="headerlink" title="13. 总结与进阶学习"></a><a href="#13-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0">13. 总结与进阶学习</a></h2><hr>
<h2 id="1-PyTorch-简介-1"><a href="#1-PyTorch-简介-1" class="headerlink" title="1. PyTorch 简介"></a>1. PyTorch 简介</h2><h3 id="1-1-什么是-PyTorch"><a href="#1-1-什么是-PyTorch" class="headerlink" title="1.1. 什么是 PyTorch?"></a>1.1. 什么是 PyTorch?</h3><p>PyTorch 是一个由 Facebook 人工智能研究实验室 (FAIR) 首推的开源机器学习库，基于 Torch 库开发。它主要用于构建和训练深度神经网络，并广泛应用于计算机视觉、自然语言处理等领域。</p>
<h3 id="1-2-PyTorch-的核心特性"><a href="#1-2-PyTorch-的核心特性" class="headerlink" title="1.2. PyTorch 的核心特性"></a>1.2. PyTorch 的核心特性</h3><ul>
<li><strong>动态计算图 (Define-by-Run)</strong>: 与 TensorFlow 1.x 的静态图不同，PyTorch 的计算图在运行时动态构建。这使得调试更加直观，并且更容易处理动态结构的网络 (如 RNN)。</li>
<li><strong>Pythonic</strong>: API 设计简洁直观，与 Python 语言和 NumPy 等库的风格高度一致，易于上手和使用。</li>
<li><strong>强大的 GPU 加速</strong>: 通过 CUDA 提供高效的 GPU 计算支持。</li>
<li><strong>丰富的工具和库</strong>: 拥有庞大的生态系统，包括 <code>torchvision</code>, <code>torchaudio</code>, <code>torchtext</code> 等，以及大量的第三方扩展和预训练模型。</li>
<li><strong>灵活性与可扩展性</strong>: 允许研究人员和开发者轻松实现自定义层和算法。</li>
<li><strong>易于调试</strong>: 由于其动态性，调试 PyTorch 代码通常比调试静态图框架更简单。</li>
<li><strong>强大的社区</strong>: 拥有活跃的学术和工业界社区支持。</li>
</ul>
<h3 id="1-3-为什么选择-PyTorch"><a href="#1-3-为什么选择-PyTorch" class="headerlink" title="1.3. 为什么选择 PyTorch?"></a>1.3. 为什么选择 PyTorch?</h3><ul>
<li><strong>研究友好</strong>: 动态图和 Pythonic 的特性使其成为学术研究的首选。</li>
<li><strong>快速原型</strong>: 易于快速实现和测试新的想法。</li>
<li><strong>生产部署</strong>: 随着 TorchScript, ONNX 支持以及 <code>torch.compile()</code> 的发展，PyTorch 在生产环境中的部署能力也越来越强。</li>
<li><strong>易学性</strong>: 对于熟悉 Python 和 NumPy 的开发者来说，学习曲线相对平缓。</li>
</ul>
<h2 id="2-安装与环境配置-1"><a href="#2-安装与环境配置-1" class="headerlink" title="2. 安装与环境配置"></a>2. 安装与环境配置</h2><p>PyTorch 的安装通常通过 <code>pip</code> 或 <code>conda</code> 完成。官方网站 (<a target="_blank" rel="noopener" href="https://pytorch.org/">pytorch.org</a>) 提供了根据你的操作系统、包管理器、Python 版本和 CUDA 版本（如果需要 GPU 支持）生成安装命令的便捷工具。</p>
<p><strong>示例 (pip, Linux, CUDA 12.1):</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p><strong>验证安装:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available()) <span class="comment"># 检查 CUDA 是否可用</span></span><br></pre></td></tr></table></figure>

<p>强烈建议在虚拟环境 (如 <code>venv</code> 或 <code>conda environment</code>) 中安装 PyTorch，以避免包冲突。</p>
<h2 id="3-PyTorch-核心：张量-Tensors-1"><a href="#3-PyTorch-核心：张量-Tensors-1" class="headerlink" title="3. PyTorch 核心：张量 (Tensors)"></a>3. PyTorch 核心：张量 (Tensors)</h2><h3 id="3-1-张量基础"><a href="#3-1-张量基础" class="headerlink" title="3.1. 张量基础"></a>3.1. 张量基础</h3><p>张量 (Tensor) 是 PyTorch 中的核心数据结构，类似于 NumPy 的 <code>ndarray</code>，但具有额外的功能，如 GPU 计算和自动微分。张量可以表示标量 (0D)、向量 (1D)、矩阵 (2D) 或更高维度的数组。<br>(图片来源: PyTorch 官方教程)</p>
<h3 id="3-2-创建张量"><a href="#3-2-创建张量" class="headerlink" title="3.2. 创建张量"></a>3.2. 创建张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 Python 列表创建</span></span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data, dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;From list:\n&quot;</span>, x_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 NumPy 数组创建 (共享内存，除非显式复制)</span></span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array) <span class="comment"># 默认继承 NumPy 数组的数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;From NumPy:\n&quot;</span>, x_np)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建特定形状和类型的张量</span></span><br><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">rand_tensor = torch.rand(shape)          <span class="comment"># 0-1 均匀分布</span></span><br><span class="line">randn_tensor = torch.randn(shape)        <span class="comment"># 标准正态分布</span></span><br><span class="line">zeros_tensor = torch.zeros(shape, dtype=torch.long)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">eye_tensor = torch.eye(<span class="number">3</span>)                <span class="comment"># 3x3 单位矩阵</span></span><br><span class="line">arange_tensor = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)   <span class="comment"># 类似 range</span></span><br><span class="line">linspace_tensor = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">5</span>)<span class="comment"># 线性间隔</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制另一个张量的属性 (形状, 数据类型)</span></span><br><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># 形状和数据类型与 x_data 相同，但值为1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ones_like x_data:\n&quot;</span>, x_ones)</span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># 形状与 x_data 相同，但数据类型和值不同</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;rand_like x_data:\n&quot;</span>, x_rand)</span><br></pre></td></tr></table></figure>

<h3 id="3-3-张量属性"><a href="#3-3-张量属性" class="headerlink" title="3.3. 张量属性"></a>3.3. 张量属性</h3><ul>
<li><code>tensor.shape</code> 或 <code>tensor.size()</code>: 张量的形状 (一个元组)。</li>
<li><code>tensor.dtype</code>: 张量中元素的数据类型 (如 <code>torch.float32</code>, <code>torch.long</code>, <code>torch.bool</code>)。</li>
<li><code>tensor.device</code>: 张量所在的设备 (如 <code>cpu</code>, <code>cuda:0</code>)。</li>
<li><code>tensor.ndim</code>: 张量的维度数量。</li>
<li><code>tensor.numel()</code>: 张量中元素的总数。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of dimensions: <span class="subst">&#123;tensor.ndim&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of elements: <span class="subst">&#123;tensor.numel()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-4-张量操作"><a href="#3-4-张量操作" class="headerlink" title="3.4. 张量操作"></a>3.4. 张量操作</h3><h4 id="算术运算"><a href="#算术运算" class="headerlink" title="算术运算"></a>算术运算</h4><p>支持逐元素运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Addition:\n&quot;</span>, x + y)      <span class="comment"># 或 torch.add(x, y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Subtraction:\n&quot;</span>, x - y)   <span class="comment"># 或 torch.sub(x, y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Multiplication (element-wise):\n&quot;</span>, x * y) <span class="comment"># 或 torch.mul(x, y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Division (element-wise):\n&quot;</span>, x / y)    <span class="comment"># 或 torch.div(x, y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Exponentiation:\n&quot;</span>, x ** <span class="number">2</span>) <span class="comment"># 或 torch.pow(x, 2)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In-place operations (修改原始张量，通常带有下划线后缀)</span></span><br><span class="line">x.add_(y) <span class="comment"># x = x + y</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In-place addition (x):\n&quot;</span>, x)</span><br></pre></td></tr></table></figure>

<h4 id="索引、切片、连接与变形"><a href="#索引、切片、连接与变形" class="headerlink" title="索引、切片、连接与变形"></a>索引、切片、连接与变形</h4><ul>
<li><p><strong>索引与切片</strong>: 类似于 NumPy。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 0-11 的数字，3x4 矩阵</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original Tensor:\n&quot;</span>, tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First row:\n&quot;</span>, tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;First column:\n&quot;</span>, tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sub-tensor (rows 0-1, cols 1-2):\n&quot;</span>, tensor[<span class="number">0</span>:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Element at (1,1):\n&quot;</span>, tensor[<span class="number">1</span>, <span class="number">1</span>].item()) <span class="comment"># .item() 用于获取单元素张量的 Python 数字</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 布尔索引</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Elements &gt; 5:\n&quot;</span>, tensor[tensor &gt; <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>连接 (<code>torch.cat</code>, <code>torch.stack</code>)</strong>:</p>
<ul>
<li><code>torch.cat(tensors, dim=0)</code>: 沿指定维度连接张量序列。</li>
<li><code>torch.stack(tensors, dim=0)</code>: 沿新维度堆叠张量序列。<!-- end list -->
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">t_cat_dim0 = torch.cat([t1, t2], dim=<span class="number">0</span>) <span class="comment"># shape: (4, 3)</span></span><br><span class="line">t_cat_dim1 = torch.cat([t1, t2], dim=<span class="number">1</span>) <span class="comment"># shape: (2, 6)</span></span><br><span class="line">t_stack_dim0 = torch.stack([t1, t2], dim=<span class="number">0</span>) <span class="comment"># shape: (2, 2, 3) (新维度在最前)</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>变形 (<code>.view()</code>, <code>.reshape()</code>, <code>.squeeze()</code>, <code>.unsqueeze()</code>, <code>.permute()</code>)</strong>:</p>
<ul>
<li><code>tensor.view(new_shape)</code>: 返回具有新形状的张量，与原张量共享数据（如果可能）。新形状必须与原形状元素总数兼容。<code>-1</code> 表示该维度大小由其他维度推断。</li>
<li><code>tensor.reshape(new_shape)</code>: 功能类似 <code>view</code>，但可能返回副本或视图，更灵活。</li>
<li><code>tensor.squeeze()</code>: 移除所有大小为 1 的维度。</li>
<li><code>tensor.unsqueeze(dim)</code>: 在指定维度 <code>dim</code> 插入一个大小为 1 的新维度。</li>
<li><code>tensor.permute(dims)</code>: 重新排列张量的维度。<!-- end list --></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reshaped to (6, 4):\n&quot;</span>, x.reshape(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Reshaped to (-1, 2):\n&quot;</span>, x.reshape(-<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># (12, 2)</span></span><br><span class="line"></span><br><span class="line">y = torch.zeros(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Squeezed:\n&quot;</span>, y.squeeze().shape) <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Unsqueezed at dim 0:\n&quot;</span>, x.unsqueeze(<span class="number">0</span>).shape) <span class="comment"># torch.Size([1, 2, 3, 4])</span></span><br><span class="line"></span><br><span class="line">z = torch.randn(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Permuted (0,2,1):\n&quot;</span>, z.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>).shape) <span class="comment"># torch.Size([2, 4, 3])</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="数学运算-聚合、比较等"><a href="#数学运算-聚合、比较等" class="headerlink" title="数学运算 (聚合、比较等)"></a>数学运算 (聚合、比较等)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sum:&quot;</span>, torch.<span class="built_in">sum</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean:&quot;</span>, torch.mean(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Std dev:&quot;</span>, torch.std(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Max value:&quot;</span>, torch.<span class="built_in">max</span>(a))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Argmax (index of max):&quot;</span>, torch.argmax(a, dim=<span class="number">1</span>)) <span class="comment"># 沿维度1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">mat1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">mat2 = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Matrix multiplication (matmul):\n&quot;</span>, torch.matmul(mat1, mat2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Matrix multiplication (@ operator):\n&quot;</span>, mat1 @ mat2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Element-wise greater than 3:\n&quot;</span>, a &gt; <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Element-wise equality:\n&quot;</span>, torch.eq(a, torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]], dtype=torch.<span class="built_in">float</span>)))</span><br></pre></td></tr></table></figure>

<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p>与 NumPy 类似，PyTorch 也支持广播，允许对不同形状的张量进行操作。</p>
<h3 id="3-5-与-NumPy-的互操作"><a href="#3-5-与-NumPy-的互操作" class="headerlink" title="3.5. 与 NumPy 的互操作"></a>3.5. 与 NumPy 的互操作</h3><p>PyTorch 张量和 NumPy 数组可以高效地相互转换。</p>
<ul>
<li><code>torch.from_numpy(numpy_array)</code>: NumPy 数组 -&gt; PyTorch 张量。</li>
<li><code>tensor.numpy()</code>: PyTorch 张量 (CPU 上的) -&gt; NumPy 数组。<br><strong>重要</strong>: 转换后的张量&#x2F;数组与原始数据<strong>共享内存</strong>（如果张量在 CPU 上）。修改一个会影响另一个。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np_arr = np.ones(<span class="number">5</span>)</span><br><span class="line">torch_tensor = torch.from_numpy(np_arr)</span><br><span class="line">np_arr += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy array:&quot;</span>, np_arr)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Torch tensor (shared memory):&quot;</span>, torch_tensor) <span class="comment"># 也被修改了</span></span><br><span class="line"></span><br><span class="line">torch_cpu_tensor = torch.ones(<span class="number">5</span>)</span><br><span class="line">np_from_torch = torch_cpu_tensor.numpy()</span><br><span class="line">torch_cpu_tensor.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Torch tensor:&quot;</span>, torch_cpu_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;NumPy array (shared memory):&quot;</span>, np_from_torch) <span class="comment"># 也被修改了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果张量在 GPU 上，需要先移到 CPU</span></span><br><span class="line"><span class="comment"># if torch_gpu_tensor.is_cuda:</span></span><br><span class="line"><span class="comment">#    np_from_gpu = torch_gpu_tensor.cpu().numpy()</span></span><br></pre></td></tr></table></figure>

<h3 id="3-6-GPU-加速"><a href="#3-6-GPU-加速" class="headerlink" title="3.6. GPU 加速"></a>3.6. GPU 加速</h3><p>PyTorch 可以利用 NVIDIA GPU (通过 CUDA) 来加速计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查 CUDA 是否可用</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># CUDA GPU device object</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;CUDA is available! Using GPU: <span class="subst">&#123;torch.cuda.get_device_name(<span class="number">0</span>)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;CUDA not available. Using CPU.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建张量并移动到设备</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x_cpu = x.to(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">x_gpu = x.to(device) <span class="comment"># 如果 device 是 &quot;cuda&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型也可以移动到设备</span></span><br><span class="line"><span class="comment"># model = MyNeuralNetwork().to(device)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保所有参与运算的张量都在同一个设备上</span></span><br><span class="line"><span class="comment"># y_gpu = torch.randn(3,3, device=device)</span></span><br><span class="line"><span class="comment"># z_gpu = x_gpu + y_gpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果移回 CPU (例如，用于打印或与 NumPy 交互)</span></span><br><span class="line"><span class="comment"># z_cpu = z_gpu.cpu()</span></span><br></pre></td></tr></table></figure>

<h2 id="4-Autograd：自动微分-1"><a href="#4-Autograd：自动微分-1" class="headerlink" title="4. Autograd：自动微分"></a>4. Autograd：自动微分</h2><p><code>torch.autograd</code> 是 PyTorch 的自动微分引擎，它为神经网络训练提供了强大的支持。</p>
<h3 id="4-1-计算图"><a href="#4-1-计算图" class="headerlink" title="4.1. 计算图"></a>4.1. 计算图</h3><p>PyTorch 使用动态计算图。当对张量执行操作时，如果张量的 <code>requires_grad</code> 为 <code>True</code>，PyTorch 会自动构建一个记录这些操作的计算图。这个图由张量 (节点) 和函数 (边，表示操作) 组成。</p>
<h3 id="4-2-requires-grad-属性"><a href="#4-2-requires-grad-属性" class="headerlink" title="4.2. requires_grad 属性"></a>4.2. <code>requires_grad</code> 属性</h3><ul>
<li>张量的 <code>requires_grad</code> 属性默认为 <code>False</code>。</li>
<li>如果一个张量需要计算梯度 (例如，模型的可学习参数)，则应将其 <code>requires_grad</code> 设置为 <code>True</code>。</li>
<li>如果一个操作的任何输入张量 <code>requires_grad=True</code>，则其输出张量也会自动 <code>requires_grad=True</code>。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>) <span class="comment"># 叶子节点，需要梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad) <span class="comment"># True</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y.requires_grad) <span class="comment"># True, y 是由需要梯度的 x 计算得到的</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(out.requires_grad) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>

<h3 id="4-3-梯度计算-backward-和-grad"><a href="#4-3-梯度计算-backward-和-grad" class="headerlink" title="4.3. 梯度计算 (backward() 和 .grad)"></a>4.3. 梯度计算 (<code>backward()</code> 和 <code>.grad</code>)</h3><ul>
<li>当在计算图的最终输出标量张量 (例如损失 <code>loss</code>) 上调用 <code>.backward()</code> 方法时，Autograd 会自动计算图中所有 <code>requires_grad=True</code> 的叶子张量相对于该输出的梯度。</li>
<li>梯度会累积到相应张量的 <code>.grad</code> 属性中。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 续上例</span></span><br><span class="line">out.backward() <span class="comment"># 计算梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># d(out)/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="comment">#         [4.5000, 4.5000]])</span></span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>:</p>
<ul>
<li><code>backward()</code> 只能对标量输出调用。如果输出是多维张量，需要提供一个与输出形状相同的 <code>gradient</code> 参数 (通常是全 1 的张量，相当于对输出求和后再求导)。</li>
<li>默认情况下，梯度是累积的。在每次优化步骤之前，通常需要使用 <code>optimizer.zero_grad()</code> 或手动将参数的 <code>.grad</code> 属性清零。</li>
</ul>
<h3 id="4-4-禁用梯度跟踪-torch-no-grad-detach"><a href="#4-4-禁用梯度跟踪-torch-no-grad-detach" class="headerlink" title="4.4. 禁用梯度跟踪 (torch.no_grad(), .detach())"></a>4.4. 禁用梯度跟踪 (<code>torch.no_grad()</code>, <code>.detach()</code>)</h3><p>在某些情况下 (如模型评估、推理，或更新不需要梯度的参数时)，需要临时禁用梯度计算以提高效率和减少内存消耗。</p>
<ul>
<li><strong><code>with torch.no_grad():</code></strong>: 上下文管理器，在其内部执行的所有操作都不会被跟踪梯度。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad) <span class="comment"># True</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></table></figure></li>
<li><strong><code>tensor.detach()</code></strong>: 创建一个与原张量共享数据但不参与梯度计算的新张量。它从计算图中分离出来。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a.detach()</span><br><span class="line"><span class="built_in">print</span>(b.requires_grad) <span class="comment"># False</span></span><br><span class="line"><span class="comment"># 对 b 的操作不会影响 a 的梯度计算</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-5-梯度累积"><a href="#4-5-梯度累积" class="headerlink" title="4.5. 梯度累积"></a>4.5. 梯度累积</h3><p>如前所述，调用 <code>backward()</code> 时，梯度会累积到 <code>.grad</code> 属性中。这在某些场景下很有用 (如梯度裁剪前的梯度总和，或模拟大批量训练)。但在典型的训练循环中，每次迭代开始时都需要清零梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 model 和 optimizer 已定义</span></span><br><span class="line"><span class="comment"># for input, target in data_loader:</span></span><br><span class="line"><span class="comment">#     optimizer.zero_grad() # 清零梯度</span></span><br><span class="line"><span class="comment">#     output = model(input)</span></span><br><span class="line"><span class="comment">#     loss = loss_fn(output, target)</span></span><br><span class="line"><span class="comment">#     loss.backward()       # 计算梯度 (累积到 .grad)</span></span><br><span class="line"><span class="comment">#     optimizer.step()      # 更新参数</span></span><br></pre></td></tr></table></figure>

<h3 id="4-6-计算图与叶子节点"><a href="#4-6-计算图与叶子节点" class="headerlink" title="4.6. 计算图与叶子节点"></a>4.6. 计算图与叶子节点</h3><ul>
<li><strong>叶子节点 (Leaf Tensors)</strong>: 由用户直接创建的张量 (例如 <code>x = torch.tensor(...)</code> 或 <code>nn.Parameter</code>)，且 <code>requires_grad=True</code>。这些是 <code>backward()</code> 计算梯度的目标。</li>
<li><strong>非叶子节点</strong>: 计算图中的中间张量，它们是操作的结果。默认情况下，即使它们 <code>requires_grad=True</code>，<code>backward()</code> 执行后它们的 <code>.grad</code> 属性也会被释放以节省内存。如果需要保留中间节点的梯度，可以使用 <code>tensor.retain_grad()</code>。</li>
</ul>
<h2 id="5-构建神经网络-torch-nn-1"><a href="#5-构建神经网络-torch-nn-1" class="headerlink" title="5. 构建神经网络 (torch.nn)"></a>5. 构建神经网络 (<code>torch.nn</code>)</h2><p><code>torch.nn</code> 模块是构建神经网络的核心。它提供了各种层、激活函数、损失函数和容器。</p>
<h3 id="5-1-nn-Module-模型基类"><a href="#5-1-nn-Module-模型基类" class="headerlink" title="5.1. nn.Module: 模型基类"></a>5.1. <code>nn.Module</code>: 模型基类</h3><p>所有神经网络模型都应该继承自 <code>nn.Module</code>。</p>
<ul>
<li>子类必须实现 <code>forward()</code> 方法，该方法定义了数据如何通过网络传播。</li>
<li><code>nn.Module</code> 会自动跟踪其子模块 (其他 <code>nn.Module</code> 实例，在 <code>__init__</code> 中定义为属性) 和参数 (<code>nn.Parameter</code> 实例)。</li>
</ul>
<h3 id="5-2-定义网络结构"><a href="#5-2-定义网络结构" class="headerlink" title="5.2. 定义网络结构"></a>5.2. 定义网络结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># 通常包含无状态的操作，如激活函数、池化函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__() <span class="comment"># 调用父类构造函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_size, hidden_size) <span class="comment"># 全连接层1</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()                         <span class="comment"># ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_size, num_classes) <span class="comment"># 全连接层2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: 输入张量</span></span><br><span class="line">        out = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.relu(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc2(out)</span><br><span class="line">        <span class="comment"># 通常分类任务的最后一层不直接加 Softmax，而是由损失函数 (如 CrossEntropyLoss) 处理</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">input_dim = <span class="number">784</span> <span class="comment"># 例如 MNIST 图像 28*28</span></span><br><span class="line">hidden_dim = <span class="number">128</span></span><br><span class="line">output_dim = <span class="number">10</span> <span class="comment"># 10个类别</span></span><br><span class="line">model = SimpleNet(input_dim, hidden_dim, output_dim)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<h3 id="5-3-常用层-Layers"><a href="#5-3-常用层-Layers" class="headerlink" title="5.3. 常用层 (Layers)"></a>5.3. 常用层 (Layers)</h3><h4 id="线性层-nn-Linear"><a href="#线性层-nn-Linear" class="headerlink" title="线性层 (nn.Linear)"></a>线性层 (<code>nn.Linear</code>)</h4><p>也称为全连接层或密集层。对输入数据应用线性变换 $y &#x3D; xA^T + b$。<br><code>nn.Linear(in_features, out_features, bias=True)</code></p>
<ul>
<li><code>in_features</code>: 输入样本的大小 (特征数量)。</li>
<li><code>out_features</code>: 输出样本的大小。</li>
</ul>
<h4 id="卷积层-nn-Conv1d-nn-Conv2d-nn-Conv3d"><a href="#卷积层-nn-Conv1d-nn-Conv2d-nn-Conv3d" class="headerlink" title="卷积层 (nn.Conv1d, nn.Conv2d, nn.Conv3d)"></a>卷积层 (<code>nn.Conv1d</code>, <code>nn.Conv2d</code>, <code>nn.Conv3d</code>)</h4><p>用于处理网格状数据，如图像 (2D) 和序列 (1D)。<br><code>nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p>
<ul>
<li><code>in_channels</code>: 输入图像的通道数 (如 RGB 图像为 3)。</li>
<li><code>out_channels</code>: 输出特征图的通道数 (卷积核的数量)。</li>
<li><code>kernel_size</code>: 卷积核的大小 (可以是一个整数或元组)。</li>
<li><code>stride</code>: 步长。</li>
<li><code>padding</code>: 填充。</li>
</ul>
<h4 id="池化层-nn-MaxPool2d-nn-AvgPool2d-nn-AdaptiveAvgPool2d"><a href="#池化层-nn-MaxPool2d-nn-AvgPool2d-nn-AdaptiveAvgPool2d" class="headerlink" title="池化层 (nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d)"></a>池化层 (<code>nn.MaxPool2d</code>, <code>nn.AvgPool2d</code>, <code>nn.AdaptiveAvgPool2d</code>)</h4><p>用于降低特征图的空间维度，减少参数数量并控制过拟合。</p>
<ul>
<li><code>nn.MaxPool2d(kernel_size, stride=None, padding=0, ...)</code>: 最大池化。</li>
<li><code>nn.AvgPool2d(kernel_size, stride=None, padding=0, ...)</code>: 平均池化。</li>
<li><code>nn.AdaptiveAvgPool2d(output_size)</code>: 自适应平均池化，输出固定大小的特征图。</li>
</ul>
<h4 id="循环层-nn-RNN-nn-LSTM-nn-GRU"><a href="#循环层-nn-RNN-nn-LSTM-nn-GRU" class="headerlink" title="循环层 (nn.RNN, nn.LSTM, nn.GRU)"></a>循环层 (<code>nn.RNN</code>, <code>nn.LSTM</code>, <code>nn.GRU</code>)</h4><p>用于处理序列数据，如文本或时间序列。</p>
<ul>
<li><code>nn.RNN(input_size, hidden_size, num_layers, batch_first=False, ...)</code>: 基本循环神经网络。</li>
<li><code>nn.LSTM(input_size, hidden_size, num_layers, batch_first=False, ...)</code>: 长短期记忆网络，能更好地处理长序列依赖。</li>
<li><code>nn.GRU(input_size, hidden_size, num_layers, batch_first=False, ...)</code>: 门控循环单元，LSTM 的一种变体，参数更少。<br><code>batch_first=True</code> 表示输入和输出张量的第一维是批次大小。</li>
</ul>
<h4 id="Transformer-层-nn-Transformer"><a href="#Transformer-层-nn-Transformer" class="headerlink" title="Transformer 层 (nn.Transformer)"></a>Transformer 层 (<code>nn.Transformer</code>)</h4><p>基于自注意力机制的模型，在 NLP 和其他序列到序列任务中表现出色。<br><code>nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, ...)</code><br>它由 <code>nn.TransformerEncoder</code> 和 <code>nn.TransformerDecoder</code> 组成，每个又由多个 <code>nn.TransformerEncoderLayer</code> 和 <code>nn.TransformerDecoderLayer</code> 堆叠而成。</p>
<h4 id="激活函数-nn-ReLU-nn-Sigmoid-nn-Tanh-nn-Softmax-etc"><a href="#激活函数-nn-ReLU-nn-Sigmoid-nn-Tanh-nn-Softmax-etc" class="headerlink" title="激活函数 (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.Softmax, etc.)"></a>激活函数 (<code>nn.ReLU</code>, <code>nn.Sigmoid</code>, <code>nn.Tanh</code>, <code>nn.Softmax</code>, etc.)</h4><p>为神经网络引入非线性，使其能够学习更复杂的模式。</p>
<ul>
<li><code>nn.ReLU()</code>: $max(0, x)$</li>
<li><code>nn.LeakyReLU(negative_slope=0.01)</code></li>
<li><code>nn.Sigmoid()</code>: $1 &#x2F; (1 + e^{-x})$，输出范围 (0, 1)，常用于二分类输出层。</li>
<li><code>nn.Tanh()</code>: $(e^x - e^{-x}) &#x2F; (e^x + e^{-x})$，输出范围 (-1, 1)。</li>
<li><code>nn.Softmax(dim=None)</code>: 将输出转换为概率分布 (总和为 1)，常用于多分类输出层。</li>
<li><code>nn.LogSoftmax(dim=None)</code>: Softmax 后取对数，常与 <code>NLLLoss</code> 结合使用。<br>也可以使用 <code>torch.nn.functional</code> 中的函数版本，如 <code>F.relu(x)</code>。</li>
</ul>
<h4 id="正则化层-nn-Dropout-nn-BatchNorm1d-2d-3d"><a href="#正则化层-nn-Dropout-nn-BatchNorm1d-2d-3d" class="headerlink" title="正则化层 (nn.Dropout, nn.BatchNorm1d/2d/3d)"></a>正则化层 (<code>nn.Dropout</code>, <code>nn.BatchNorm1d/2d/3d</code>)</h4><ul>
<li><code>nn.Dropout(p=0.5)</code>: 在训练期间以概率 <code>p</code> 随机将输入张量中的部分元素置零，防止过拟合。在评估模式 (<code>model.eval()</code>) 下不生效。</li>
<li><code>nn.BatchNorm1d/2d/3d(num_features)</code>: 批归一化。对小批量数据的每个通道进行归一化，有助于加速训练、提高稳定性，并具有一定的正则化效果。在训练和评估模式下行为不同。</li>
</ul>
<h3 id="5-4-损失函数-Loss-Functions"><a href="#5-4-损失函数-Loss-Functions" class="headerlink" title="5.4. 损失函数 (Loss Functions)"></a>5.4. 损失函数 (Loss Functions)</h3><p>衡量模型输出与真实目标之间的差异。训练的目标是最小化损失函数。<br>定义在 <code>torch.nn</code> 模块中。</p>
<ul>
<li><code>nn.MSELoss()</code>: 均方误差，用于回归。</li>
<li><code>nn.L1Loss()</code>: 平均绝对误差，用于回归。</li>
<li><code>nn.CrossEntropyLoss()</code>: 交叉熵损失。常用于多分类问题。它内部结合了 <code>LogSoftmax</code> 和 <code>NLLLoss</code>，因此模型输出层不应再接 <code>LogSoftmax</code>。期望输入是原始 logits。</li>
<li><code>nn.NLLLoss()</code>: 负对数似然损失。通常与 <code>LogSoftmax</code> 层一起使用。</li>
<li><code>nn.BCELoss()</code>: 二元交叉熵损失，用于二分类问题 (目标是 0 或 1)。模型输出应是 Sigmoid 激活后的概率。</li>
<li><code>nn.BCEWithLogitsLoss()</code>: 结合了 Sigmoid 层和 BCELoss，数值上更稳定。期望输入是原始 logits。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># output = model(inputs) # output shape: (batch_size, num_classes)</span></span><br><span class="line"><span class="comment"># targets # shape: (batch_size,), 包含类别索引</span></span><br><span class="line"><span class="comment"># loss = criterion(output, targets)</span></span><br></pre></td></tr></table></figure>

<h3 id="5-5-容器-Containers"><a href="#5-5-容器-Containers" class="headerlink" title="5.5. 容器 (Containers)"></a>5.5. 容器 (Containers)</h3><p>用于组织多个层。</p>
<h4 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a><code>nn.Sequential</code></h4><p>一个有序的模块容器。数据会按照定义的顺序依次通过包含的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">seq_model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">128</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="comment"># nn.LogSoftmax(dim=1) # 如果损失函数是 NLLLoss</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># output = seq_model(input_tensor)</span></span><br></pre></td></tr></table></figure>

<h4 id="nn-ModuleList"><a href="#nn-ModuleList" class="headerlink" title="nn.ModuleList"></a><code>nn.ModuleList</code></h4><p>将子模块存储在列表中。可以像普通 Python 列表一样索引模块，但这些模块会被正确注册，以便模型可以跟踪它们的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModuleWithList</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linears = nn.ModuleList([nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> i, linear_layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.linears):</span><br><span class="line">            x = linear_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h4 id="nn-ModuleDict"><a href="#nn-ModuleDict" class="headerlink" title="nn.ModuleDict"></a><code>nn.ModuleDict</code></h4><p>将子模块存储在字典中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModuleWithDict</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.choices = nn.ModuleDict(&#123;</span><br><span class="line">            <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">10</span>,<span class="number">10</span>),</span><br><span class="line">            <span class="string">&#x27;conv&#x27;</span>: nn.Conv2d(<span class="number">3</span>,<span class="number">8</span>,<span class="number">3</span>)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="variable language_">self</span>.activations = nn.ModuleDict([</span><br><span class="line">            [<span class="string">&#x27;relu&#x27;</span>, nn.ReLU()],</span><br><span class="line">            [<span class="string">&#x27;prelu&#x27;</span>, nn.PReLU()]</span><br><span class="line">        ])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, choice_key, act_key</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.choices[choice_key](x)</span><br><span class="line">        x = <span class="variable language_">self</span>.activations[act_key](x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="5-6-初始化权重"><a href="#5-6-初始化权重" class="headerlink" title="5.6. 初始化权重"></a>5.6. 初始化权重</h3><p><code>nn.Module</code> 中的参数 (权重和偏置) 会有默认的初始化。有时需要自定义初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weights_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        nn.init.xavier_uniform_(m.weight) <span class="comment"># Xavier 初始化</span></span><br><span class="line">        <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            nn.init.zeros_(m.bias)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">        nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>) <span class="comment"># Kaiming 初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model = SimpleNet(...)</span></span><br><span class="line"><span class="comment"># model.apply(weights_init) # 递归地将函数应用于每个子模块</span></span><br></pre></td></tr></table></figure>

<h3 id="5-7-访问模型参数"><a href="#5-7-访问模型参数" class="headerlink" title="5.7. 访问模型参数"></a>5.7. 访问模型参数</h3><p><code>model.parameters()</code> 返回一个包含模型所有可学习参数 (权重和偏置) 的迭代器。<br><code>model.named_parameters()</code> 返回参数名和参数本身的迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for name, param in model.named_parameters():</span></span><br><span class="line"><span class="comment">#     if param.requires_grad:</span></span><br><span class="line"><span class="comment">#         print(name, param.data.shape)</span></span><br></pre></td></tr></table></figure>

<h2 id="6-优化器-torch-optim-1"><a href="#6-优化器-torch-optim-1" class="headerlink" title="6. 优化器 (torch.optim)"></a>6. 优化器 (<code>torch.optim</code>)</h2><h3 id="6-1-优化器基础"><a href="#6-1-优化器基础" class="headerlink" title="6.1. 优化器基础"></a>6.1. 优化器基础</h3><p>优化器实现了各种优化算法，用于根据计算得到的梯度来更新模型的参数 (权重)，以最小化损失函数。</p>
<h3 id="6-2-常用优化器"><a href="#6-2-常用优化器" class="headerlink" title="6.2. 常用优化器"></a>6.2. 常用优化器</h3><ul>
<li><code>optim.SGD(params, lr, momentum=0, weight_decay=0, ...)</code>: 随机梯度下降。<code>lr</code> 是学习率。<code>momentum</code> 和 <code>weight_decay</code> (L2 正则化) 是常用选项。</li>
<li><code>optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, ...)</code>: Adam 优化器，一种自适应学习率方法，通常效果良好且收敛快。</li>
<li><code>optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, ...)</code>: Adam 的一个变体，改进了权重衰减的处理方式，通常比 Adam 配合权重衰减效果更好。</li>
<li><code>optim.RMSprop(params, lr=0.01, alpha=0.99, ...)</code>: RMSprop 优化器。</li>
<li><code>optim.Adagrad(params, lr=0.01, ...)</code>: Adagrad 优化器。</li>
</ul>
<h3 id="6-3-使用优化器"><a href="#6-3-使用优化器" class="headerlink" title="6.3. 使用优化器"></a>6.3. 使用优化器</h3><p>典型的优化步骤：</p>
<ol>
<li><code>optimizer.zero_grad()</code>: 清除先前计算的梯度。必须在 <code>loss.backward()</code> 之前或之后（但在 <code>optimizer.step()</code> 之前）调用，通常在每次迭代开始时调用。</li>
<li><code>loss.backward()</code>: 计算损失相对于模型参数的梯度。</li>
<li><code>optimizer.step()</code>: 根据梯度更新模型参数。</li>
</ol>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># model = SimpleNet(...)</span></span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss()</span></span><br><span class="line"><span class="comment"># optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 inputs, labels 已定义</span></span><br><span class="line"><span class="comment"># optimizer.zero_grad()</span></span><br><span class="line"><span class="comment"># outputs = model(inputs)</span></span><br><span class="line"><span class="comment"># loss = criterion(outputs, labels)</span></span><br><span class="line"><span class="comment"># loss.backward()</span></span><br><span class="line"><span class="comment"># optimizer.step()</span></span><br></pre></td></tr></table></figure>

<h3 id="6-4-学习率调度-lr-scheduler"><a href="#6-4-学习率调度-lr-scheduler" class="headerlink" title="6.4. 学习率调度 (lr_scheduler)"></a>6.4. 学习率调度 (<code>lr_scheduler</code>)</h3><p><code>torch.optim.lr_scheduler</code> 模块提供了多种在训练过程中调整学习率的方法。</p>
<ul>
<li><code>StepLR</code>: 每隔几个 epoch 将学习率乘以一个因子。</li>
<li><code>MultiStepLR</code>: 在指定的 epoch 将学习率乘以一个因子。</li>
<li><code>ReduceLROnPlateau</code>: 当某个指标 (如验证损失) 停止改善时，降低学习率。</li>
<li><code>CosineAnnealingLR</code>: 使用余弦退火调度学习率。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)</span></span><br><span class="line"><span class="comment"># In training loop, after optimizer.step():</span></span><br><span class="line"><span class="comment"># scheduler.step()</span></span><br></pre></td></tr></table></figure>

<h2 id="7-数据加载与处理-torch-utils-data-1"><a href="#7-数据加载与处理-torch-utils-data-1" class="headerlink" title="7. 数据加载与处理 (torch.utils.data)"></a>7. 数据加载与处理 (<code>torch.utils.data</code>)</h2><p>高效的数据加载和预处理对训练至关重要。</p>
<h3 id="7-1-Dataset"><a href="#7-1-Dataset" class="headerlink" title="7.1. Dataset"></a>7.1. <code>Dataset</code></h3><p><code>torch.utils.data.Dataset</code> 是一个抽象类，代表一个数据集。自定义数据集应继承它并重写以下方法：</p>
<ul>
<li><code>__len__(self)</code>: 返回数据集的大小。</li>
<li><code>__getitem__(self, idx)</code>: 返回索引为 <code>idx</code> 的一个样本 (通常是 <code>(data, label)</code> 元组)。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_array, label_array, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data_array</span><br><span class="line">        <span class="variable language_">self</span>.labels = label_array</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        sample_data = <span class="variable language_">self</span>.data[idx]</span><br><span class="line">        sample_label = <span class="variable language_">self</span>.labels[idx]</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            sample_data = <span class="variable language_">self</span>.transform(sample_data) <span class="comment"># 应用数据增强/转换</span></span><br><span class="line">        <span class="keyword">return</span> sample_data, sample_label</span><br><span class="line"></span><br><span class="line"><span class="comment"># dummy_data = torch.randn(100, 784)</span></span><br><span class="line"><span class="comment"># dummy_labels = torch.randint(0, 10, (100,))</span></span><br><span class="line"><span class="comment"># custom_ds = CustomDataset(dummy_data, dummy_labels)</span></span><br></pre></td></tr></table></figure>

<h3 id="7-2-DataLoader"><a href="#7-2-DataLoader" class="headerlink" title="7.2. DataLoader"></a>7.2. <code>DataLoader</code></h3><p><code>torch.utils.data.DataLoader</code> 将 <code>Dataset</code> 包装成一个可迭代对象，提供以下功能：</p>
<ul>
<li><strong>批量 (Batching)</strong>: 将数据组织成小批量。</li>
<li><strong>打乱 (Shuffling)</strong>: 在每个 epoch 开始时打乱数据顺序。</li>
<li><strong>并行加载 (Multiprocessing)</strong>: 使用多个工作进程并行加载数据。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># train_dataset = CustomDataset(...)</span></span><br><span class="line"><span class="comment"># train_loader = DataLoader(dataset=train_dataset,</span></span><br><span class="line"><span class="comment">#                           batch_size=64,</span></span><br><span class="line"><span class="comment">#                           shuffle=True,      # 打乱训练数据</span></span><br><span class="line"><span class="comment">#                           num_workers=4,     # 使用4个子进程加载数据</span></span><br><span class="line"><span class="comment">#                           pin_memory=True)   # 如果使用GPU，可以加速数据传输</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for batch_idx, (data, targets) in enumerate(train_loader):</span></span><br><span class="line"><span class="comment">#     # data 和 targets 已经是小批量数据</span></span><br><span class="line"><span class="comment">#     # 将数据移到设备</span></span><br><span class="line"><span class="comment">#     # data, targets = data.to(device), targets.to(device)</span></span><br><span class="line"><span class="comment">#     # ... 训练步骤 ...</span></span><br></pre></td></tr></table></figure>

<h3 id="7-3-torchvision-datasets-与-torchvision-transforms"><a href="#7-3-torchvision-datasets-与-torchvision-transforms" class="headerlink" title="7.3. torchvision.datasets 与 torchvision.transforms"></a>7.3. <code>torchvision.datasets</code> 与 <code>torchvision.transforms</code></h3><p><code>torchvision</code> 库为计算机视觉任务提供了常用的数据集、模型架构和图像转换工具。</p>
<ul>
<li><strong><code>torchvision.datasets</code></strong>: 包含 MNIST, CIFAR10, ImageNet 等常用数据集的加载器。</li>
<li><strong><code>torchvision.transforms</code></strong>: 包含常用的图像预处理操作，如缩放、裁剪、归一化、数据增强等。可以使用 <code>transforms.Compose</code> 将多个转换串联起来。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义转换</span></span><br><span class="line"><span class="comment"># transform_chain = transforms.Compose([</span></span><br><span class="line"><span class="comment">#     transforms.Resize((32, 32)),</span></span><br><span class="line"><span class="comment">#     transforms.RandomHorizontalFlip(),</span></span><br><span class="line"><span class="comment">#     transforms.ToTensor(), # 将 PIL Image 或 NumPy ndarray 转换为张量，并缩放到 [0,1]</span></span><br><span class="line"><span class="comment">#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet 均值和标准差</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 CIFAR10 数据集</span></span><br><span class="line"><span class="comment"># train_dataset_cifar = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span></span><br><span class="line"><span class="comment">#                                                 download=True, transform=transform_chain)</span></span><br><span class="line"><span class="comment"># train_loader_cifar = DataLoader(train_dataset_cifar, batch_size=64, shuffle=True)</span></span><br></pre></td></tr></table></figure>

<h2 id="8-模型训练标准流程-1"><a href="#8-模型训练标准流程-1" class="headerlink" title="8. 模型训练标准流程"></a>8. 模型训练标准流程</h2><h3 id="8-1-训练循环"><a href="#8-1-训练循环" class="headerlink" title="8.1. 训练循环"></a>8.1. 训练循环</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.train() # 设置为训练模式 (启用 Dropout, BatchNorm 更新等)</span></span><br><span class="line"><span class="comment"># num_epochs = 10</span></span><br><span class="line"><span class="comment"># for epoch in range(num_epochs):</span></span><br><span class="line"><span class="comment">#     running_loss = 0.0</span></span><br><span class="line"><span class="comment">#     correct_predictions = 0</span></span><br><span class="line"><span class="comment">#     total_samples = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     for batch_idx, (inputs, labels) in enumerate(train_loader):</span></span><br><span class="line"><span class="comment">#         inputs, labels = inputs.to(device), labels.to(device) # 移动数据到设备</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         # 1. 前向传播</span></span><br><span class="line"><span class="comment">#         outputs = model(inputs)</span></span><br><span class="line"><span class="comment">#         loss = criterion(outputs, labels) # 计算损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         # 2. 反向传播与优化</span></span><br><span class="line"><span class="comment">#         optimizer.zero_grad() # 清零梯度</span></span><br><span class="line"><span class="comment">#         loss.backward()       # 计算梯度</span></span><br><span class="line"><span class="comment">#         optimizer.step()      # 更新权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         # 统计损失和准确率</span></span><br><span class="line"><span class="comment">#         running_loss += loss.item() * inputs.size(0)</span></span><br><span class="line"><span class="comment">#         _, predicted_classes = torch.max(outputs.data, 1)</span></span><br><span class="line"><span class="comment">#         total_samples += labels.size(0)</span></span><br><span class="line"><span class="comment">#         correct_predictions += (predicted_classes == labels).sum().item()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         if (batch_idx + 1) % 100 == 0: # 每100个batch打印一次</span></span><br><span class="line"><span class="comment">#             print(f&#x27;Epoch [&#123;epoch+1&#125;/&#123;num_epochs&#125;], Step [&#123;batch_idx+1&#125;/&#123;len(train_loader)&#125;], Loss: &#123;loss.item():.4f&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     epoch_loss = running_loss / total_samples</span></span><br><span class="line"><span class="comment">#     epoch_acc = correct_predictions / total_samples</span></span><br><span class="line"><span class="comment">#     print(f&#x27;Epoch &#123;epoch+1&#125; Training Loss: &#123;epoch_loss:.4f&#125;, Accuracy: &#123;epoch_acc:.4f&#125;&#x27;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># （可选）学习率调度</span></span><br><span class="line">    <span class="comment"># if scheduler:</span></span><br><span class="line">    <span class="comment">#     scheduler.step() # 或者 scheduler.step(val_loss) for ReduceLROnPlateau</span></span><br></pre></td></tr></table></figure>

<h3 id="8-2-验证-测试循环"><a href="#8-2-验证-测试循环" class="headerlink" title="8.2. 验证&#x2F;测试循环"></a>8.2. 验证&#x2F;测试循环</h3><p>在每个 epoch 结束或训练完成后，在验证集或测试集上评估模型性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.eval() # 设置为评估模式 (禁用 Dropout, BatchNorm 使用运行统计等)</span></span><br><span class="line"><span class="comment"># test_loss = 0.0</span></span><br><span class="line"><span class="comment"># correct_test = 0</span></span><br><span class="line"><span class="comment"># total_test = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with torch.no_grad(): # 在评估期间不计算梯度</span></span><br><span class="line"><span class="comment">#     for inputs_test, labels_test in test_loader: # 假设 test_loader 已定义</span></span><br><span class="line"><span class="comment">#         inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)</span></span><br><span class="line"><span class="comment">#         outputs_test = model(inputs_test)</span></span><br><span class="line"><span class="comment">#         loss_test = criterion(outputs_test, labels_test)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         test_loss += loss_test.item() * inputs_test.size(0)</span></span><br><span class="line"><span class="comment">#         _, predicted_test = torch.max(outputs_test.data, 1)</span></span><br><span class="line"><span class="comment">#         total_test += labels_test.size(0)</span></span><br><span class="line"><span class="comment">#         correct_test += (predicted_test == labels_test).sum().item()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># avg_test_loss = test_loss / total_test</span></span><br><span class="line"><span class="comment"># test_accuracy = correct_test / total_test</span></span><br><span class="line"><span class="comment"># print(f&#x27;Test Loss: &#123;avg_test_loss:.4f&#125;, Test Accuracy: &#123;test_accuracy:.4f&#125;&#x27;)</span></span><br></pre></td></tr></table></figure>

<h3 id="8-3-设备管理-CPU-GPU"><a href="#8-3-设备管理-CPU-GPU" class="headerlink" title="8.3. 设备管理 (CPU&#x2F;GPU)"></a>8.3. 设备管理 (CPU&#x2F;GPU)</h3><p>确保模型和数据都在同一个设备上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment"># model.to(device)</span></span><br><span class="line"><span class="comment"># inputs = inputs.to(device)</span></span><br><span class="line"><span class="comment"># labels = labels.to(device)</span></span><br></pre></td></tr></table></figure>

<h2 id="9-模型保存与加载-1"><a href="#9-模型保存与加载-1" class="headerlink" title="9. 模型保存与加载"></a>9. 模型保存与加载</h2><h3 id="9-1-保存和加载整个模型"><a href="#9-1-保存和加载整个模型" class="headerlink" title="9.1. 保存和加载整个模型"></a>9.1. 保存和加载整个模型</h3><p>保存整个模型（包括结构和参数）。不推荐，因为序列化代码可能在不同 PyTorch 版本或不同项目间不兼容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.save(model, &#x27;entire_model.pth&#x27;)</span></span><br><span class="line"><span class="comment"># loaded_model = torch.load(&#x27;entire_model.pth&#x27;)</span></span><br><span class="line"><span class="comment"># loaded_model.eval()</span></span><br></pre></td></tr></table></figure>

<h3 id="9-2-保存和加载模型参数-state-dict-推荐"><a href="#9-2-保存和加载模型参数-state-dict-推荐" class="headerlink" title="9.2. 保存和加载模型参数 (state_dict - 推荐)"></a>9.2. 保存和加载模型参数 (<code>state_dict</code> - 推荐)</h3><p>只保存模型的可学习参数 (状态字典)。更灵活、更推荐。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存</span></span><br><span class="line"><span class="comment"># torch.save(model.state_dict(), &#x27;model_state_dict.pth&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载</span></span><br><span class="line"><span class="comment"># model_to_load = SimpleNet(input_dim, hidden_dim, output_dim) # 先创建模型实例</span></span><br><span class="line"><span class="comment"># model_to_load.load_state_dict(torch.load(&#x27;model_state_dict.pth&#x27;))</span></span><br><span class="line"><span class="comment"># model_to_load.to(device) # 确保加载后模型在正确设备上</span></span><br><span class="line"><span class="comment"># model_to_load.eval()</span></span><br></pre></td></tr></table></figure>

<h3 id="9-3-保存和加载检查点-Checkpointing"><a href="#9-3-保存和加载检查点-Checkpointing" class="headerlink" title="9.3. 保存和加载检查点 (Checkpointing)"></a>9.3. 保存和加载检查点 (Checkpointing)</h3><p>保存训练过程中的更多信息 (如 epoch, 优化器状态, 损失等)，以便恢复训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CHECKPOINT_PATH = &#x27;training_checkpoint.pth&#x27;</span></span><br><span class="line"><span class="comment"># epoch = 5</span></span><br><span class="line"><span class="comment"># current_loss = 0.05</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存检查点</span></span><br><span class="line"><span class="comment"># torch.save(&#123;</span></span><br><span class="line"><span class="comment">#             &#x27;epoch&#x27;: epoch,</span></span><br><span class="line"><span class="comment">#             &#x27;model_state_dict&#x27;: model.state_dict(),</span></span><br><span class="line"><span class="comment">#             &#x27;optimizer_state_dict&#x27;: optimizer.state_dict(),</span></span><br><span class="line"><span class="comment">#             &#x27;loss&#x27;: current_loss,</span></span><br><span class="line"><span class="comment">#             # ... any other state you want to save</span></span><br><span class="line"><span class="comment">#             &#125;, CHECKPOINT_PATH)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载检查点</span></span><br><span class="line"><span class="comment"># model_resume = SimpleNet(input_dim, hidden_dim, output_dim)</span></span><br><span class="line"><span class="comment"># optimizer_resume = optim.AdamW(model_resume.parameters())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># checkpoint = torch.load(CHECKPOINT_PATH)</span></span><br><span class="line"><span class="comment"># model_resume.load_state_dict(checkpoint[&#x27;model_state_dict&#x27;])</span></span><br><span class="line"><span class="comment"># optimizer_resume.load_state_dict(checkpoint[&#x27;optimizer_state_dict&#x27;])</span></span><br><span class="line"><span class="comment"># epoch_resume = checkpoint[&#x27;epoch&#x27;]</span></span><br><span class="line"><span class="comment"># loss_resume = checkpoint[&#x27;loss&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model_resume.to(device)</span></span><br><span class="line"><span class="comment"># model_resume.train() # or model_resume.eval()</span></span><br></pre></td></tr></table></figure>

<h2 id="10-GPU-并行与分布式训练-1"><a href="#10-GPU-并行与分布式训练-1" class="headerlink" title="10. GPU 并行与分布式训练"></a>10. GPU 并行与分布式训练</h2><h3 id="10-1-nn-DataParallel-单机多卡，简单但不推荐用于大规模"><a href="#10-1-nn-DataParallel-单机多卡，简单但不推荐用于大规模" class="headerlink" title="10.1. nn.DataParallel (单机多卡，简单但不推荐用于大规模)"></a>10.1. <code>nn.DataParallel</code> (单机多卡，简单但不推荐用于大规模)</h3><p><code>nn.DataParallel</code> 可以简单地将模型包装起来，在多个 GPU 上并行处理一个小批量数据。它会将输入数据划分到不同 GPU，复制模型到每个 GPU，进行前向传播，然后将结果收集到主 GPU 计算损失。<br><strong>缺点</strong>: 主 GPU 负载较重，可能存在负载不均，且通常比 <code>DistributedDataParallel</code> 慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model = SimpleNet(...)</span></span><br><span class="line"><span class="comment"># if torch.cuda.device_count() &gt; 1:</span></span><br><span class="line"><span class="comment">#   print(f&quot;Let&#x27;s use &#123;torch.cuda.device_count()&#125; GPUs!&quot;)</span></span><br><span class="line"><span class="comment">#   model = nn.DataParallel(model) # 将模型包装起来</span></span><br><span class="line"><span class="comment"># model.to(device) # device 应该是主 GPU (如 &quot;cuda:0&quot;)</span></span><br></pre></td></tr></table></figure>

<h3 id="10-2-nn-parallel-DistributedDataParallel-推荐，单机-多机多卡"><a href="#10-2-nn-parallel-DistributedDataParallel-推荐，单机-多机多卡" class="headerlink" title="10.2. nn.parallel.DistributedDataParallel (推荐，单机&#x2F;多机多卡)"></a>10.2. <code>nn.parallel.DistributedDataParallel</code> (推荐，单机&#x2F;多机多卡)</h3><p><code>torch.nn.parallel.DistributedDataParallel</code> (DDP) 是进行分布式训练（包括单机多卡和多机多卡）的推荐方式。它为每个 GPU 创建一个进程，模型在每个进程中独立复制，梯度在反向传播期间进行同步和平均。通常性能更好，负载更均衡。<br>设置 DDP 相对复杂，需要配置进程组 (<code>torch.distributed.init_process_group</code>) 和使用 <code>torch.utils.data.distributed.DistributedSampler</code>。</p>
<h2 id="11-PyTorch-生态与工具-1"><a href="#11-PyTorch-生态与工具-1" class="headerlink" title="11. PyTorch 生态与工具"></a>11. PyTorch 生态与工具</h2><h3 id="11-1-torchvision"><a href="#11-1-torchvision" class="headerlink" title="11.1. torchvision"></a>11.1. <code>torchvision</code></h3><p>提供计算机视觉相关的流行数据集、模型架构 (如 ResNet, VGG) 和通用的图像转换。</p>
<h3 id="11-2-torchaudio"><a href="#11-2-torchaudio" class="headerlink" title="11.2. torchaudio"></a>11.2. <code>torchaudio</code></h3><p>提供音频处理相关的工具、数据集和模型。</p>
<h3 id="11-3-torchtext"><a href="#11-3-torchtext" class="headerlink" title="11.3. torchtext"></a>11.3. <code>torchtext</code></h3><p>（注意：<code>torchtext</code> 的 API 和维护状态在近几年有较大变化，社区转向更现代的 NLP 库如 Hugging Face Transformers。但它仍提供一些基础 NLP 功能、数据集和词嵌入。）</p>
<h3 id="11-4-TensorBoard-与-PyTorch"><a href="#11-4-TensorBoard-与-PyTorch" class="headerlink" title="11.4. TensorBoard 与 PyTorch"></a>11.4. TensorBoard 与 PyTorch</h3><p>PyTorch 可以通过 <code>torch.utils.tensorboard.SummaryWriter</code> 与 TensorBoard 集成，用于可视化训练过程 (如损失曲线、准确率、网络图、图像等)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># writer = SummaryWriter(&#x27;runs/my_experiment_1&#x27;) # 日志会保存在 runs/my_experiment_1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练循环中记录</span></span><br><span class="line"><span class="comment"># writer.add_scalar(&#x27;Training Loss&#x27;, epoch_loss, epoch)</span></span><br><span class="line"><span class="comment"># writer.add_scalar(&#x27;Training Accuracy&#x27;, epoch_acc, epoch)</span></span><br><span class="line"><span class="comment"># writer.add_histogram(&#x27;fc1.bias&#x27;, model.fc1.bias, epoch)</span></span><br><span class="line"><span class="comment"># writer.add_graph(model, sample_input_to_model) # 记录模型图</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># writer.close()</span></span><br><span class="line"><span class="comment"># 在终端运行: tensorboard --logdir=runs</span></span><br></pre></td></tr></table></figure>

<h3 id="11-5-Torch-Hub"><a href="#11-5-Torch-Hub" class="headerlink" title="11.5. Torch Hub"></a>11.5. Torch Hub</h3><p>一个包含预训练模型（如图像分类、分割、目标检测、文本模型）的中心仓库，方便通过 <code>torch.hub.load()</code> 加载。</p>
<h3 id="11-6-PyTorch-JIT-torch-jit-与-TorchScript"><a href="#11-6-PyTorch-JIT-torch-jit-与-TorchScript" class="headerlink" title="11.6. PyTorch JIT (torch.jit) 与 TorchScript"></a>11.6. PyTorch JIT (<code>torch.jit</code>) 与 TorchScript</h3><p>PyTorch Just-In-Time (JIT) 编译器可以将 PyTorch 模型转换为 TorchScript 格式。TorchScript 是一种可以在非 Python 环境 (如 C++) 中运行的模型表示。</p>
<ul>
<li><strong>Tracing (<code>torch.jit.trace</code>)</strong>: 通过传递示例输入来记录模型执行的操作，生成静态图。不适用于包含控制流 (如 if 语句) 的模型。</li>
<li><strong>Scripting (<code>torch.jit.script</code>)</strong>: 直接分析和编译 Python 模型代码，可以处理控制流。</li>
</ul>
<!-- end list -->

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># traced_model = torch.jit.trace(model, example_input_tensor)</span></span><br><span class="line"><span class="comment"># scripted_model = torch.jit.script(model) # 如果模型有控制流</span></span><br><span class="line"><span class="comment"># scripted_model.save(&quot;scripted_model.pt&quot;)</span></span><br></pre></td></tr></table></figure>

<h3 id="11-7-torch-compile-PyTorch-2-0"><a href="#11-7-torch-compile-PyTorch-2-0" class="headerlink" title="11.7. torch.compile() (PyTorch 2.0+)"></a>11.7. <code>torch.compile()</code> (PyTorch 2.0+)</h3><p><code>torch.compile()</code> 是 PyTorch 2.0 引入的一个重要特性，旨在通过将 PyTorch 程序编译成更优化的内核来显著加速模型训练和推理，而无需大量代码更改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model = SimpleNet(...)</span></span><br><span class="line"><span class="comment"># opt_model = torch.compile(model) # 默认使用 TorchInductor 后端</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 之后像往常一样使用 opt_model</span></span><br><span class="line"><span class="comment"># output = opt_model(input_tensor)</span></span><br></pre></td></tr></table></figure>

<p>它支持多种后端和优化模式。</p>
<h2 id="12-实践技巧与注意事项-1"><a href="#12-实践技巧与注意事项-1" class="headerlink" title="12. 实践技巧与注意事项"></a>12. 实践技巧与注意事项</h2><ul>
<li><strong>设置随机种子</strong>: 为了实验的可复现性，在开始时设置随机种子。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import random</span></span><br><span class="line"><span class="comment"># seed = 42</span></span><br><span class="line"><span class="comment"># torch.manual_seed(seed)</span></span><br><span class="line"><span class="comment"># np.random.seed(seed)</span></span><br><span class="line"><span class="comment"># random.seed(seed)</span></span><br><span class="line"><span class="comment"># if torch.cuda.is_available():</span></span><br><span class="line"><span class="comment">#     torch.cuda.manual_seed_all(seed)</span></span><br><span class="line"><span class="comment">#     torch.backends.cudnn.deterministic = True</span></span><br><span class="line"><span class="comment">#     torch.backends.cudnn.benchmark = False</span></span><br></pre></td></tr></table></figure></li>
<li><strong><code>.item()</code></strong>: 从只包含一个元素的张量中获取 Python 数字。</li>
<li><strong>张量形状</strong>: 时刻关注张量的形状，使用 <code>print(tensor.shape)</code> 调试。维度不匹配是常见错误来源。</li>
<li><strong><code>.eval()</code> vs <code>torch.no_grad()</code></strong>:<ul>
<li><code>model.eval()</code>: 将模型设置为评估模式 (影响 Dropout, BatchNorm 等层的行为)。</li>
<li><code>with torch.no_grad()</code>: 禁用梯度计算，用于推理或不需要梯度的计算，以节省内存和计算。<br>通常在评估时两者都使用。</li>
</ul>
</li>
<li><strong>内存管理</strong>: 对于大张量或在 GPU 上操作时，注意及时删除不再需要的张量 (<code>del tensor_name</code>)，并调用 <code>torch.cuda.empty_cache()</code> (尽管后者不一定立即释放内存给 OS)。</li>
<li><strong>调试</strong>: 使用 <code>print()</code> 语句、Python 调试器 (<code>pdb</code> 或 IDE 调试器)。由于动态图，逐行调试通常很有效。</li>
</ul>
<h2 id="13-总结与进阶学习-1"><a href="#13-总结与进阶学习-1" class="headerlink" title="13. 总结与进阶学习"></a>13. 总结与进阶学习</h2><p>PyTorch 是一个强大、灵活且对开发者友好的深度学习框架。本指南涵盖了其核心概念和常用功能，为构建和训练神经网络奠定了基础。</p>
<p><strong>进阶学习方向</strong>:</p>
<ul>
<li><strong>特定应用领域</strong>: 深入计算机视觉 (如目标检测、分割、生成模型)、自然语言处理 (Transformer 变体、大型语言模型)、强化学习、图神经网络等。</li>
<li><strong>高级优化技术</strong>: 更复杂的学习率调度器、自定义优化器。</li>
<li><strong>模型部署</strong>: TorchServe, ONNX Runtime, TensorRT, LibTorch (C++)。</li>
<li><strong>分布式训练</strong>: 深入理解 <code>DistributedDataParallel</code> 和后端 (NCCL, Gloo)。</li>
<li><strong>底层机制</strong>: 进一步理解 Autograd 的工作原理、CUDA 编程基础。</li>
<li><strong>PyTorch 生态中的新库</strong>: 如 <code>functorch</code> (JAX-like functional transformations), <code>torchrl</code> (reinforcement learning)。</li>
<li><strong>阅读 SOTA 论文并尝试复现</strong>: 这是提升理解和实践能力的绝佳方式。</li>
</ul>
<p>持续学习和实践是掌握 PyTorch 并将其应用于解决复杂问题的关键。官方文档、教程和活跃的社区是宝贵的资源。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Jackey Zhou</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/')">pytorch基础知识</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=pytorch基础知识&amp;url=http://example.com/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Welcome To LifeTech's Blog</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>python相关学习<span class="tagsPageCount">4</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/05/12/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">python基础知识</div></div></a></div><div class="next-post pull-right"><a href="/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据科学与机器学习</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2025/05/12/python%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="python基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">python基础知识</div></div></a></div><div><a href="/2025/05/12/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" title="模型部署"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">模型部署</div></div></a></div><div><a href="/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="数据科学与机器学习"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-05-12</div><div class="title">数据科学与机器学习</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description">记录技术成长的个人博客</div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-PyTorch-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">1. PyTorch 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text">2. 安装与环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PyTorch-%E6%A0%B8%E5%BF%83%EF%BC%9A%E5%BC%A0%E9%87%8F-Tensors"><span class="toc-number">3.</span> <span class="toc-text">3. PyTorch 核心：张量 (Tensors)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Autograd%EF%BC%9A%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">4.</span> <span class="toc-text">4. Autograd：自动微分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-torch-nn"><span class="toc-number">5.</span> <span class="toc-text">5. 构建神经网络 (torch.nn)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BC%98%E5%8C%96%E5%99%A8-torch-optim"><span class="toc-number">6.</span> <span class="toc-text">6. 优化器 (torch.optim)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%A4%84%E7%90%86-torch-utils-data"><span class="toc-number">7.</span> <span class="toc-text">7. 数据加载与处理 (torch.utils.data)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B"><span class="toc-number">8.</span> <span class="toc-text">8. 模型训练标准流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD"><span class="toc-number">9.</span> <span class="toc-text">9. 模型保存与加载</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-GPU-%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">10.</span> <span class="toc-text">10. GPU 并行与分布式训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-PyTorch-%E7%94%9F%E6%80%81%E4%B8%8E%E5%B7%A5%E5%85%B7"><span class="toc-number">11.</span> <span class="toc-text">11. PyTorch 生态与工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AE%9E%E8%B7%B5%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">12.</span> <span class="toc-text">12. 实践技巧与注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0"><span class="toc-number">13.</span> <span class="toc-text">13. 总结与进阶学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-PyTorch-%E7%AE%80%E4%BB%8B-1"><span class="toc-number">14.</span> <span class="toc-text">1. PyTorch 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF-PyTorch"><span class="toc-number">14.1.</span> <span class="toc-text">1.1. 什么是 PyTorch?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-PyTorch-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%89%B9%E6%80%A7"><span class="toc-number">14.2.</span> <span class="toc-text">1.2. PyTorch 的核心特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9-PyTorch"><span class="toc-number">14.3.</span> <span class="toc-text">1.3. 为什么选择 PyTorch?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-1"><span class="toc-number">15.</span> <span class="toc-text">2. 安装与环境配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PyTorch-%E6%A0%B8%E5%BF%83%EF%BC%9A%E5%BC%A0%E9%87%8F-Tensors-1"><span class="toc-number">16.</span> <span class="toc-text">3. PyTorch 核心：张量 (Tensors)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%BC%A0%E9%87%8F%E5%9F%BA%E7%A1%80"><span class="toc-number">16.1.</span> <span class="toc-text">3.1. 张量基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F"><span class="toc-number">16.2.</span> <span class="toc-text">3.2. 创建张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%BC%A0%E9%87%8F%E5%B1%9E%E6%80%A7"><span class="toc-number">16.3.</span> <span class="toc-text">3.3. 张量属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="toc-number">16.4.</span> <span class="toc-text">3.4. 张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97"><span class="toc-number">16.4.1.</span> <span class="toc-text">算术运算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E3%80%81%E5%88%87%E7%89%87%E3%80%81%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%8F%98%E5%BD%A2"><span class="toc-number">16.4.2.</span> <span class="toc-text">索引、切片、连接与变形</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97-%E8%81%9A%E5%90%88%E3%80%81%E6%AF%94%E8%BE%83%E7%AD%89"><span class="toc-number">16.4.3.</span> <span class="toc-text">数学运算 (聚合、比较等)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">16.4.4.</span> <span class="toc-text">广播机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E4%B8%8E-NumPy-%E7%9A%84%E4%BA%92%E6%93%8D%E4%BD%9C"><span class="toc-number">16.5.</span> <span class="toc-text">3.5. 与 NumPy 的互操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-GPU-%E5%8A%A0%E9%80%9F"><span class="toc-number">16.6.</span> <span class="toc-text">3.6. GPU 加速</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Autograd%EF%BC%9A%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86-1"><span class="toc-number">17.</span> <span class="toc-text">4. Autograd：自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">17.1.</span> <span class="toc-text">4.1. 计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-requires-grad-%E5%B1%9E%E6%80%A7"><span class="toc-number">17.2.</span> <span class="toc-text">4.2. requires_grad 属性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97-backward-%E5%92%8C-grad"><span class="toc-number">17.3.</span> <span class="toc-text">4.3. 梯度计算 (backward() 和 .grad)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E7%A6%81%E7%94%A8%E6%A2%AF%E5%BA%A6%E8%B7%9F%E8%B8%AA-torch-no-grad-detach"><span class="toc-number">17.4.</span> <span class="toc-text">4.4. 禁用梯度跟踪 (torch.no_grad(), .detach())</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-number">17.5.</span> <span class="toc-text">4.5. 梯度累积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%8F%B6%E5%AD%90%E8%8A%82%E7%82%B9"><span class="toc-number">17.6.</span> <span class="toc-text">4.6. 计算图与叶子节点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-torch-nn-1"><span class="toc-number">18.</span> <span class="toc-text">5. 构建神经网络 (torch.nn)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-nn-Module-%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%B1%BB"><span class="toc-number">18.1.</span> <span class="toc-text">5.1. nn.Module: 模型基类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">18.2.</span> <span class="toc-text">5.2. 定义网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%B8%B8%E7%94%A8%E5%B1%82-Layers"><span class="toc-number">18.3.</span> <span class="toc-text">5.3. 常用层 (Layers)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82-nn-Linear"><span class="toc-number">18.3.1.</span> <span class="toc-text">线性层 (nn.Linear)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82-nn-Conv1d-nn-Conv2d-nn-Conv3d"><span class="toc-number">18.3.2.</span> <span class="toc-text">卷积层 (nn.Conv1d, nn.Conv2d, nn.Conv3d)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82-nn-MaxPool2d-nn-AvgPool2d-nn-AdaptiveAvgPool2d"><span class="toc-number">18.3.3.</span> <span class="toc-text">池化层 (nn.MaxPool2d, nn.AvgPool2d, nn.AdaptiveAvgPool2d)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E5%B1%82-nn-RNN-nn-LSTM-nn-GRU"><span class="toc-number">18.3.4.</span> <span class="toc-text">循环层 (nn.RNN, nn.LSTM, nn.GRU)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer-%E5%B1%82-nn-Transformer"><span class="toc-number">18.3.5.</span> <span class="toc-text">Transformer 层 (nn.Transformer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-nn-ReLU-nn-Sigmoid-nn-Tanh-nn-Softmax-etc"><span class="toc-number">18.3.6.</span> <span class="toc-text">激活函数 (nn.ReLU, nn.Sigmoid, nn.Tanh, nn.Softmax, etc.)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82-nn-Dropout-nn-BatchNorm1d-2d-3d"><span class="toc-number">18.3.7.</span> <span class="toc-text">正则化层 (nn.Dropout, nn.BatchNorm1d&#x2F;2d&#x2F;3d)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-Functions"><span class="toc-number">18.4.</span> <span class="toc-text">5.4. 损失函数 (Loss Functions)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%AE%B9%E5%99%A8-Containers"><span class="toc-number">18.5.</span> <span class="toc-text">5.5. 容器 (Containers)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Sequential"><span class="toc-number">18.5.1.</span> <span class="toc-text">nn.Sequential</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ModuleList"><span class="toc-number">18.5.2.</span> <span class="toc-text">nn.ModuleList</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ModuleDict"><span class="toc-number">18.5.3.</span> <span class="toc-text">nn.ModuleDict</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D"><span class="toc-number">18.6.</span> <span class="toc-text">5.6. 初始化权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-7-%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">18.7.</span> <span class="toc-text">5.7. 访问模型参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BC%98%E5%8C%96%E5%99%A8-torch-optim-1"><span class="toc-number">19.</span> <span class="toc-text">6. 优化器 (torch.optim)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%BC%98%E5%8C%96%E5%99%A8%E5%9F%BA%E7%A1%80"><span class="toc-number">19.1.</span> <span class="toc-text">6.1. 优化器基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">19.2.</span> <span class="toc-text">6.2. 常用优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E4%BD%BF%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">19.3.</span> <span class="toc-text">6.3. 使用优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6-lr-scheduler"><span class="toc-number">19.4.</span> <span class="toc-text">6.4. 学习率调度 (lr_scheduler)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%A4%84%E7%90%86-torch-utils-data-1"><span class="toc-number">20.</span> <span class="toc-text">7. 数据加载与处理 (torch.utils.data)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-Dataset"><span class="toc-number">20.1.</span> <span class="toc-text">7.1. Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-DataLoader"><span class="toc-number">20.2.</span> <span class="toc-text">7.2. DataLoader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-torchvision-datasets-%E4%B8%8E-torchvision-transforms"><span class="toc-number">20.3.</span> <span class="toc-text">7.3. torchvision.datasets 与 torchvision.transforms</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%A0%87%E5%87%86%E6%B5%81%E7%A8%8B-1"><span class="toc-number">21.</span> <span class="toc-text">8. 模型训练标准流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-number">21.1.</span> <span class="toc-text">8.1. 训练循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E9%AA%8C%E8%AF%81-%E6%B5%8B%E8%AF%95%E5%BE%AA%E7%8E%AF"><span class="toc-number">21.2.</span> <span class="toc-text">8.2. 验证&#x2F;测试循环</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86-CPU-GPU"><span class="toc-number">21.3.</span> <span class="toc-text">8.3. 设备管理 (CPU&#x2F;GPU)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD-1"><span class="toc-number">22.</span> <span class="toc-text">9. 模型保存与加载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%95%B4%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-number">22.1.</span> <span class="toc-text">9.1. 保存和加载整个模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-state-dict-%E6%8E%A8%E8%8D%90"><span class="toc-number">22.2.</span> <span class="toc-text">9.2. 保存和加载模型参数 (state_dict - 推荐)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD%E6%A3%80%E6%9F%A5%E7%82%B9-Checkpointing"><span class="toc-number">22.3.</span> <span class="toc-text">9.3. 保存和加载检查点 (Checkpointing)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-GPU-%E5%B9%B6%E8%A1%8C%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-1"><span class="toc-number">23.</span> <span class="toc-text">10. GPU 并行与分布式训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-nn-DataParallel-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%EF%BC%8C%E7%AE%80%E5%8D%95%E4%BD%86%E4%B8%8D%E6%8E%A8%E8%8D%90%E7%94%A8%E4%BA%8E%E5%A4%A7%E8%A7%84%E6%A8%A1"><span class="toc-number">23.1.</span> <span class="toc-text">10.1. nn.DataParallel (单机多卡，简单但不推荐用于大规模)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-nn-parallel-DistributedDataParallel-%E6%8E%A8%E8%8D%90%EF%BC%8C%E5%8D%95%E6%9C%BA-%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1"><span class="toc-number">23.2.</span> <span class="toc-text">10.2. nn.parallel.DistributedDataParallel (推荐，单机&#x2F;多机多卡)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-PyTorch-%E7%94%9F%E6%80%81%E4%B8%8E%E5%B7%A5%E5%85%B7-1"><span class="toc-number">24.</span> <span class="toc-text">11. PyTorch 生态与工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-torchvision"><span class="toc-number">24.1.</span> <span class="toc-text">11.1. torchvision</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-torchaudio"><span class="toc-number">24.2.</span> <span class="toc-text">11.2. torchaudio</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-torchtext"><span class="toc-number">24.3.</span> <span class="toc-text">11.3. torchtext</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-TensorBoard-%E4%B8%8E-PyTorch"><span class="toc-number">24.4.</span> <span class="toc-text">11.4. TensorBoard 与 PyTorch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-5-Torch-Hub"><span class="toc-number">24.5.</span> <span class="toc-text">11.5. Torch Hub</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-PyTorch-JIT-torch-jit-%E4%B8%8E-TorchScript"><span class="toc-number">24.6.</span> <span class="toc-text">11.6. PyTorch JIT (torch.jit) 与 TorchScript</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-7-torch-compile-PyTorch-2-0"><span class="toc-number">24.7.</span> <span class="toc-text">11.7. torch.compile() (PyTorch 2.0+)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AE%9E%E8%B7%B5%E6%8A%80%E5%B7%A7%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9-1"><span class="toc-number">25.</span> <span class="toc-text">12. 实践技巧与注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E6%80%BB%E7%BB%93%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0-1"><span class="toc-number">26.</span> <span class="toc-text">13. 总结与进阶学习</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/15/tauri%E5%89%8D%E5%90%8E%E7%AB%AF%E4%BA%A4%E4%BA%92/" title="tauri前后端交互">tauri前后端交互</a><time datetime="2025-05-14T16:28:49.000Z" title="发表于 2025-05-15 00:28:49">2025-05-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/vue3%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="vue3基础知识">vue3基础知识</a><time datetime="2025-05-12T15:28:53.000Z" title="发表于 2025-05-12 23:28:53">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" title="模型部署">模型部署</a><time datetime="2025-05-11T18:17:36.000Z" title="发表于 2025-05-12 02:17:36">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="数据科学与机器学习">数据科学与机器学习</a><time datetime="2025-05-11T18:17:28.000Z" title="发表于 2025-05-12 02:17:28">2025-05-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/12/pytorch%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="pytorch基础知识">pytorch基础知识</a><time datetime="2025-05-11T18:17:15.000Z" title="发表于 2025-05-12 02:17:15">2025-05-12</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Jackey Zhou" target="_blank">Jackey Zhou</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">16</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/CSharp%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">CSharp学习<sup>3</sup></a><a href="/tags/IDE%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 0.88rem;">IDE常用快捷键<sup>2</sup></a><a href="/tags/Rust%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 0.88rem;">Rust基础知识<sup>21</sup></a><a href="/tags/csharp%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">csharp相关学习<sup>5</sup></a><a href="/tags/docker%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">docker学习<sup>1</sup></a><a href="/tags/hexo%E5%8D%9A%E5%AE%A2%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/" style="font-size: 0.88rem;">hexo博客常见命令<sup>1</sup></a><a href="/tags/python%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">python相关学习<sup>4</sup></a><a href="/tags/tauri%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" style="font-size: 0.88rem;">tauri基础知识<sup>1</sup></a><a href="/tags/xmake%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">xmake学习<sup>3</sup></a><a href="/tags/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">个人学习<sup>1</sup></a><a href="/tags/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">前端学习<sup>1</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" style="font-size: 0.88rem;">数据分析<sup>1</sup></a><a href="/tags/%E6%97%A0%E7%BA%BF%E8%B0%83%E8%AF%95/" style="font-size: 0.88rem;">无线调试<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">深度学习<sup>1</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" style="font-size: 0.88rem;">系统常用命令<sup>2</sup></a><a href="/tags/%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/" style="font-size: 0.88rem;">系统常用快捷键<sup>2</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Jackey Zhou 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>